{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lg\\anaconda3\\envs\\spatial_atlas_ssl\\lib\\site-packages\\geopandas\\_compat.py:124: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lg\\anaconda3\\envs\\spatial_atlas_ssl\\lib\\site-packages\\spatialdata\\__init__.py:9: UserWarning: Geopandas was set to use PyGEOS, changing to shapely 2.0 with:\n",
      "\n",
      "\tgeopandas.options.use_pygeos = True\n",
      "\n",
      "If you intended to use PyGEOS, set the option to False.\n",
      "  _check_geopandas_using_shapely()\n"
     ]
    }
   ],
   "source": [
    "from spatialSSL.Dataloader import EgoNetDataloader, FullImageConstracter\n",
    "from spatialSSL.Utils import split_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Create an instance of Full_image_dataloader\n",
    "\n",
    "#file_path = \"./data/img_119670929.h5ad\"\n",
    "file_path = \"./data/subset_6img_atlas_brain.h5ad\"\n",
    "data_constracter = FullImageConstracter(file_path=file_path, image_col=\"section\", label_col=\"class_id_label\", include_label=False, radius=20,node_level = 1, batch_size=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Constructing Graphs:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78c0547e220a4db884fb1adeece088f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "data_constracter.load_data()\n",
    "\n",
    "# Construct the graph\n",
    "graph_list = data_constracter.construct_graph()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[26230, 550], edge_index=[2, 102942], y=[5246, 550], mask=[26230], cell_type=['22 MY GABA', '22 MY GABA', '22 MY GABA', '22 MY GABA', '22 MY GABA', ..., '31 Vascular', '31 Vascular', '31 Vascular', '31 Vascular', '31 Vascular']\n",
      "Length: 26230\n",
      "Categories (15, object): ['11 HY GABA', '15 HY Glut', '17 P Glut', '18 MB-HB Sero', ..., '30 OEG', '31 Vascular', '32 Immune', '33 LQ'], cell_type_masked=['24 CB GABA', '19 MY Glut', '19 MY Glut', '28 Astro-Epen', '25 CB Glut', ..., '25 CB Glut', '19 MY Glut', '29 Oligo', '25 CB Glut', '25 CB Glut']\n",
      "Length: 5246\n",
      "Categories (15, object): ['11 HY GABA', '15 HY Glut', '17 P Glut', '18 MB-HB Sero', ..., '30 OEG', '31 Vascular', '32 Immune', '33 LQ'], image='1199650929')\n"
     ]
    }
   ],
   "source": [
    "for x in graph_list:\n",
    "    print(x)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_loader, test_loader, val_loader = split_dataset(graph_list,split_percent=(0.6, 0.2, 0.2), batch_size=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9492\n"
     ]
    }
   ],
   "source": [
    "for x in train_loader:\n",
    "    print(list(x.mask).count(False))\n",
    "\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4\n",
      "Validation size: 1\n",
      "Test size: 1\n"
     ]
    }
   ],
   "source": [
    "# Print out the size of each set to verify\n",
    "print(f\"Train size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation size: {len(val_loader.dataset)}\")\n",
    "print(f\"Test size: {len(test_loader.dataset)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torch import nn, optim, Tensor\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.linear = nn.Linear(out_channels, 550)\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        x = self.act(self.conv1(x, edge_index))\n",
    "        x = self.act(self.conv2(x, edge_index))\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the model\n",
    "model = GCN(550, 550, 550).to(device) # in_channels is set to 100 as an example. Please replace it with your actual feature size.\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1) # learning rate scheduler\n",
    "\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data.x.float(), data.edge_index.long())\n",
    "        loss = criterion(outputs[~data.mask], data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        targets_list.append(data.y.cpu().detach())\n",
    "        outputs_list.append(outputs[~data.mask].cpu().detach())\n",
    "\n",
    "    return total_loss / len(loader.dataset), r2_score(torch.cat(targets_list).numpy(), torch.cat(outputs_list).numpy())\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data.x.float(), data.edge_index.long())\n",
    "            loss = criterion(outputs[~data.mask], data.y.float())\n",
    "\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            targets_list.append(data.y.cpu())\n",
    "            outputs_list.append(outputs[~data.mask].cpu())\n",
    "\n",
    "    return total_loss / len(loader.dataset), r2_score(torch.cat(targets_list).numpy(), torch.cat(outputs_list).numpy())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, train loss: 0.9219, train r2: -0.3284,  val loss: 0.7252, val r2: -0.3058, Time: 10.841485977172852s\n",
      "Epoch 2/50, train loss: 0.6982, train r2: -0.1905,  val loss: 0.6613, val r2: -0.1409, Time: 10.559338569641113s\n",
      "Epoch 3/50, train loss: 0.6495, train r2: -0.1078,  val loss: 0.6369, val r2: -0.0996, Time: 10.808122873306274s\n",
      "Epoch 4/50, train loss: 0.6142, train r2: -0.0552,  val loss: 0.5991, val r2: -0.0425, Time: 11.104062557220459s\n",
      "Epoch 5/50, train loss: 0.5833, train r2: -0.0165,  val loss: 0.5787, val r2: -0.0132, Time: 11.197389364242554s\n",
      "Epoch 6/50, train loss: 0.5636, train r2: 0.0094,  val loss: 0.5619, val r2: 0.0095, Time: 11.517719984054565s\n",
      "Epoch 7/50, train loss: 0.5482, train r2: 0.0282,  val loss: 0.5497, val r2: 0.0234, Time: 11.639735221862793s\n",
      "Epoch 8/50, train loss: 0.5374, train r2: 0.0434,  val loss: 0.5397, val r2: 0.0376, Time: 11.201942443847656s\n",
      "Epoch 9/50, train loss: 0.5280, train r2: 0.0558,  val loss: 0.5302, val r2: 0.0508, Time: 11.333412885665894s\n",
      "Epoch 10/50, train loss: 0.5197, train r2: 0.0683,  val loss: 0.5224, val r2: 0.0612, Time: 11.499740600585938s\n",
      "Epoch 11/50, train loss: 0.5128, train r2: 0.0781,  val loss: 0.5156, val r2: 0.0686, Time: 11.614700078964233s\n",
      "Epoch 12/50, train loss: 0.5064, train r2: 0.0857,  val loss: 0.5093, val r2: 0.0761, Time: 11.572927713394165s\n",
      "Epoch 13/50, train loss: 0.5004, train r2: 0.0928,  val loss: 0.5040, val r2: 0.0831, Time: 12.18575406074524s\n",
      "Epoch 14/50, train loss: 0.4947, train r2: 0.0997,  val loss: 0.5005, val r2: 0.0877, Time: 11.537107944488525s\n",
      "Epoch 15/50, train loss: 0.4903, train r2: 0.1054,  val loss: 0.4968, val r2: 0.0926, Time: 11.549302339553833s\n",
      "Epoch 16/50, train loss: 0.4874, train r2: 0.1097,  val loss: 0.4882, val r2: 0.1014, Time: 11.765207290649414s\n",
      "Epoch 17/50, train loss: 0.4823, train r2: 0.1151,  val loss: 0.4896, val r2: 0.0984, Time: 11.63438081741333s\n",
      "Epoch 18/50, train loss: 0.4818, train r2: 0.1167,  val loss: 0.4829, val r2: 0.1083, Time: 12.735787868499756s\n",
      "Epoch 19/50, train loss: 0.4788, train r2: 0.1191,  val loss: 0.4884, val r2: 0.1048, Time: 13.213289499282837s\n",
      "Epoch 20/50, train loss: 0.4767, train r2: 0.1228,  val loss: 0.4794, val r2: 0.1121, Time: 12.486274003982544s\n",
      "Epoch 21/50, train loss: 0.4731, train r2: 0.1270,  val loss: 0.4777, val r2: 0.1148, Time: 12.473301410675049s\n",
      "Epoch 22/50, train loss: 0.4714, train r2: 0.1284,  val loss: 0.4792, val r2: 0.1147, Time: 12.185182094573975s\n",
      "Epoch 23/50, train loss: 0.4702, train r2: 0.1307,  val loss: 0.4753, val r2: 0.1177, Time: 11.794526100158691s\n",
      "Epoch 24/50, train loss: 0.4680, train r2: 0.1332,  val loss: 0.4743, val r2: 0.1190, Time: 11.932190656661987s\n",
      "Epoch 25/50, train loss: 0.4667, train r2: 0.1345,  val loss: 0.4748, val r2: 0.1197, Time: 11.847422122955322s\n",
      "Epoch 26/50, train loss: 0.4657, train r2: 0.1359,  val loss: 0.4729, val r2: 0.1214, Time: 12.126500368118286s\n",
      "Epoch 27/50, train loss: 0.4641, train r2: 0.1379,  val loss: 0.4718, val r2: 0.1220, Time: 11.978771686553955s\n",
      "Epoch 28/50, train loss: 0.4630, train r2: 0.1394,  val loss: 0.4707, val r2: 0.1233, Time: 12.207817554473877s\n",
      "Epoch 29/50, train loss: 0.4622, train r2: 0.1405,  val loss: 0.4709, val r2: 0.1242, Time: 11.88296365737915s\n",
      "Epoch 30/50, train loss: 0.4615, train r2: 0.1413,  val loss: 0.4729, val r2: 0.1229, Time: 11.93678617477417s\n",
      "Epoch 31/50, train loss: 0.4615, train r2: 0.1422,  val loss: 0.4693, val r2: 0.1257, Time: 12.649696111679077s\n",
      "Epoch 32/50, train loss: 0.4598, train r2: 0.1439,  val loss: 0.4697, val r2: 0.1245, Time: 12.009846448898315s\n",
      "Epoch 33/50, train loss: 0.4600, train r2: 0.1437,  val loss: 0.4690, val r2: 0.1258, Time: 12.01268458366394s\n",
      "Epoch 34/50, train loss: 0.4591, train r2: 0.1447,  val loss: 0.4699, val r2: 0.1256, Time: 11.98068904876709s\n",
      "Epoch 35/50, train loss: 0.4593, train r2: 0.1446,  val loss: 0.4692, val r2: 0.1261, Time: 12.50314998626709s\n",
      "Epoch 36/50, train loss: 0.4589, train r2: 0.1450,  val loss: 0.4688, val r2: 0.1261, Time: 11.807404041290283s\n",
      "Epoch 37/50, train loss: 0.4589, train r2: 0.1450,  val loss: 0.4687, val r2: 0.1263, Time: 13.932925939559937s\n",
      "Epoch 38/50, train loss: 0.4586, train r2: 0.1454,  val loss: 0.4690, val r2: 0.1264, Time: 13.71348261833191s\n",
      "Epoch 39/50, train loss: 0.4585, train r2: 0.1455,  val loss: 0.4688, val r2: 0.1265, Time: 13.444311618804932s\n",
      "Epoch 40/50, train loss: 0.4584, train r2: 0.1456,  val loss: 0.4685, val r2: 0.1266, Time: 13.676997661590576s\n",
      "Epoch 41/50, train loss: 0.4583, train r2: 0.1457,  val loss: 0.4685, val r2: 0.1267, Time: 13.109399795532227s\n",
      "Epoch 42/50, train loss: 0.4581, train r2: 0.1459,  val loss: 0.4686, val r2: 0.1268, Time: 13.253151178359985s\n",
      "Epoch 43/50, train loss: 0.4580, train r2: 0.1461,  val loss: 0.4684, val r2: 0.1269, Time: 13.357996702194214s\n",
      "Epoch 44/50, train loss: 0.4579, train r2: 0.1462,  val loss: 0.4683, val r2: 0.1270, Time: 12.769595861434937s\n",
      "Epoch 45/50, train loss: 0.4578, train r2: 0.1463,  val loss: 0.4682, val r2: 0.1270, Time: 12.296486616134644s\n",
      "Epoch 46/50, train loss: 0.4577, train r2: 0.1465,  val loss: 0.4682, val r2: 0.1271, Time: 12.134196758270264s\n",
      "Epoch 47/50, train loss: 0.4576, train r2: 0.1466,  val loss: 0.4681, val r2: 0.1272, Time: 14.06389307975769s\n",
      "Epoch 48/50, train loss: 0.4575, train r2: 0.1468,  val loss: 0.4680, val r2: 0.1273, Time: 12.972051620483398s\n",
      "Epoch 49/50, train loss: 0.4573, train r2: 0.1469,  val loss: 0.4680, val r2: 0.1274, Time: 12.652963638305664s\n",
      "Epoch 50/50, train loss: 0.4572, train r2: 0.1471,  val loss: 0.4679, val r2: 0.1275, Time: 13.9191734790802s\n",
      "Best val loss: 0.4679, at epoch 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_r2 = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_r2 = validate_one_epoch(model, val_loader, criterion)\n",
    "    scheduler.step() # Decrease learning rate by scheduler\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, train loss: {train_loss:.4f}, train r2: {train_r2:.4f},  val loss: {val_loss:.4f}, val r2: {val_r2:.4f}, Time: {time.time()-start_time}s\")\n",
    "\n",
    "print(f\"Best val loss: {best_val_loss:.4f}, at epoch {best_epoch+1}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
