{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbca5746-6306-4485-a7ff-b0291d67c3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheng-wei_liao/miniconda3/envs/master_prak/lib/python3.11/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/home/cheng-wei_liao/miniconda3/envs/master_prak/lib/python3.11/site-packages/spatialdata/__init__.py:9: UserWarning: Geopandas was set to use PyGEOS, changing to shapely 2.0 with:\n",
      "\n",
      "\tgeopandas.options.use_pygeos = True\n",
      "\n",
      "If you intended to use PyGEOS, set the option to False.\n",
      "  _check_geopandas_using_shapely()\n",
      "/home/cheng-wei_liao/miniconda3/envs/master_prak/lib/python3.11/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/cheng-wei_liao/miniconda3/envs/master_prak/lib/python3.11/site-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/cheng-wei_liao/miniconda3/envs/master_prak/lib/python3.11/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/cheng-wei_liao/miniconda3/envs/master_prak/lib/python3.11/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "from spatialSSL.Dataloader import FullImageDatasetConstructor\n",
    "from spatialSSL.Utils import split_dataset\n",
    "from spatialSSL.Training import train\n",
    "from spatialSSL.Models import *\n",
    "from spatialSSL.Training import train_epoch\n",
    "from spatialSSL.Testing import test\n",
    "from spatialSSL.Dataset import InMemoryGraphDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e4acec-2b34-45a3-8571-648888229da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import zipfile\n",
    "\n",
    "# Define a function to load the data from the ZIP file\n",
    "def load_from_zip(zip_path, file_name):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "        with zipf.open(file_name) as file:\n",
    "            return torch.load(file)\n",
    "\n",
    "# Load the pre_train_list and pre_val_list from the ZIP file\n",
    "pre_train_list = load_from_zip('./processed_data/pre_training_data_img6_r30_n1_random_01.zip', 'pre_train_list.pt')\n",
    "pre_val_list = load_from_zip('./processed_data/pre_training_data_img6_r30_n1_random_01.zip', 'pre_val_list.pt')\n",
    "\n",
    "# Now pre_train_list and pre_val_list contain the loaded data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed3e102-6c41-4608-bb4f-464df8fce20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Create DataLoader objects for pre-training and pre-validation\n",
    "pre_train_loader = DataLoader(pre_train_list, batch_size=1, shuffle=True)\n",
    "pre_val_loader = DataLoader(pre_val_list, batch_size=1, shuffle=False)\n",
    "\n",
    "# Now you can use pre_train_loader and pre_val_loader in your training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "000f91b5-404f-40dd-b507-fe6edf5a4948",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb127030-25fe-42b8-ab2c-fe855d643eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, train loss: 1.0381, train r2: -0.4004, train mse: 1.0414,  val loss: 1.0148, val r2: -0.3105, val mse: 1.0110, Time: 1.3393s\n",
      "Epoch 2/300, train loss: 0.9298, train r2: -0.3592, train mse: 0.9846,  val loss: 0.8647, val r2: -0.3487, val mse: 0.9365, Time: 1.2152s\n",
      "Epoch 3/300, train loss: 0.7923, train r2: -0.3518, train mse: 0.9213,  val loss: 0.7446, val r2: -0.2847, val mse: 0.8718, Time: 1.2187s\n",
      "Epoch 4/300, train loss: 0.6944, train r2: -0.2966, train mse: 0.8643,  val loss: 0.6943, val r2: -0.2477, val mse: 0.8270, Time: 1.2333s\n",
      "Epoch 5/300, train loss: 0.6524, train r2: -0.2608, train mse: 0.8219,  val loss: 0.6707, val r2: -0.2160, val mse: 0.7954, Time: 1.2284s\n",
      "Epoch 6/300, train loss: 0.6287, train r2: -0.2281, train mse: 0.7898,  val loss: 0.6523, val r2: -0.1887, val mse: 0.7713, Time: 1.2196s\n",
      "Epoch 7/300, train loss: 0.6111, train r2: -0.2021, train mse: 0.7642,  val loss: 0.6354, val r2: -0.1667, val mse: 0.7516, Time: 1.2023s\n",
      "Epoch 8/300, train loss: 0.5945, train r2: -0.1792, train mse: 0.7430,  val loss: 0.6208, val r2: -0.1478, val mse: 0.7351, Time: 1.2483s\n",
      "Epoch 9/300, train loss: 0.5826, train r2: -0.1595, train mse: 0.7252,  val loss: 0.6083, val r2: -0.1312, val mse: 0.7208, Time: 1.2373s\n",
      "Epoch 10/300, train loss: 0.5730, train r2: -0.1418, train mse: 0.7100,  val loss: 0.5975, val r2: -0.1167, val mse: 0.7083, Time: 1.2010s\n",
      "Epoch 11/300, train loss: 0.5647, train r2: -0.1269, train mse: 0.6968,  val loss: 0.5877, val r2: -0.1043, val mse: 0.6972, Time: 1.2248s\n",
      "Epoch 12/300, train loss: 0.5566, train r2: -0.1137, train mse: 0.6851,  val loss: 0.5775, val r2: -0.0930, val mse: 0.6871, Time: 1.2048s\n",
      "Epoch 13/300, train loss: 0.5481, train r2: -0.1021, train mse: 0.6746,  val loss: 0.5665, val r2: -0.0829, val mse: 0.6777, Time: 1.2499s\n",
      "Epoch 14/300, train loss: 0.5398, train r2: -0.0916, train mse: 0.6650,  val loss: 0.5546, val r2: -0.0735, val mse: 0.6688, Time: 1.1821s\n",
      "Epoch 15/300, train loss: 0.5313, train r2: -0.0821, train mse: 0.6561,  val loss: 0.5449, val r2: -0.0647, val mse: 0.6605, Time: 1.1960s\n",
      "Epoch 16/300, train loss: 0.5231, train r2: -0.0732, train mse: 0.6477,  val loss: 0.5378, val r2: -0.0567, val mse: 0.6527, Time: 1.2119s\n",
      "Epoch 17/300, train loss: 0.5168, train r2: -0.0649, train mse: 0.6400,  val loss: 0.5312, val r2: -0.0492, val mse: 0.6455, Time: 1.2589s\n",
      "Epoch 18/300, train loss: 0.5114, train r2: -0.0572, train mse: 0.6329,  val loss: 0.5250, val r2: -0.0422, val mse: 0.6388, Time: 1.2475s\n",
      "Epoch 19/300, train loss: 0.5070, train r2: -0.0501, train mse: 0.6262,  val loss: 0.5215, val r2: -0.0356, val mse: 0.6325, Time: 1.2189s\n",
      "Epoch 20/300, train loss: 0.5029, train r2: -0.0434, train mse: 0.6200,  val loss: 0.5172, val r2: -0.0295, val mse: 0.6267, Time: 1.2094s\n",
      "Epoch 21/300, train loss: 0.4996, train r2: -0.0371, train mse: 0.6143,  val loss: 0.5145, val r2: -0.0238, val mse: 0.6213, Time: 1.2244s\n",
      "Epoch 22/300, train loss: 0.4962, train r2: -0.0312, train mse: 0.6089,  val loss: 0.5106, val r2: -0.0184, val mse: 0.6162, Time: 1.2136s\n",
      "Epoch 23/300, train loss: 0.4935, train r2: -0.0257, train mse: 0.6039,  val loss: 0.5074, val r2: -0.0134, val mse: 0.6114, Time: 1.1966s\n",
      "Epoch 24/300, train loss: 0.4910, train r2: -0.0205, train mse: 0.5992,  val loss: 0.5067, val r2: -0.0087, val mse: 0.6070, Time: 1.2237s\n",
      "Epoch 25/300, train loss: 0.4887, train r2: -0.0156, train mse: 0.5948,  val loss: 0.5038, val r2: -0.0043, val mse: 0.6028, Time: 1.2095s\n",
      "Epoch 26/300, train loss: 0.4865, train r2: -0.0110, train mse: 0.5906,  val loss: 0.5021, val r2: -0.0002, val mse: 0.5989, Time: 1.2656s\n",
      "Epoch 27/300, train loss: 0.4846, train r2: -0.0066, train mse: 0.5867,  val loss: 0.5002, val r2: 0.0037, val mse: 0.5952, Time: 1.2236s\n",
      "Epoch 28/300, train loss: 0.4832, train r2: -0.0024, train mse: 0.5830,  val loss: 0.4985, val r2: 0.0074, val mse: 0.5917, Time: 1.1894s\n",
      "Epoch 29/300, train loss: 0.4816, train r2: 0.0015, train mse: 0.5795,  val loss: 0.4986, val r2: 0.0109, val mse: 0.5885, Time: 1.2062s\n",
      "Epoch 30/300, train loss: 0.4805, train r2: 0.0052, train mse: 0.5761,  val loss: 0.4976, val r2: 0.0141, val mse: 0.5854, Time: 1.2136s\n",
      "Epoch 31/300, train loss: 0.4783, train r2: 0.0088, train mse: 0.5730,  val loss: 0.4964, val r2: 0.0172, val mse: 0.5825, Time: 1.2055s\n",
      "Epoch 32/300, train loss: 0.4775, train r2: 0.0122, train mse: 0.5700,  val loss: 0.4958, val r2: 0.0201, val mse: 0.5797, Time: 1.2192s\n",
      "Epoch 33/300, train loss: 0.4760, train r2: 0.0155, train mse: 0.5671,  val loss: 0.4951, val r2: 0.0229, val mse: 0.5771, Time: 1.2146s\n",
      "Epoch 34/300, train loss: 0.4747, train r2: 0.0186, train mse: 0.5644,  val loss: 0.4940, val r2: 0.0256, val mse: 0.5747, Time: 1.2064s\n",
      "Epoch 35/300, train loss: 0.4741, train r2: 0.0215, train mse: 0.5618,  val loss: 0.4930, val r2: 0.0281, val mse: 0.5723, Time: 1.1831s\n",
      "Epoch 36/300, train loss: 0.4734, train r2: 0.0244, train mse: 0.5594,  val loss: 0.4935, val r2: 0.0305, val mse: 0.5701, Time: 1.2157s\n",
      "Epoch 37/300, train loss: 0.4722, train r2: 0.0271, train mse: 0.5570,  val loss: 0.4921, val r2: 0.0328, val mse: 0.5679, Time: 1.2110s\n",
      "Epoch 38/300, train loss: 0.4712, train r2: 0.0297, train mse: 0.5548,  val loss: 0.4914, val r2: 0.0350, val mse: 0.5659, Time: 1.1762s\n",
      "Epoch 39/300, train loss: 0.4703, train r2: 0.0322, train mse: 0.5526,  val loss: 0.4913, val r2: 0.0371, val mse: 0.5639, Time: 1.2165s\n",
      "Epoch 40/300, train loss: 0.4698, train r2: 0.0346, train mse: 0.5505,  val loss: 0.4914, val r2: 0.0391, val mse: 0.5621, Time: 1.1981s\n",
      "Epoch 41/300, train loss: 0.4687, train r2: 0.0369, train mse: 0.5485,  val loss: 0.4899, val r2: 0.0410, val mse: 0.5603, Time: 1.2326s\n",
      "Epoch 42/300, train loss: 0.4678, train r2: 0.0391, train mse: 0.5466,  val loss: 0.4897, val r2: 0.0428, val mse: 0.5586, Time: 1.1942s\n",
      "Epoch 43/300, train loss: 0.4671, train r2: 0.0413, train mse: 0.5447,  val loss: 0.4893, val r2: 0.0446, val mse: 0.5569, Time: 1.1802s\n",
      "Epoch 44/300, train loss: 0.4658, train r2: 0.0434, train mse: 0.5429,  val loss: 0.4884, val r2: 0.0463, val mse: 0.5554, Time: 1.2233s\n",
      "Epoch 45/300, train loss: 0.4651, train r2: 0.0454, train mse: 0.5412,  val loss: 0.4880, val r2: 0.0480, val mse: 0.5538, Time: 1.2162s\n",
      "Epoch 46/300, train loss: 0.4645, train r2: 0.0473, train mse: 0.5395,  val loss: 0.4869, val r2: 0.0496, val mse: 0.5524, Time: 1.1932s\n",
      "Epoch 47/300, train loss: 0.4635, train r2: 0.0492, train mse: 0.5379,  val loss: 0.4865, val r2: 0.0512, val mse: 0.5509, Time: 1.2017s\n",
      "Epoch 48/300, train loss: 0.4628, train r2: 0.0511, train mse: 0.5364,  val loss: 0.4866, val r2: 0.0527, val mse: 0.5496, Time: 1.2172s\n",
      "Epoch 49/300, train loss: 0.4625, train r2: 0.0528, train mse: 0.5349,  val loss: 0.4864, val r2: 0.0541, val mse: 0.5482, Time: 1.2009s\n",
      "Epoch 50/300, train loss: 0.4621, train r2: 0.0545, train mse: 0.5334,  val loss: 0.4851, val r2: 0.0555, val mse: 0.5470, Time: 1.2195s\n",
      "Epoch 51/300, train loss: 0.4609, train r2: 0.0562, train mse: 0.5320,  val loss: 0.4855, val r2: 0.0568, val mse: 0.5457, Time: 1.1786s\n",
      "Epoch 52/300, train loss: 0.4599, train r2: 0.0578, train mse: 0.5306,  val loss: 0.4843, val r2: 0.0582, val mse: 0.5445, Time: 1.1822s\n",
      "Epoch 53/300, train loss: 0.4592, train r2: 0.0594, train mse: 0.5292,  val loss: 0.4844, val r2: 0.0595, val mse: 0.5434, Time: 1.1918s\n",
      "Epoch 54/300, train loss: 0.4589, train r2: 0.0609, train mse: 0.5279,  val loss: 0.4842, val r2: 0.0607, val mse: 0.5422, Time: 1.1966s\n",
      "Epoch 55/300, train loss: 0.4576, train r2: 0.0624, train mse: 0.5267,  val loss: 0.4835, val r2: 0.0619, val mse: 0.5412, Time: 1.2014s\n",
      "Epoch 56/300, train loss: 0.4571, train r2: 0.0639, train mse: 0.5254,  val loss: 0.4836, val r2: 0.0630, val mse: 0.5401, Time: 1.1933s\n",
      "Epoch 57/300, train loss: 0.4565, train r2: 0.0653, train mse: 0.5242,  val loss: 0.4831, val r2: 0.0642, val mse: 0.5391, Time: 1.2671s\n",
      "Epoch 58/300, train loss: 0.4562, train r2: 0.0667, train mse: 0.5230,  val loss: 0.4825, val r2: 0.0653, val mse: 0.5381, Time: 1.2336s\n",
      "Epoch 59/300, train loss: 0.4555, train r2: 0.0681, train mse: 0.5219,  val loss: 0.4825, val r2: 0.0663, val mse: 0.5371, Time: 1.6589s\n",
      "Epoch 60/300, train loss: 0.4552, train r2: 0.0694, train mse: 0.5208,  val loss: 0.4820, val r2: 0.0674, val mse: 0.5362, Time: 1.2081s\n",
      "Epoch 61/300, train loss: 0.4541, train r2: 0.0706, train mse: 0.5197,  val loss: 0.4821, val r2: 0.0684, val mse: 0.5353, Time: 1.2440s\n",
      "Epoch 62/300, train loss: 0.4537, train r2: 0.0719, train mse: 0.5186,  val loss: 0.4818, val r2: 0.0693, val mse: 0.5344, Time: 1.2140s\n",
      "Epoch 63/300, train loss: 0.4530, train r2: 0.0731, train mse: 0.5176,  val loss: 0.4817, val r2: 0.0703, val mse: 0.5335, Time: 1.2188s\n",
      "Epoch 64/300, train loss: 0.4526, train r2: 0.0743, train mse: 0.5166,  val loss: 0.4821, val r2: 0.0712, val mse: 0.5327, Time: 1.1966s\n",
      "Epoch 65/300, train loss: 0.4524, train r2: 0.0755, train mse: 0.5156,  val loss: 0.4817, val r2: 0.0721, val mse: 0.5319, Time: 1.2018s\n",
      "Epoch 66/300, train loss: 0.4518, train r2: 0.0766, train mse: 0.5146,  val loss: 0.4827, val r2: 0.0729, val mse: 0.5311, Time: 1.1968s\n",
      "Epoch 67/300, train loss: 0.4515, train r2: 0.0777, train mse: 0.5137,  val loss: 0.4826, val r2: 0.0738, val mse: 0.5304, Time: 1.2101s\n",
      "Epoch 68/300, train loss: 0.4521, train r2: 0.0788, train mse: 0.5128,  val loss: 0.4834, val r2: 0.0746, val mse: 0.5297, Time: 1.2011s\n",
      "Epoch 69/300, train loss: 0.4512, train r2: 0.0798, train mse: 0.5119,  val loss: 0.4815, val r2: 0.0754, val mse: 0.5290, Time: 1.2133s\n",
      "Epoch 70/300, train loss: 0.4506, train r2: 0.0809, train mse: 0.5110,  val loss: 0.4814, val r2: 0.0762, val mse: 0.5283, Time: 1.3158s\n",
      "Epoch 71/300, train loss: 0.4499, train r2: 0.0819, train mse: 0.5102,  val loss: 0.4817, val r2: 0.0769, val mse: 0.5276, Time: 1.2631s\n",
      "Epoch 72/300, train loss: 0.4495, train r2: 0.0829, train mse: 0.5093,  val loss: 0.4829, val r2: 0.0776, val mse: 0.5270, Time: 1.2228s\n",
      "Epoch 73/300, train loss: 0.4502, train r2: 0.0838, train mse: 0.5085,  val loss: 0.4843, val r2: 0.0783, val mse: 0.5264, Time: 1.2255s\n",
      "Epoch 74/300, train loss: 0.4491, train r2: 0.0848, train mse: 0.5077,  val loss: 0.4830, val r2: 0.0789, val mse: 0.5258, Time: 1.2868s\n",
      "Epoch 75/300, train loss: 0.4486, train r2: 0.0857, train mse: 0.5069,  val loss: 0.4813, val r2: 0.0796, val mse: 0.5252, Time: 1.2038s\n",
      "Epoch 76/300, train loss: 0.4485, train r2: 0.0866, train mse: 0.5062,  val loss: 0.4812, val r2: 0.0803, val mse: 0.5246, Time: 1.2354s\n",
      "Epoch 77/300, train loss: 0.4477, train r2: 0.0875, train mse: 0.5054,  val loss: 0.4815, val r2: 0.0809, val mse: 0.5240, Time: 1.1879s\n",
      "Epoch 78/300, train loss: 0.4471, train r2: 0.0884, train mse: 0.5047,  val loss: 0.4814, val r2: 0.0816, val mse: 0.5234, Time: 1.2733s\n",
      "Epoch 79/300, train loss: 0.4459, train r2: 0.0893, train mse: 0.5039,  val loss: 0.4813, val r2: 0.0822, val mse: 0.5229, Time: 1.2033s\n",
      "Epoch 80/300, train loss: 0.4452, train r2: 0.0901, train mse: 0.5032,  val loss: 0.4816, val r2: 0.0828, val mse: 0.5223, Time: 1.1953s\n",
      "Epoch 81/300, train loss: 0.4455, train r2: 0.0910, train mse: 0.5025,  val loss: 0.4825, val r2: 0.0833, val mse: 0.5218, Time: 1.2130s\n",
      "Epoch 82/300, train loss: 0.4444, train r2: 0.0918, train mse: 0.5018,  val loss: 0.4819, val r2: 0.0839, val mse: 0.5213, Time: 1.1975s\n",
      "Epoch 83/300, train loss: 0.4442, train r2: 0.0926, train mse: 0.5011,  val loss: 0.4812, val r2: 0.0845, val mse: 0.5208, Time: 1.2532s\n",
      "Epoch 84/300, train loss: 0.4435, train r2: 0.0934, train mse: 0.5004,  val loss: 0.4808, val r2: 0.0850, val mse: 0.5203, Time: 1.2169s\n",
      "Epoch 85/300, train loss: 0.4429, train r2: 0.0942, train mse: 0.4997,  val loss: 0.4808, val r2: 0.0855, val mse: 0.5199, Time: 1.1968s\n",
      "Epoch 86/300, train loss: 0.4423, train r2: 0.0950, train mse: 0.4990,  val loss: 0.4816, val r2: 0.0861, val mse: 0.5194, Time: 1.1719s\n",
      "Epoch 87/300, train loss: 0.4417, train r2: 0.0958, train mse: 0.4984,  val loss: 0.4810, val r2: 0.0866, val mse: 0.5189, Time: 1.2641s\n",
      "Epoch 88/300, train loss: 0.4410, train r2: 0.0966, train mse: 0.4977,  val loss: 0.4811, val r2: 0.0871, val mse: 0.5185, Time: 1.2313s\n",
      "Epoch 89/300, train loss: 0.4408, train r2: 0.0973, train mse: 0.4971,  val loss: 0.4813, val r2: 0.0875, val mse: 0.5181, Time: 1.1968s\n",
      "Epoch 90/300, train loss: 0.4407, train r2: 0.0981, train mse: 0.4965,  val loss: 0.4817, val r2: 0.0880, val mse: 0.5176, Time: 1.1969s\n",
      "Epoch 91/300, train loss: 0.4398, train r2: 0.0988, train mse: 0.4959,  val loss: 0.4818, val r2: 0.0885, val mse: 0.5172, Time: 1.6054s\n",
      "Early stopping!\n",
      "Best val loss: 0.4808, at epoch 84\n"
     ]
    }
   ],
   "source": [
    "# Run pretraining\n",
    "\n",
    "\n",
    "\n",
    "# Pretraining\n",
    "# Define the device\n",
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu') #\"cpu\"\n",
    "\n",
    "# Create the model\n",
    "model = GAT_4(550, 256,33, 550).to(device) # in_channels is set to 100 as an example. Please replace it with your actual feature size.\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 300\n",
    "patience = 8\n",
    "\n",
    "train(model=model, train_loader=pre_train_loader, val_loader=pre_val_loader, criterion=criterion, num_epochs= num_epochs, patience = patience, optimizer= optimizer,weight_loss = False ,model_path = './models/img6_r30_n1_random_01_GAT4_0.001.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
