{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f84cd1-2543-40a9-b8a0-4202e66d5904",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-27T10:10:31.423492Z",
     "start_time": "2023-07-27T10:10:21.661468Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.nn import GCNConv, Sequential\n",
    "from torch_geometric.data import Data   # Create data containers\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871a2c06-5303-4a71-a2ff-d2b3fde38430",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-27T10:32:34.877682Z",
     "start_time": "2023-07-27T10:32:34.866464Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "\n",
    "    # Constructor\n",
    "    # file_path: path to the .h5ad file\n",
    "    # image_col: column name of the image id\n",
    "    # label_col: column name of the label\n",
    "    # include_label: whether to include the label in the graph\n",
    "    # radius: radius of the ego graph\n",
    "    # node_level: number of node levels to include in the ego graph\n",
    "    # batch_size: batch size for the data loader\n",
    "    # split_percent: tuple of percentages for train, validation, and test sets\n",
    "    def __init__(self, file_path, image_col ,label_col, include_label, radius,node_level, batch_size, split_percent):\n",
    "        self.file_path = file_path\n",
    "        self.image_col = image_col\n",
    "        self.label_col = label_col\n",
    "        self.node_level = node_level\n",
    "        self.include_label = include_label\n",
    "        self.radius = radius\n",
    "        self.batch_size = batch_size\n",
    "        self.split_percent = split_percent\n",
    "\n",
    "    def load_data(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def construct_graph(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def split_data(self, loader):\n",
    "        # Assuming split_percent is a tuple like (0.7, 0.2, 0.1)\n",
    "        train_size = int(self.split_percent[0] * len(loader.dataset))\n",
    "        val_size = int(self.split_percent[1] * len(loader.dataset))\n",
    "        test_size = len(loader.dataset) - train_size - val_size\n",
    "        \n",
    "        print(train_size,val_size,test_size)\n",
    "        \n",
    "        \n",
    "        train_data, val_data, test_data = random_split(loader.dataset, [train_size, val_size, test_size])\n",
    "\n",
    "        # Create data loaders for each set\n",
    "        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=self.batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Ego_net_dataloader(Dataloader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Ego_net_dataloader, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load data from .h5ad file and return a scanpy AnnData object\n",
    "        adata = sc.read(self.file_path)\n",
    "        return adata\n",
    "\n",
    "    def construct_graph(self, adata):\n",
    "        # Constructing graph from coordinates using scanpy's spatial_neighbors function\n",
    "        images = np.unique(adata.obs[self.image_col])\n",
    "        print(images)\n",
    "        sub_g_ensemble = []\n",
    "        for image in images:\n",
    "            sub_adata = adata[adata.obs[self.image_col] == image].copy()\n",
    "            sq.gr.spatial_neighbors(adata=sub_adata, radius=self.radius, key_added=\"adjacency_matrix\", coord_type=\"generic\")\n",
    "            edge_index, _ = from_scipy_sparse_matrix(sub_adata.obsp['adjacency_matrix_connectivities'])\n",
    "\n",
    "            # Create subgraphs for each node\n",
    "            G = nx.Graph()\n",
    "\n",
    "            # Add nodes with features to the graph\n",
    "            print('Adding nodes...')\n",
    "            for i, features in tqdm(enumerate(adata.X.toarray())):\n",
    "                G.add_node(i, features=features)\n",
    "\n",
    "            # Add edges to the graph\n",
    "            print('Adding edges...')\n",
    "            G.add_edges_from(edge_index.t().tolist())\n",
    "\n",
    "            # Create subgraphs for each node of G\n",
    "            print('Creating subgraphs...')\n",
    "            subgraphs = [nx.ego_graph(G, node, radius=self.node_level) for node in tqdm(G.nodes())]\n",
    "\n",
    "            sub_g_dataset = [from_networkx(graph, group_node_attrs=['features']) for graph in tqdm(subgraphs)]\n",
    "\n",
    "            # Extend the ensemble with the new subgraphs\n",
    "            sub_g_ensemble.extend(sub_g_dataset)\n",
    "\n",
    "        loader = DataLoader(sub_g_ensemble, batch_size=32, shuffle=True)\n",
    "        return loader\n",
    "\n",
    "\n",
    "class Full_image_dataloader(Dataloader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Full_image_dataloader, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load data from .h5ad file and return a scanpy AnnData object\n",
    "        adata = sc.read(self.file_path)\n",
    "        return adata\n",
    "\n",
    "    def construct_graph(self, adata):\n",
    "        # Constructing graph from coordinates using scanpy's spatial_neighbors function\n",
    "        images = np.unique(adata.obs[self.image_col])\n",
    "        \n",
    "        graph_dict = {}\n",
    "        for image in tqdm(images, desc=\"Constructing Graphs\"):\n",
    "            sub_adata = adata[adata.obs[self.image_col] == image].copy()\n",
    "            sq.gr.spatial_neighbors(adata=sub_adata, radius=self.radius, key_added=\"adjacency_matrix\", coord_type=\"generic\")\n",
    "            edge_index, _ = from_scipy_sparse_matrix(sub_adata.obsp['adjacency_matrix_connectivities'])\n",
    "\n",
    "            # Construct graph\n",
    "            G = nx.Graph()\n",
    "            # Adding nodes\n",
    "            for i, features in enumerate(sub_adata.X.toarray()):\n",
    "                G.add_node(i, features=features)\n",
    "            # Adding edges\n",
    "            G.add_edges_from(edge_index.t().tolist())\n",
    "            \n",
    "            # Convert networkx graph to PyG format\n",
    "            graph = from_networkx(G)\n",
    "            graph_dict[image] = graph\n",
    "\n",
    "        return graph_dict\n",
    "\n",
    "    def split_data(self, graph_dict):\n",
    "        # split by entire images\n",
    "        images = list(graph_dict.keys())\n",
    "        train_images, test_images = train_test_split(images, test_size=(self.split_percent[1] + self.split_percent[2]), random_state=42)\n",
    "        val_images, test_images = train_test_split(test_images, test_size=self.split_percent[2]/(self.split_percent[1] + self.split_percent[2]), random_state=42)\n",
    "        \n",
    "        train_data = [graph_dict[image] for image in train_images]\n",
    "        val_data = [graph_dict[image] for image in val_images]\n",
    "        test_data = [graph_dict[image] for image in test_images]\n",
    "        \n",
    "        # Create data loaders for each set\n",
    "        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=self.batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7da79b-c64a-47cd-87d5-75a3156a9dea",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-27T10:47:50.305518Z",
     "start_time": "2023-07-27T10:34:41.221010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1199650929' '1199650932' '1199650935' '1199650938' '1199650941'\n",
      " '1199650944']\n",
      "Adding nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240945it [00:00, 345479.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding edges...\n",
      "Creating subgraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240945/240945 [00:12<00:00, 19048.12it/s]\n",
      "  0%|          | 0/240945 [00:00<?, ?it/s]/Users/leopoldendres/opt/miniconda3/envs/spatial_atlas_ssl_env/lib/python3.8/site-packages/torch_geometric/utils/convert.py:249: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343680142/work/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  data[key] = torch.tensor(value)\n",
      "100%|██████████| 240945/240945 [01:41<00:00, 2374.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240945it [00:00, 590633.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding edges...\n",
      "Creating subgraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240945/240945 [00:12<00:00, 19625.11it/s]\n",
      "100%|██████████| 240945/240945 [01:47<00:00, 2243.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240945it [00:00, 620614.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding edges...\n",
      "Creating subgraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240945/240945 [00:11<00:00, 21069.02it/s]\n",
      "100%|██████████| 240945/240945 [01:44<00:00, 2307.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240945it [00:00, 545722.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding edges...\n",
      "Creating subgraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240945/240945 [00:12<00:00, 19706.23it/s]\n",
      "100%|██████████| 240945/240945 [01:52<00:00, 2148.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240945it [00:00, 543060.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding edges...\n",
      "Creating subgraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240945/240945 [00:13<00:00, 18114.74it/s]\n",
      "100%|██████████| 240945/240945 [02:00<00:00, 1991.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240945it [00:00, 550056.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding edges...\n",
      "Creating subgraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240945/240945 [00:19<00:00, 12264.00it/s]\n",
      "100%|██████████| 240945/240945 [02:04<00:00, 1936.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of Ego_net_dataloader\n",
    "\n",
    "#file_path = \"../data/img_119670929.h5ad\"\n",
    "file_path = \"../example_files/subset_6img_atlas_brain.h5ad\"\n",
    "dataloader = Ego_net_dataloader(file_path=file_path, image_col=\"section\", label_col=\"class_id_label\", include_label=False, radius=20,node_level = 1, batch_size=32, split_percent=(0.7, 0.2, 0.1))\n",
    "\n",
    "# Load the data\n",
    "adata = dataloader.load_data()\n",
    "\n",
    "# Construct the graph\n",
    "loader = dataloader.construct_graph(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53ff18a2-1069-45a1-9399-300d82ace804",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18361 5246 2623\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, val_loader = dataloader.split_data(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15dae914-9a93-4a05-b0b6-ef1c2d95bf32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77742 22212 11106\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, val_loader = dataloader.split_data(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc0b836d-13d6-42c4-afc8-4dc0418f21cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 574\n",
      "Validation size: 82\n",
      "Test size: 164\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print out the size of each set to verify\n",
    "print(f\"Train size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation size: {len(val_loader.dataset)}\")\n",
    "print(f\"Test size: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e558a3-3352-44bd-a171-d46a36b9cc86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructing Graphs: 100%|██████████| 6/6 [00:27<00:00,  4.54s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of Ego_net_dataloader\n",
    "\n",
    "#file_path = \"../data/img_119670929.h5ad\"\n",
    "file_path = \"../data/subset_6img_atlas_brain.h5ad\"\n",
    "dataloader = Full_image_dataloader(file_path=file_path, image_col=\"section\", label_col=\"class_id_label\", include_label=False, radius=20,node_level = 1, batch_size=32, split_percent=(0.7, 0.2, 0.1))\n",
    "\n",
    "# Load the data\n",
    "adata = dataloader.load_data()\n",
    "\n",
    "# Construct the graph\n",
    "loader = dataloader.construct_graph(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f1cfa1-5ba7-4bdc-a60d-f0c4233e2a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader, val_loader = dataloader.split_data(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc97c783-1e0a-4941-8964-1cf7ef13d1ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4\n",
      "Validation size: 1\n",
      "Test size: 1\n"
     ]
    }
   ],
   "source": [
    "# Print out the size of each set to verify\n",
    "print(f\"Train size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation size: {len(val_loader.dataset)}\")\n",
    "print(f\"Test size: {len(test_loader.dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
