{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-03T23:41:39.306580Z",
     "start_time": "2023-08-03T23:40:57.219491Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/2_img_dataset.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 17\u001B[0m\n\u001B[1;32m     13\u001B[0m dataset_constructor\u001B[38;5;241m.\u001B[39mload_data()\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Construct the graph\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m#dataset = dataset_constructor.construct_graph()\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../dataset/2_img_dataset.pt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m total_cells \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataset_constructor\u001B[38;5;241m.\u001B[39madata)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(dataset), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof\u001B[39m\u001B[38;5;124m\"\u001B[39m, total_cells)\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/spatial_atlas_ssl_env/lib/python3.8/site-packages/torch/serialization.py:791\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001B[0m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    789\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 791\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m    792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m    793\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m    794\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m    795\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m    796\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/spatial_atlas_ssl_env/lib/python3.8/site-packages/torch/serialization.py:271\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 271\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    273\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/spatial_atlas_ssl_env/lib/python3.8/site-packages/torch/serialization.py:252\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 252\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../dataset/2_img_dataset.pt'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import spatialSSL\n",
    "\n",
    "file_path = \"../example_files/img_119670929_1199650932.h5ad\"\n",
    "\n",
    "# Create the dataloader\n",
    "dataset_constructor = spatialSSL.Dataloader.EgoNetDatasetConstructor(file_path=file_path, image_col=\"section\",\n",
    "                                                                     label_col=\"class_label\", include_label=False,\n",
    "                                                                     radius=20, node_level=3)\n",
    "\n",
    "# Load the data\n",
    "dataset_constructor.load_data()\n",
    "\n",
    "# Construct the graph\n",
    "#dataset = dataset_constructor.construct_graph()\n",
    "dataset = torch.load(\"../dataset/dataset2.pt\")\n",
    "\n",
    "total_cells = len(dataset_constructor.adata)\n",
    "print(len(dataset), \"of\", total_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52528 of 55530\n"
     ]
    }
   ],
   "source": [
    "# Construct the graph\n",
    "#dataset = dataset_constructor.construct_graph()\n",
    "dataset = torch.load(\"../dataset/dataset2.pt\")\n",
    "\n",
    "total_cells = len(dataset_constructor.adata)\n",
    "print(len(dataset), \"of\", total_cells)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T23:42:30.986086Z",
     "start_time": "2023-08-03T23:41:57.550905Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#torch.save(dataset, \"../dataset/dataset2.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T15:23:18.906632Z",
     "start_time": "2023-08-03T15:23:11.347438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, training=True):\n",
    "    model.train(training)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = batch.x.to(device)\n",
    "        edge_index = batch.edge_index.to(device)\n",
    "        y = batch.y.to(device)\n",
    "        out = model(x, edge_index)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        losses.append(loss.item())\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        acc = (out.argmax(dim=1) == y).sum().item() / len(y)\n",
    "        accuracies.append(acc)\n",
    "    return mean(losses), mean(accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.lin11 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.lin11(x).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.lin2(x).softmax(dim=1)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T10:01:48.763383Z",
     "start_time": "2023-08-04T10:01:48.641704Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'out_channels'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m net \u001B[38;5;241m=\u001B[39m \u001B[43mGCN\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m550\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m550\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m550\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#weigths = torch.load(\"../models/best_model.pt\")\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# load model state\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#net.load_state_dict(torch.load(\"../models/best_model.pt\"),  strict=False)\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#weigths\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'out_channels'"
     ]
    }
   ],
   "source": [
    "net = GCN(in_channels=550, hidden_channels=550, nu=550)\n",
    "\n",
    "#weigths = torch.load(\"../models/best_model.pt\")\n",
    "\n",
    "# load model state\n",
    "#net.load_state_dict(torch.load(\"../models/best_model.pt\"),  strict=False)\n",
    "#weigths"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T10:01:50.218555Z",
     "start_time": "2023-08-04T10:01:49.912532Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.bias',\n              tensor([ 1.8485e-01,  5.2149e-02,  2.5553e-01,  1.6856e-01,  2.3748e-01,\n                      -3.8776e-02,  1.3727e-01,  2.3419e-01,  1.7705e-01,  2.0290e-01,\n                       1.7215e-01,  9.0370e-02,  2.4223e-01,  8.7808e-02,  2.3943e-02,\n                       2.1018e-01,  1.6180e-01,  2.3478e-01,  2.4502e-01,  2.8955e-01,\n                       1.5057e-01,  1.2783e-01,  2.5978e-01,  1.2592e-01,  2.7632e-01,\n                       2.6478e-01, -1.7233e-02,  1.4562e-01,  1.9541e-01,  8.0785e-03,\n                       1.6502e-01,  1.5623e-01,  1.5175e-01,  1.0052e-01,  1.9142e-01,\n                       5.7890e-02,  1.3251e-01,  1.6412e-01,  6.0202e-02,  1.8642e-01,\n                       1.5235e-01,  1.6511e-01,  2.1999e-01,  3.1011e-01,  2.2046e-02,\n                       1.9942e-01,  2.5617e-01,  1.4763e-01,  2.0891e-01,  9.1363e-02,\n                       1.9775e-01,  1.7344e-01,  1.4490e-01,  1.8761e-01,  1.9027e-01,\n                       1.5716e-01,  1.4542e-01,  1.3092e-01,  2.1551e-01,  2.2406e-01,\n                       2.2685e-01,  9.9780e-02,  1.2310e-01,  2.2045e-01,  1.6048e-01,\n                       2.3204e-01,  1.6478e-01,  5.6377e-04,  2.4416e-01,  1.5092e-01,\n                       2.4642e-01,  1.4442e-01,  2.2841e-01,  1.6431e-01,  1.1585e-01,\n                       1.6063e-01,  3.3918e-02,  5.0120e-02,  8.1384e-02,  2.8329e-01,\n                      -2.7745e-02,  2.1187e-01,  1.8256e-01,  2.2876e-01,  1.2362e-01,\n                       9.6573e-02,  1.6133e-02,  1.3151e-01,  2.4455e-01,  1.8640e-01,\n                       7.5113e-02,  2.4936e-01,  1.7684e-01,  2.4821e-01,  2.0124e-01,\n                       2.0831e-01,  1.4094e-01,  6.4234e-02,  9.3472e-02,  9.2401e-02,\n                       1.5743e-01,  1.5009e-01, -6.5547e-03,  1.2855e-01,  1.5631e-01,\n                       7.2713e-02,  1.9626e-01,  2.1521e-01,  1.9377e-01,  2.6814e-01,\n                       8.0402e-02, -1.3851e-02,  1.4931e-01,  2.1327e-01,  1.0298e-01,\n                       1.0106e-01,  1.5346e-01,  1.5860e-01,  1.1011e-01,  1.4001e-01,\n                       1.7397e-01,  2.2768e-01,  2.4467e-01,  8.9902e-02,  2.0451e-01,\n                       1.1335e-02,  2.8690e-01,  1.3532e-01, -1.1143e-02,  1.2155e-01,\n                       2.2186e-01,  3.0469e-01,  1.2380e-01,  1.7407e-01,  2.5974e-01,\n                       1.8303e-01,  1.0868e-01,  1.9205e-01,  1.8926e-01,  7.0396e-03,\n                       2.5589e-01,  1.2740e-01,  2.0655e-01,  2.0560e-01,  1.7900e-02,\n                       1.4923e-01,  2.0537e-01,  7.2561e-02,  1.4306e-01,  1.5655e-01,\n                       1.4732e-01,  6.7199e-02,  2.5941e-01,  2.2985e-01,  1.4965e-01,\n                       1.0782e-01,  1.4436e-01,  1.3956e-01,  2.8022e-01,  6.6853e-04,\n                       1.3744e-01,  1.5716e-01,  1.2834e-01,  1.9055e-01,  1.3085e-02,\n                       2.2516e-01,  2.1618e-01,  1.4735e-01,  8.8396e-02,  1.3374e-01,\n                       7.7784e-02,  4.7229e-02,  2.0882e-01,  1.4718e-01,  1.3380e-01,\n                       2.5035e-01,  2.1888e-01,  2.2671e-02,  2.4817e-01,  2.0390e-01,\n                      -9.8238e-03,  1.3420e-01,  2.5132e-01,  2.9965e-01, -4.8892e-02,\n                       1.5182e-01,  1.7745e-01,  1.6768e-01, -5.4926e-04,  1.1216e-01,\n                       1.2404e-01,  2.2863e-01,  1.7228e-01,  2.5350e-01,  1.4478e-01,\n                       1.7821e-01,  1.8011e-01,  2.4736e-01,  2.1987e-01,  1.5184e-01,\n                       2.5122e-01,  2.4771e-01,  6.5260e-02,  8.8111e-02,  1.1653e-01,\n                       1.0705e-01,  1.4875e-01, -2.7144e-02,  2.4277e-01, -9.5776e-03,\n                       2.4832e-01,  1.7463e-01,  2.0603e-01,  1.5268e-01,  1.7037e-01,\n                       4.5385e-02,  3.1866e-02,  2.6076e-01,  2.2558e-01,  1.7380e-01,\n                       1.9398e-01,  2.1543e-01,  1.7802e-01,  2.2567e-01,  1.9093e-01,\n                       1.8593e-01,  7.7149e-02,  5.0428e-02,  2.2112e-01,  2.2040e-01,\n                       7.1806e-02,  2.9001e-02,  6.1762e-02,  1.6894e-01,  6.3069e-02,\n                       2.3122e-01,  2.6386e-01,  1.6374e-01,  9.7900e-02,  2.3801e-01,\n                      -1.3627e-03,  1.0599e-01,  1.4318e-01,  1.9527e-01,  7.9790e-02,\n                       2.4492e-01,  1.9606e-01, -9.0381e-04,  1.1868e-01,  2.8069e-01,\n                       4.2157e-02,  1.6073e-01,  1.6679e-01,  1.3923e-01,  1.3182e-01,\n                       1.7044e-01,  2.3186e-01,  1.9577e-01,  9.2536e-02,  1.5442e-01,\n                       2.0474e-01,  1.6660e-01,  3.0000e-01,  3.3990e-02,  2.4981e-01,\n                       9.8492e-02,  2.0493e-01,  2.2236e-01,  2.3595e-01,  1.8293e-01,\n                       1.1078e-01,  1.5513e-01,  1.1207e-01,  1.0309e-01,  1.9716e-01,\n                       1.3000e-01,  1.1119e-01,  2.7321e-03,  1.5198e-01,  2.1449e-01,\n                       2.0739e-01,  1.4816e-01,  6.2773e-02,  7.9414e-02,  1.6294e-01,\n                       1.6543e-01,  1.7404e-01,  2.4746e-01,  2.0090e-01,  8.0322e-02,\n                       1.6326e-01,  1.7688e-01,  1.0548e-01,  1.5770e-01,  2.1374e-01,\n                       4.7513e-02,  1.2729e-01,  1.6806e-01,  2.7018e-01,  3.0111e-01,\n                       1.6916e-02,  1.8126e-01,  8.7910e-02,  1.7701e-01,  1.5449e-01,\n                       1.6206e-01,  1.7603e-01,  1.4853e-01,  1.6489e-01,  2.4449e-01,\n                       1.6837e-01,  1.4007e-01,  1.8982e-01,  1.9890e-01,  7.1064e-02,\n                       5.7352e-02,  2.5305e-01,  7.9121e-02,  1.4156e-01,  1.0354e-01,\n                       1.6085e-01,  1.4212e-01,  1.4512e-01,  2.5833e-01,  1.4458e-01,\n                       9.6352e-02,  2.1406e-01,  1.5851e-01,  8.1640e-02,  4.4468e-02,\n                       1.0816e-01,  7.0428e-02,  2.0603e-01,  1.3540e-01,  9.0560e-02,\n                       2.7097e-01,  2.3813e-01,  2.0559e-01,  1.6946e-01,  1.2411e-01,\n                       2.3732e-01,  1.9676e-01,  7.6195e-02, -2.9446e-02,  1.6073e-01,\n                       1.2995e-01,  1.2700e-01,  1.3671e-01,  1.9575e-01,  2.2203e-01,\n                       2.0268e-01,  1.7363e-01,  1.0855e-01,  1.5474e-01,  2.2735e-01,\n                       2.2089e-01,  2.5747e-01,  2.2795e-02,  1.4592e-01,  2.1373e-01,\n                       1.1396e-02,  2.5342e-01,  1.2497e-01,  2.9179e-01,  1.9787e-01,\n                       5.5658e-02, -6.8447e-03,  1.3298e-01,  9.0851e-02,  1.4201e-01,\n                       2.6922e-01,  3.1274e-01,  9.6789e-02,  1.8654e-01,  2.3855e-01,\n                       1.8531e-01,  2.1629e-01,  1.1657e-01,  1.4333e-01,  2.0023e-01,\n                       1.6063e-02,  1.5936e-01,  1.8350e-01,  7.1355e-02,  1.0392e-01,\n                       6.3036e-02,  1.8861e-01,  2.0985e-01,  1.6132e-01,  1.9888e-01,\n                       1.8924e-01,  2.3211e-01,  1.8988e-01,  8.4955e-02,  9.5749e-02,\n                       1.2342e-01,  1.3421e-01,  3.0905e-02,  1.6444e-01,  2.3715e-01,\n                       1.2549e-01,  1.5014e-01,  7.9445e-02,  1.8625e-01,  7.9371e-02,\n                       2.3940e-01,  1.5158e-01,  1.7114e-01,  2.9115e-01,  1.8765e-01,\n                       2.1121e-01, -6.6247e-03,  2.4198e-01,  1.6645e-01,  2.6192e-01,\n                       4.1397e-02,  2.9695e-01,  6.8004e-02,  1.6226e-02,  1.3417e-01,\n                       1.5660e-01,  1.8576e-01,  1.7047e-01,  5.8743e-02,  2.5969e-01,\n                       3.0342e-02,  2.0776e-01,  1.2044e-01,  2.5205e-01,  1.6124e-01,\n                       5.0298e-02,  1.6785e-01,  1.0927e-01,  1.0464e-01,  6.6573e-02,\n                       2.0558e-01,  6.7997e-02,  1.3755e-01,  1.2344e-01,  2.7897e-01,\n                       1.5694e-01,  2.7355e-01,  1.7051e-01,  1.9111e-01,  1.4660e-01,\n                       2.6273e-01,  2.0791e-01,  2.8506e-01,  2.3512e-01,  2.1263e-01,\n                       1.4377e-01,  1.5697e-01,  5.5881e-02,  2.5761e-01, -3.0553e-04,\n                       1.9544e-01,  1.8897e-01,  2.2189e-01,  3.0542e-01,  1.7684e-01,\n                       2.2809e-01,  2.0340e-01, -1.9454e-03,  4.2986e-02,  1.2899e-01,\n                       1.1196e-01,  1.3832e-01,  5.3408e-02,  1.8476e-01,  1.6015e-01,\n                       3.4159e-02,  2.2295e-01,  1.0042e-01,  1.7574e-01,  1.2616e-01,\n                       1.9766e-01,  2.3120e-01,  1.9411e-01,  1.4333e-01,  1.7723e-01,\n                       2.4718e-01,  2.9439e-01,  2.2433e-01,  1.0258e-02,  1.2999e-01,\n                       1.5353e-01,  1.0847e-01,  2.4015e-01,  9.8725e-02,  3.1328e-02,\n                       1.2340e-01,  1.8933e-01,  8.3448e-02,  1.9422e-01,  2.0673e-01,\n                       1.1272e-01,  1.9087e-01,  1.0817e-01,  2.1169e-01,  1.0863e-01,\n                       1.6317e-01,  9.1915e-02,  2.1050e-01,  8.7667e-02,  7.4715e-02,\n                       2.3227e-01,  1.4166e-01,  3.0253e-01,  2.0086e-01,  1.4689e-01,\n                       2.1998e-01,  9.7012e-02,  1.4937e-01,  2.9861e-01,  1.1612e-01,\n                       1.4221e-01,  1.4524e-01,  2.2179e-01,  1.3563e-01,  2.3456e-01,\n                       2.1010e-01,  2.8749e-01,  1.9761e-01,  1.2335e-01,  2.4275e-01,\n                       3.6971e-02,  2.4170e-01, -5.8185e-02,  1.1069e-01,  2.0230e-01,\n                       1.5991e-01,  1.4572e-01,  2.8271e-01,  1.4154e-01,  1.6500e-01,\n                       1.5858e-01,  7.3406e-02,  1.9506e-01,  9.3060e-02,  2.1629e-01,\n                       1.9252e-01,  1.7534e-01,  1.4763e-01,  3.0253e-01,  2.1001e-01,\n                       1.8275e-01,  1.6962e-01,  2.4632e-02,  1.5854e-01,  9.9111e-02])),\n             ('conv1.lin.weight',\n              tensor([[ 0.0399,  0.0268, -0.0519,  ..., -0.0570, -0.0697, -0.0198],\n                      [ 0.0569,  0.0345, -0.0008,  ...,  0.0278, -0.0396,  0.1098],\n                      [ 0.0025, -0.0425, -0.0441,  ...,  0.0300,  0.0126, -0.0118],\n                      ...,\n                      [ 0.0250,  0.0095,  0.0885,  ...,  0.0235,  0.0911, -0.0281],\n                      [-0.0422,  0.0630,  0.0354,  ...,  0.0157, -0.0562,  0.0997],\n                      [-0.0115, -0.0652,  0.0391,  ...,  0.0961,  0.0255,  0.0470]])),\n             ('lin1.weight',\n              tensor([[ 0.0054, -0.0245,  0.0248,  ..., -0.0305, -0.0450,  0.0167],\n                      [ 0.0203,  0.0130, -0.0155,  ...,  0.0309, -0.0120,  0.0014],\n                      [ 0.0270,  0.0071,  0.0218,  ..., -0.0016,  0.0641, -0.0310],\n                      ...,\n                      [ 0.0001, -0.0086, -0.0013,  ...,  0.0043,  0.0006,  0.0116],\n                      [ 0.0020,  0.0023,  0.0035,  ...,  0.0080, -0.0013,  0.0034],\n                      [ 0.0003,  0.0091,  0.0035,  ...,  0.0105, -0.0037, -0.0054]])),\n             ('lin1.bias',\n              tensor([ 7.1005e-03, -1.5774e-04,  6.4521e-02,  1.0825e-01,  3.2342e-02,\n                       2.3915e-02,  3.1321e-02,  1.9586e-02,  7.0526e-02,  5.1634e-02,\n                       7.4801e-02,  1.5856e-03,  8.8172e-02,  2.7410e-02,  4.4370e-03,\n                       3.2543e-02,  5.6643e-02,  2.8571e-02, -7.2299e-05,  6.5116e-02,\n                       1.3178e-02,  2.7955e-03,  7.0829e-02,  9.5379e-02,  4.3011e-02,\n                       5.9723e-02, -1.5675e-02,  8.8198e-02,  6.5428e-02,  8.4637e-02,\n                       1.0815e-01,  1.7626e-01,  1.4158e-01, -1.0145e-03,  9.1921e-02,\n                       6.2826e-02,  3.0033e-02,  1.0844e-03,  4.0177e-02,  1.9745e-02,\n                       3.3739e-02,  6.0843e-02,  1.3822e-02,  5.2545e-02,  1.7879e-01,\n                       6.7130e-02,  8.4623e-02,  9.8934e-02,  1.2450e-01,  8.7494e-02,\n                       6.8426e-02,  1.0213e-01,  8.2926e-02,  1.2638e-01, -4.8533e-03,\n                       6.7830e-02,  3.9714e-02,  6.1248e-02,  4.2278e-02,  7.0054e-02,\n                       1.3417e-02,  9.8456e-03,  6.9485e-03,  1.3387e-01,  3.7583e-02,\n                       1.9560e-02,  7.2855e-02,  5.2389e-02,  9.5648e-02,  1.3006e-01,\n                       7.2179e-02,  1.0699e-01,  4.2468e-02,  2.2907e-02,  1.6964e-02,\n                       6.5212e-02,  1.0297e-01,  1.0932e-01,  1.5811e-02,  1.9961e-02,\n                       1.0728e-01,  1.4520e-01,  1.0083e-01, -1.1064e-02,  4.5948e-03,\n                       6.6635e-02,  1.2599e-01,  6.2064e-03,  7.9031e-02,  5.3121e-02,\n                       5.6224e-02,  1.0247e-01,  1.2860e-01,  3.0876e-02,  3.6186e-02,\n                       9.0453e-02,  4.0460e-02,  7.9643e-02,  6.8432e-02,  7.2676e-03,\n                       3.2689e-02,  3.6158e-03,  8.7829e-03,  2.7767e-02,  8.5016e-02,\n                       1.4124e-02, -2.0218e-02,  4.8019e-02,  7.3316e-02, -7.0496e-03,\n                       8.8273e-03,  3.0502e-02,  5.8905e-02,  9.3163e-02, -2.0409e-03,\n                       4.8339e-02, -2.6669e-02,  2.7763e-02,  6.8285e-02,  4.2328e-02,\n                       6.9946e-02,  4.4069e-02,  8.4792e-02,  8.6959e-02,  4.7647e-03,\n                       7.2001e-03,  4.2012e-02,  2.6560e-02,  2.4356e-02,  8.1536e-02,\n                       1.7101e-02,  1.3422e-02,  2.1882e-02,  9.2540e-03, -2.1189e-03,\n                       3.0341e-02,  3.4444e-02,  1.0545e-01, -1.3849e-02,  2.2386e-02,\n                       5.7463e-02,  3.2156e-02, -1.0943e-02, -6.3761e-03,  2.8963e-02,\n                       3.0762e-02,  5.7277e-02,  3.4084e-02,  9.0434e-02,  1.3796e-02,\n                      -1.0150e-02, -3.1245e-02,  2.5650e-03,  8.4970e-02,  1.0114e-02,\n                       2.6730e-02,  6.3526e-02,  7.2126e-02,  8.9261e-02,  1.7417e-02,\n                       3.2578e-02,  8.4578e-03,  4.8359e-02,  3.2796e-02,  1.1776e-02,\n                       4.2887e-02, -3.3832e-03,  8.5710e-02,  5.5065e-02, -4.7329e-03,\n                       8.1066e-03,  3.0400e-02,  4.6880e-02,  9.8175e-02, -1.2383e-02,\n                      -8.8133e-03,  3.6636e-02,  2.7116e-02,  5.2327e-02,  1.0797e-01,\n                       3.1136e-02,  2.4380e-02,  2.9864e-02,  5.1657e-02,  4.1148e-02,\n                       4.4720e-02, -8.0952e-03, -7.1492e-03,  2.3945e-03,  6.0869e-02,\n                       4.8771e-02,  2.2853e-02,  4.1554e-02,  3.5141e-02, -6.6212e-03,\n                       6.4646e-02,  2.3349e-02,  2.0531e-02,  9.5587e-03,  3.9920e-02,\n                       9.2446e-02,  2.0753e-02,  3.9118e-02,  1.0496e-02,  4.1371e-03,\n                      -6.1558e-03,  7.0777e-03,  4.5824e-02,  5.9083e-02,  7.5463e-02,\n                       1.8036e-02,  3.2459e-02,  6.4037e-03,  7.1601e-02,  6.1576e-02,\n                       8.7845e-02,  9.5700e-03,  9.7675e-02,  6.0397e-02,  4.5801e-02,\n                       5.1017e-02,  6.9791e-02,  6.3269e-02,  1.3115e-02,  6.9480e-03,\n                       1.2666e-02,  2.8236e-02,  4.8137e-02,  8.5563e-02,  2.7087e-02,\n                      -8.2860e-03, -5.1361e-03,  2.7147e-02,  3.7014e-02,  7.9769e-03,\n                       2.8408e-02,  8.7227e-03,  3.9483e-02,  8.5783e-02,  5.7463e-02,\n                      -5.6920e-03,  3.6979e-02,  2.3466e-02, -1.7207e-02,  1.7803e-02,\n                       3.1593e-02,  1.7918e-02,  2.2972e-02,  5.9402e-02,  3.4341e-02,\n                       3.1674e-02,  1.0679e-02,  9.2944e-04,  9.4734e-04, -3.3846e-03,\n                       1.4659e-02, -1.1881e-02,  2.8535e-02, -1.2096e-02,  4.4068e-03,\n                      -2.3091e-03, -2.1395e-02,  1.7066e-02, -1.4985e-02,  1.1627e-02,\n                       2.7396e-02,  3.4192e-02,  1.7690e-02,  1.9808e-03,  3.5007e-03,\n                       6.0094e-03,  9.6252e-02,  6.9052e-02,  2.7810e-02,  4.6193e-02,\n                       5.5051e-02,  4.1896e-02,  1.3981e-02, -9.4268e-03,  6.1485e-02,\n                       1.1376e-01,  3.3655e-02,  4.7662e-03,  8.0320e-03,  2.6336e-02,\n                       1.5847e-02, -2.5538e-02,  8.1685e-02, -1.3575e-02, -1.4373e-02,\n                       3.6671e-02,  1.9281e-02, -2.4967e-02,  3.2555e-02, -1.8897e-02,\n                      -3.3027e-02,  7.9526e-02,  3.2261e-02,  2.3468e-03,  2.8618e-03,\n                       4.7266e-02, -2.2239e-02,  8.1038e-03,  3.4181e-02, -2.1629e-02,\n                       3.6511e-02,  7.9015e-02, -2.5515e-03, -1.0515e-02, -2.1589e-02,\n                      -9.0025e-03,  3.4782e-02, -1.2788e-02,  6.2110e-02,  7.6538e-03,\n                       5.7754e-02,  2.8051e-02,  3.2841e-02,  1.8490e-02,  3.8258e-02,\n                       4.1668e-02,  1.9781e-02, -8.2080e-03,  1.0854e-02,  2.6238e-02,\n                       1.8414e-02,  5.0454e-02,  1.5261e-02,  8.6916e-03,  7.9400e-03,\n                       2.5291e-02,  4.0253e-03,  2.1923e-04, -2.5752e-02,  4.2672e-02,\n                       6.2142e-02,  3.5900e-03, -1.5476e-02, -2.8570e-02,  3.3400e-03,\n                       2.1830e-02,  3.3882e-02, -3.3093e-03,  2.1143e-02,  1.0876e-02,\n                       4.7325e-02, -8.0325e-03,  5.4634e-02, -2.7667e-03,  5.6239e-02,\n                       5.2291e-02,  8.0879e-03,  4.9097e-02, -1.3555e-02,  5.4650e-02,\n                       3.7184e-02,  4.6312e-02,  3.7548e-02, -1.6382e-02,  2.4203e-02,\n                       1.1110e-01,  3.1891e-02,  2.3385e-02,  1.4679e-01,  6.0288e-02,\n                       6.0746e-02,  7.5881e-02,  9.4696e-02, -7.5338e-03,  4.7555e-02,\n                       1.7970e-02,  2.5708e-02,  4.9468e-02,  4.1334e-03,  8.1744e-03,\n                       1.4069e-02,  3.4180e-02,  8.8634e-03,  5.0994e-02, -1.6305e-02,\n                      -3.8909e-03,  3.5498e-02,  3.7327e-02,  5.7945e-04, -2.5237e-03,\n                       1.6049e-02,  3.8678e-02,  2.9816e-02, -1.7294e-02,  6.9034e-02,\n                       3.1534e-02, -7.1494e-03,  2.0877e-02,  1.3006e-02,  5.2721e-02,\n                       3.3864e-03,  1.3647e-01,  3.2233e-02,  3.2173e-02, -1.3954e-02,\n                      -1.3950e-02, -1.1712e-02, -2.2847e-02,  1.9769e-02, -2.5221e-02,\n                       2.4192e-02, -3.3107e-03,  6.4434e-03,  4.7105e-02,  4.9574e-02,\n                      -3.7450e-04,  4.4164e-02,  3.8647e-02,  2.4604e-02,  4.3708e-02,\n                       3.3224e-02,  4.3697e-02,  2.6379e-02,  5.2367e-02,  3.4475e-02,\n                      -1.7982e-02,  3.2766e-02,  1.1707e-02,  4.0473e-02, -2.5842e-02,\n                       3.4724e-02,  9.9870e-03, -3.8057e-03, -1.4084e-02,  3.5722e-03,\n                       3.5478e-02,  5.0594e-02,  3.1872e-02, -2.0432e-02,  4.7862e-02,\n                       4.8990e-02, -8.6978e-03,  2.7698e-02,  6.7555e-03,  1.4186e-02,\n                      -1.6816e-02,  5.6678e-02,  2.0556e-02,  3.9375e-02,  1.1426e-01,\n                      -1.4905e-02, -6.8814e-03, -4.1307e-03, -2.2322e-02,  8.6816e-03,\n                       7.8755e-02,  1.3975e-01,  3.3124e-02,  3.1422e-02,  8.6533e-03,\n                      -4.4773e-03, -2.0892e-02,  6.9524e-02, -5.5738e-03,  1.1185e-02,\n                       1.2697e-02,  3.4615e-02,  1.5952e-02,  2.1525e-02,  2.5201e-02,\n                      -1.0566e-02, -9.1406e-03, -5.7862e-03,  1.0105e-02,  1.9061e-02,\n                      -1.4667e-03,  5.2289e-02,  6.2695e-02,  2.0604e-02,  3.4132e-02,\n                       1.8818e-02,  6.1269e-02,  1.3101e-03,  2.6824e-02, -1.8801e-02,\n                       1.0113e-01,  6.2023e-02, -1.1430e-02,  2.3684e-02, -5.2958e-03,\n                       2.2559e-02,  1.7132e-02,  8.6094e-02,  8.0219e-03,  1.6718e-02,\n                      -1.3282e-02,  2.6632e-02,  8.6989e-03,  9.3759e-04,  4.1254e-02,\n                       3.7190e-02, -2.2759e-03, -3.0796e-03, -1.3822e-02,  1.9237e-03,\n                       7.7910e-03,  1.8392e-02,  4.1946e-03, -2.2235e-02,  1.4167e-02,\n                       2.1969e-02, -9.4622e-03, -2.4217e-02,  2.0078e-02,  1.8211e-02,\n                       3.2670e-02, -7.0043e-03,  6.8892e-03, -2.3756e-02,  3.3952e-04,\n                      -1.1687e-02,  1.4528e-02,  3.0780e-02,  3.9705e-03,  3.5723e-02,\n                       1.6759e-02,  2.1029e-02,  1.3736e-02,  2.7744e-02,  4.5304e-02,\n                       2.3603e-02, -2.9128e-02, -2.5507e-02, -5.4489e-03, -2.5541e-02,\n                      -2.3866e-02,  8.1965e-03,  9.0280e-03, -8.2619e-03,  2.4149e-02,\n                      -7.1127e-03,  1.4616e-02, -1.2184e-02, -1.5055e-03,  1.4971e-02,\n                      -2.2341e-02,  2.5975e-02, -3.7818e-03, -1.3764e-02, -8.5748e-03,\n                      -1.7207e-02,  2.3453e-02,  2.2108e-02, -2.4027e-02, -3.0636e-03]))])"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigths"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T12:24:31.189063Z",
     "start_time": "2023-08-03T12:24:31.153315Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.bias',\n              tensor([ 1.8485e-01,  5.2149e-02,  2.5553e-01,  1.6856e-01,  2.3748e-01,\n                      -3.8776e-02,  1.3727e-01,  2.3419e-01,  1.7705e-01,  2.0290e-01,\n                       1.7215e-01,  9.0370e-02,  2.4223e-01,  8.7808e-02,  2.3943e-02,\n                       2.1018e-01,  1.6180e-01,  2.3478e-01,  2.4502e-01,  2.8955e-01,\n                       1.5057e-01,  1.2783e-01,  2.5978e-01,  1.2592e-01,  2.7632e-01,\n                       2.6478e-01, -1.7233e-02,  1.4562e-01,  1.9541e-01,  8.0785e-03,\n                       1.6502e-01,  1.5623e-01,  1.5175e-01,  1.0052e-01,  1.9142e-01,\n                       5.7890e-02,  1.3251e-01,  1.6412e-01,  6.0202e-02,  1.8642e-01,\n                       1.5235e-01,  1.6511e-01,  2.1999e-01,  3.1011e-01,  2.2046e-02,\n                       1.9942e-01,  2.5617e-01,  1.4763e-01,  2.0891e-01,  9.1363e-02,\n                       1.9775e-01,  1.7344e-01,  1.4490e-01,  1.8761e-01,  1.9027e-01,\n                       1.5716e-01,  1.4542e-01,  1.3092e-01,  2.1551e-01,  2.2406e-01,\n                       2.2685e-01,  9.9780e-02,  1.2310e-01,  2.2045e-01,  1.6048e-01,\n                       2.3204e-01,  1.6478e-01,  5.6377e-04,  2.4416e-01,  1.5092e-01,\n                       2.4642e-01,  1.4442e-01,  2.2841e-01,  1.6431e-01,  1.1585e-01,\n                       1.6063e-01,  3.3918e-02,  5.0120e-02,  8.1384e-02,  2.8329e-01,\n                      -2.7745e-02,  2.1187e-01,  1.8256e-01,  2.2876e-01,  1.2362e-01,\n                       9.6573e-02,  1.6133e-02,  1.3151e-01,  2.4455e-01,  1.8640e-01,\n                       7.5113e-02,  2.4936e-01,  1.7684e-01,  2.4821e-01,  2.0124e-01,\n                       2.0831e-01,  1.4094e-01,  6.4234e-02,  9.3472e-02,  9.2401e-02,\n                       1.5743e-01,  1.5009e-01, -6.5547e-03,  1.2855e-01,  1.5631e-01,\n                       7.2713e-02,  1.9626e-01,  2.1521e-01,  1.9377e-01,  2.6814e-01,\n                       8.0402e-02, -1.3851e-02,  1.4931e-01,  2.1327e-01,  1.0298e-01,\n                       1.0106e-01,  1.5346e-01,  1.5860e-01,  1.1011e-01,  1.4001e-01,\n                       1.7397e-01,  2.2768e-01,  2.4467e-01,  8.9902e-02,  2.0451e-01,\n                       1.1335e-02,  2.8690e-01,  1.3532e-01, -1.1143e-02,  1.2155e-01,\n                       2.2186e-01,  3.0469e-01,  1.2380e-01,  1.7407e-01,  2.5974e-01,\n                       1.8303e-01,  1.0868e-01,  1.9205e-01,  1.8926e-01,  7.0396e-03,\n                       2.5589e-01,  1.2740e-01,  2.0655e-01,  2.0560e-01,  1.7900e-02,\n                       1.4923e-01,  2.0537e-01,  7.2561e-02,  1.4306e-01,  1.5655e-01,\n                       1.4732e-01,  6.7199e-02,  2.5941e-01,  2.2985e-01,  1.4965e-01,\n                       1.0782e-01,  1.4436e-01,  1.3956e-01,  2.8022e-01,  6.6853e-04,\n                       1.3744e-01,  1.5716e-01,  1.2834e-01,  1.9055e-01,  1.3085e-02,\n                       2.2516e-01,  2.1618e-01,  1.4735e-01,  8.8396e-02,  1.3374e-01,\n                       7.7784e-02,  4.7229e-02,  2.0882e-01,  1.4718e-01,  1.3380e-01,\n                       2.5035e-01,  2.1888e-01,  2.2671e-02,  2.4817e-01,  2.0390e-01,\n                      -9.8238e-03,  1.3420e-01,  2.5132e-01,  2.9965e-01, -4.8892e-02,\n                       1.5182e-01,  1.7745e-01,  1.6768e-01, -5.4926e-04,  1.1216e-01,\n                       1.2404e-01,  2.2863e-01,  1.7228e-01,  2.5350e-01,  1.4478e-01,\n                       1.7821e-01,  1.8011e-01,  2.4736e-01,  2.1987e-01,  1.5184e-01,\n                       2.5122e-01,  2.4771e-01,  6.5260e-02,  8.8111e-02,  1.1653e-01,\n                       1.0705e-01,  1.4875e-01, -2.7144e-02,  2.4277e-01, -9.5776e-03,\n                       2.4832e-01,  1.7463e-01,  2.0603e-01,  1.5268e-01,  1.7037e-01,\n                       4.5385e-02,  3.1866e-02,  2.6076e-01,  2.2558e-01,  1.7380e-01,\n                       1.9398e-01,  2.1543e-01,  1.7802e-01,  2.2567e-01,  1.9093e-01,\n                       1.8593e-01,  7.7149e-02,  5.0428e-02,  2.2112e-01,  2.2040e-01,\n                       7.1806e-02,  2.9001e-02,  6.1762e-02,  1.6894e-01,  6.3069e-02,\n                       2.3122e-01,  2.6386e-01,  1.6374e-01,  9.7900e-02,  2.3801e-01,\n                      -1.3627e-03,  1.0599e-01,  1.4318e-01,  1.9527e-01,  7.9790e-02,\n                       2.4492e-01,  1.9606e-01, -9.0381e-04,  1.1868e-01,  2.8069e-01,\n                       4.2157e-02,  1.6073e-01,  1.6679e-01,  1.3923e-01,  1.3182e-01,\n                       1.7044e-01,  2.3186e-01,  1.9577e-01,  9.2536e-02,  1.5442e-01,\n                       2.0474e-01,  1.6660e-01,  3.0000e-01,  3.3990e-02,  2.4981e-01,\n                       9.8492e-02,  2.0493e-01,  2.2236e-01,  2.3595e-01,  1.8293e-01,\n                       1.1078e-01,  1.5513e-01,  1.1207e-01,  1.0309e-01,  1.9716e-01,\n                       1.3000e-01,  1.1119e-01,  2.7321e-03,  1.5198e-01,  2.1449e-01,\n                       2.0739e-01,  1.4816e-01,  6.2773e-02,  7.9414e-02,  1.6294e-01,\n                       1.6543e-01,  1.7404e-01,  2.4746e-01,  2.0090e-01,  8.0322e-02,\n                       1.6326e-01,  1.7688e-01,  1.0548e-01,  1.5770e-01,  2.1374e-01,\n                       4.7513e-02,  1.2729e-01,  1.6806e-01,  2.7018e-01,  3.0111e-01,\n                       1.6916e-02,  1.8126e-01,  8.7910e-02,  1.7701e-01,  1.5449e-01,\n                       1.6206e-01,  1.7603e-01,  1.4853e-01,  1.6489e-01,  2.4449e-01,\n                       1.6837e-01,  1.4007e-01,  1.8982e-01,  1.9890e-01,  7.1064e-02,\n                       5.7352e-02,  2.5305e-01,  7.9121e-02,  1.4156e-01,  1.0354e-01,\n                       1.6085e-01,  1.4212e-01,  1.4512e-01,  2.5833e-01,  1.4458e-01,\n                       9.6352e-02,  2.1406e-01,  1.5851e-01,  8.1640e-02,  4.4468e-02,\n                       1.0816e-01,  7.0428e-02,  2.0603e-01,  1.3540e-01,  9.0560e-02,\n                       2.7097e-01,  2.3813e-01,  2.0559e-01,  1.6946e-01,  1.2411e-01,\n                       2.3732e-01,  1.9676e-01,  7.6195e-02, -2.9446e-02,  1.6073e-01,\n                       1.2995e-01,  1.2700e-01,  1.3671e-01,  1.9575e-01,  2.2203e-01,\n                       2.0268e-01,  1.7363e-01,  1.0855e-01,  1.5474e-01,  2.2735e-01,\n                       2.2089e-01,  2.5747e-01,  2.2795e-02,  1.4592e-01,  2.1373e-01,\n                       1.1396e-02,  2.5342e-01,  1.2497e-01,  2.9179e-01,  1.9787e-01,\n                       5.5658e-02, -6.8447e-03,  1.3298e-01,  9.0851e-02,  1.4201e-01,\n                       2.6922e-01,  3.1274e-01,  9.6789e-02,  1.8654e-01,  2.3855e-01,\n                       1.8531e-01,  2.1629e-01,  1.1657e-01,  1.4333e-01,  2.0023e-01,\n                       1.6063e-02,  1.5936e-01,  1.8350e-01,  7.1355e-02,  1.0392e-01,\n                       6.3036e-02,  1.8861e-01,  2.0985e-01,  1.6132e-01,  1.9888e-01,\n                       1.8924e-01,  2.3211e-01,  1.8988e-01,  8.4955e-02,  9.5749e-02,\n                       1.2342e-01,  1.3421e-01,  3.0905e-02,  1.6444e-01,  2.3715e-01,\n                       1.2549e-01,  1.5014e-01,  7.9445e-02,  1.8625e-01,  7.9371e-02,\n                       2.3940e-01,  1.5158e-01,  1.7114e-01,  2.9115e-01,  1.8765e-01,\n                       2.1121e-01, -6.6247e-03,  2.4198e-01,  1.6645e-01,  2.6192e-01,\n                       4.1397e-02,  2.9695e-01,  6.8004e-02,  1.6226e-02,  1.3417e-01,\n                       1.5660e-01,  1.8576e-01,  1.7047e-01,  5.8743e-02,  2.5969e-01,\n                       3.0342e-02,  2.0776e-01,  1.2044e-01,  2.5205e-01,  1.6124e-01,\n                       5.0298e-02,  1.6785e-01,  1.0927e-01,  1.0464e-01,  6.6573e-02,\n                       2.0558e-01,  6.7997e-02,  1.3755e-01,  1.2344e-01,  2.7897e-01,\n                       1.5694e-01,  2.7355e-01,  1.7051e-01,  1.9111e-01,  1.4660e-01,\n                       2.6273e-01,  2.0791e-01,  2.8506e-01,  2.3512e-01,  2.1263e-01,\n                       1.4377e-01,  1.5697e-01,  5.5881e-02,  2.5761e-01, -3.0553e-04,\n                       1.9544e-01,  1.8897e-01,  2.2189e-01,  3.0542e-01,  1.7684e-01,\n                       2.2809e-01,  2.0340e-01, -1.9454e-03,  4.2986e-02,  1.2899e-01,\n                       1.1196e-01,  1.3832e-01,  5.3408e-02,  1.8476e-01,  1.6015e-01,\n                       3.4159e-02,  2.2295e-01,  1.0042e-01,  1.7574e-01,  1.2616e-01,\n                       1.9766e-01,  2.3120e-01,  1.9411e-01,  1.4333e-01,  1.7723e-01,\n                       2.4718e-01,  2.9439e-01,  2.2433e-01,  1.0258e-02,  1.2999e-01,\n                       1.5353e-01,  1.0847e-01,  2.4015e-01,  9.8725e-02,  3.1328e-02,\n                       1.2340e-01,  1.8933e-01,  8.3448e-02,  1.9422e-01,  2.0673e-01,\n                       1.1272e-01,  1.9087e-01,  1.0817e-01,  2.1169e-01,  1.0863e-01,\n                       1.6317e-01,  9.1915e-02,  2.1050e-01,  8.7667e-02,  7.4715e-02,\n                       2.3227e-01,  1.4166e-01,  3.0253e-01,  2.0086e-01,  1.4689e-01,\n                       2.1998e-01,  9.7012e-02,  1.4937e-01,  2.9861e-01,  1.1612e-01,\n                       1.4221e-01,  1.4524e-01,  2.2179e-01,  1.3563e-01,  2.3456e-01,\n                       2.1010e-01,  2.8749e-01,  1.9761e-01,  1.2335e-01,  2.4275e-01,\n                       3.6971e-02,  2.4170e-01, -5.8185e-02,  1.1069e-01,  2.0230e-01,\n                       1.5991e-01,  1.4572e-01,  2.8271e-01,  1.4154e-01,  1.6500e-01,\n                       1.5858e-01,  7.3406e-02,  1.9506e-01,  9.3060e-02,  2.1629e-01,\n                       1.9252e-01,  1.7534e-01,  1.4763e-01,  3.0253e-01,  2.1001e-01,\n                       1.8275e-01,  1.6962e-01,  2.4632e-02,  1.5854e-01,  9.9111e-02])),\n             ('conv1.lin.weight',\n              tensor([[ 0.0399,  0.0268, -0.0519,  ..., -0.0570, -0.0697, -0.0198],\n                      [ 0.0569,  0.0345, -0.0008,  ...,  0.0278, -0.0396,  0.1098],\n                      [ 0.0025, -0.0425, -0.0441,  ...,  0.0300,  0.0126, -0.0118],\n                      ...,\n                      [ 0.0250,  0.0095,  0.0885,  ...,  0.0235,  0.0911, -0.0281],\n                      [-0.0422,  0.0630,  0.0354,  ...,  0.0157, -0.0562,  0.0997],\n                      [-0.0115, -0.0652,  0.0391,  ...,  0.0961,  0.0255,  0.0470]])),\n             ('lin1.weight',\n              tensor([[ 0.0054, -0.0245,  0.0248,  ..., -0.0305, -0.0450,  0.0167],\n                      [ 0.0203,  0.0130, -0.0155,  ...,  0.0309, -0.0120,  0.0014],\n                      [ 0.0270,  0.0071,  0.0218,  ..., -0.0016,  0.0641, -0.0310],\n                      ...,\n                      [ 0.0001, -0.0086, -0.0013,  ...,  0.0043,  0.0006,  0.0116],\n                      [ 0.0020,  0.0023,  0.0035,  ...,  0.0080, -0.0013,  0.0034],\n                      [ 0.0003,  0.0091,  0.0035,  ...,  0.0105, -0.0037, -0.0054]])),\n             ('lin1.bias',\n              tensor([ 7.1005e-03, -1.5774e-04,  6.4521e-02,  1.0825e-01,  3.2342e-02,\n                       2.3915e-02,  3.1321e-02,  1.9586e-02,  7.0526e-02,  5.1634e-02,\n                       7.4801e-02,  1.5856e-03,  8.8172e-02,  2.7410e-02,  4.4370e-03,\n                       3.2543e-02,  5.6643e-02,  2.8571e-02, -7.2299e-05,  6.5116e-02,\n                       1.3178e-02,  2.7955e-03,  7.0829e-02,  9.5379e-02,  4.3011e-02,\n                       5.9723e-02, -1.5675e-02,  8.8198e-02,  6.5428e-02,  8.4637e-02,\n                       1.0815e-01,  1.7626e-01,  1.4158e-01, -1.0145e-03,  9.1921e-02,\n                       6.2826e-02,  3.0033e-02,  1.0844e-03,  4.0177e-02,  1.9745e-02,\n                       3.3739e-02,  6.0843e-02,  1.3822e-02,  5.2545e-02,  1.7879e-01,\n                       6.7130e-02,  8.4623e-02,  9.8934e-02,  1.2450e-01,  8.7494e-02,\n                       6.8426e-02,  1.0213e-01,  8.2926e-02,  1.2638e-01, -4.8533e-03,\n                       6.7830e-02,  3.9714e-02,  6.1248e-02,  4.2278e-02,  7.0054e-02,\n                       1.3417e-02,  9.8456e-03,  6.9485e-03,  1.3387e-01,  3.7583e-02,\n                       1.9560e-02,  7.2855e-02,  5.2389e-02,  9.5648e-02,  1.3006e-01,\n                       7.2179e-02,  1.0699e-01,  4.2468e-02,  2.2907e-02,  1.6964e-02,\n                       6.5212e-02,  1.0297e-01,  1.0932e-01,  1.5811e-02,  1.9961e-02,\n                       1.0728e-01,  1.4520e-01,  1.0083e-01, -1.1064e-02,  4.5948e-03,\n                       6.6635e-02,  1.2599e-01,  6.2064e-03,  7.9031e-02,  5.3121e-02,\n                       5.6224e-02,  1.0247e-01,  1.2860e-01,  3.0876e-02,  3.6186e-02,\n                       9.0453e-02,  4.0460e-02,  7.9643e-02,  6.8432e-02,  7.2676e-03,\n                       3.2689e-02,  3.6158e-03,  8.7829e-03,  2.7767e-02,  8.5016e-02,\n                       1.4124e-02, -2.0218e-02,  4.8019e-02,  7.3316e-02, -7.0496e-03,\n                       8.8273e-03,  3.0502e-02,  5.8905e-02,  9.3163e-02, -2.0409e-03,\n                       4.8339e-02, -2.6669e-02,  2.7763e-02,  6.8285e-02,  4.2328e-02,\n                       6.9946e-02,  4.4069e-02,  8.4792e-02,  8.6959e-02,  4.7647e-03,\n                       7.2001e-03,  4.2012e-02,  2.6560e-02,  2.4356e-02,  8.1536e-02,\n                       1.7101e-02,  1.3422e-02,  2.1882e-02,  9.2540e-03, -2.1189e-03,\n                       3.0341e-02,  3.4444e-02,  1.0545e-01, -1.3849e-02,  2.2386e-02,\n                       5.7463e-02,  3.2156e-02, -1.0943e-02, -6.3761e-03,  2.8963e-02,\n                       3.0762e-02,  5.7277e-02,  3.4084e-02,  9.0434e-02,  1.3796e-02,\n                      -1.0150e-02, -3.1245e-02,  2.5650e-03,  8.4970e-02,  1.0114e-02,\n                       2.6730e-02,  6.3526e-02,  7.2126e-02,  8.9261e-02,  1.7417e-02,\n                       3.2578e-02,  8.4578e-03,  4.8359e-02,  3.2796e-02,  1.1776e-02,\n                       4.2887e-02, -3.3832e-03,  8.5710e-02,  5.5065e-02, -4.7329e-03,\n                       8.1066e-03,  3.0400e-02,  4.6880e-02,  9.8175e-02, -1.2383e-02,\n                      -8.8133e-03,  3.6636e-02,  2.7116e-02,  5.2327e-02,  1.0797e-01,\n                       3.1136e-02,  2.4380e-02,  2.9864e-02,  5.1657e-02,  4.1148e-02,\n                       4.4720e-02, -8.0952e-03, -7.1492e-03,  2.3945e-03,  6.0869e-02,\n                       4.8771e-02,  2.2853e-02,  4.1554e-02,  3.5141e-02, -6.6212e-03,\n                       6.4646e-02,  2.3349e-02,  2.0531e-02,  9.5587e-03,  3.9920e-02,\n                       9.2446e-02,  2.0753e-02,  3.9118e-02,  1.0496e-02,  4.1371e-03,\n                      -6.1558e-03,  7.0777e-03,  4.5824e-02,  5.9083e-02,  7.5463e-02,\n                       1.8036e-02,  3.2459e-02,  6.4037e-03,  7.1601e-02,  6.1576e-02,\n                       8.7845e-02,  9.5700e-03,  9.7675e-02,  6.0397e-02,  4.5801e-02,\n                       5.1017e-02,  6.9791e-02,  6.3269e-02,  1.3115e-02,  6.9480e-03,\n                       1.2666e-02,  2.8236e-02,  4.8137e-02,  8.5563e-02,  2.7087e-02,\n                      -8.2860e-03, -5.1361e-03,  2.7147e-02,  3.7014e-02,  7.9769e-03,\n                       2.8408e-02,  8.7227e-03,  3.9483e-02,  8.5783e-02,  5.7463e-02,\n                      -5.6920e-03,  3.6979e-02,  2.3466e-02, -1.7207e-02,  1.7803e-02,\n                       3.1593e-02,  1.7918e-02,  2.2972e-02,  5.9402e-02,  3.4341e-02,\n                       3.1674e-02,  1.0679e-02,  9.2944e-04,  9.4734e-04, -3.3846e-03,\n                       1.4659e-02, -1.1881e-02,  2.8535e-02, -1.2096e-02,  4.4068e-03,\n                      -2.3091e-03, -2.1395e-02,  1.7066e-02, -1.4985e-02,  1.1627e-02,\n                       2.7396e-02,  3.4192e-02,  1.7690e-02,  1.9808e-03,  3.5007e-03,\n                       6.0094e-03,  9.6252e-02,  6.9052e-02,  2.7810e-02,  4.6193e-02,\n                       5.5051e-02,  4.1896e-02,  1.3981e-02, -9.4268e-03,  6.1485e-02,\n                       1.1376e-01,  3.3655e-02,  4.7662e-03,  8.0320e-03,  2.6336e-02,\n                       1.5847e-02, -2.5538e-02,  8.1685e-02, -1.3575e-02, -1.4373e-02,\n                       3.6671e-02,  1.9281e-02, -2.4967e-02,  3.2555e-02, -1.8897e-02,\n                      -3.3027e-02,  7.9526e-02,  3.2261e-02,  2.3468e-03,  2.8618e-03,\n                       4.7266e-02, -2.2239e-02,  8.1038e-03,  3.4181e-02, -2.1629e-02,\n                       3.6511e-02,  7.9015e-02, -2.5515e-03, -1.0515e-02, -2.1589e-02,\n                      -9.0025e-03,  3.4782e-02, -1.2788e-02,  6.2110e-02,  7.6538e-03,\n                       5.7754e-02,  2.8051e-02,  3.2841e-02,  1.8490e-02,  3.8258e-02,\n                       4.1668e-02,  1.9781e-02, -8.2080e-03,  1.0854e-02,  2.6238e-02,\n                       1.8414e-02,  5.0454e-02,  1.5261e-02,  8.6916e-03,  7.9400e-03,\n                       2.5291e-02,  4.0253e-03,  2.1923e-04, -2.5752e-02,  4.2672e-02,\n                       6.2142e-02,  3.5900e-03, -1.5476e-02, -2.8570e-02,  3.3400e-03,\n                       2.1830e-02,  3.3882e-02, -3.3093e-03,  2.1143e-02,  1.0876e-02,\n                       4.7325e-02, -8.0325e-03,  5.4634e-02, -2.7667e-03,  5.6239e-02,\n                       5.2291e-02,  8.0879e-03,  4.9097e-02, -1.3555e-02,  5.4650e-02,\n                       3.7184e-02,  4.6312e-02,  3.7548e-02, -1.6382e-02,  2.4203e-02,\n                       1.1110e-01,  3.1891e-02,  2.3385e-02,  1.4679e-01,  6.0288e-02,\n                       6.0746e-02,  7.5881e-02,  9.4696e-02, -7.5338e-03,  4.7555e-02,\n                       1.7970e-02,  2.5708e-02,  4.9468e-02,  4.1334e-03,  8.1744e-03,\n                       1.4069e-02,  3.4180e-02,  8.8634e-03,  5.0994e-02, -1.6305e-02,\n                      -3.8909e-03,  3.5498e-02,  3.7327e-02,  5.7945e-04, -2.5237e-03,\n                       1.6049e-02,  3.8678e-02,  2.9816e-02, -1.7294e-02,  6.9034e-02,\n                       3.1534e-02, -7.1494e-03,  2.0877e-02,  1.3006e-02,  5.2721e-02,\n                       3.3864e-03,  1.3647e-01,  3.2233e-02,  3.2173e-02, -1.3954e-02,\n                      -1.3950e-02, -1.1712e-02, -2.2847e-02,  1.9769e-02, -2.5221e-02,\n                       2.4192e-02, -3.3107e-03,  6.4434e-03,  4.7105e-02,  4.9574e-02,\n                      -3.7450e-04,  4.4164e-02,  3.8647e-02,  2.4604e-02,  4.3708e-02,\n                       3.3224e-02,  4.3697e-02,  2.6379e-02,  5.2367e-02,  3.4475e-02,\n                      -1.7982e-02,  3.2766e-02,  1.1707e-02,  4.0473e-02, -2.5842e-02,\n                       3.4724e-02,  9.9870e-03, -3.8057e-03, -1.4084e-02,  3.5722e-03,\n                       3.5478e-02,  5.0594e-02,  3.1872e-02, -2.0432e-02,  4.7862e-02,\n                       4.8990e-02, -8.6978e-03,  2.7698e-02,  6.7555e-03,  1.4186e-02,\n                      -1.6816e-02,  5.6678e-02,  2.0556e-02,  3.9375e-02,  1.1426e-01,\n                      -1.4905e-02, -6.8814e-03, -4.1307e-03, -2.2322e-02,  8.6816e-03,\n                       7.8755e-02,  1.3975e-01,  3.3124e-02,  3.1422e-02,  8.6533e-03,\n                      -4.4773e-03, -2.0892e-02,  6.9524e-02, -5.5738e-03,  1.1185e-02,\n                       1.2697e-02,  3.4615e-02,  1.5952e-02,  2.1525e-02,  2.5201e-02,\n                      -1.0566e-02, -9.1406e-03, -5.7862e-03,  1.0105e-02,  1.9061e-02,\n                      -1.4667e-03,  5.2289e-02,  6.2695e-02,  2.0604e-02,  3.4132e-02,\n                       1.8818e-02,  6.1269e-02,  1.3101e-03,  2.6824e-02, -1.8801e-02,\n                       1.0113e-01,  6.2023e-02, -1.1430e-02,  2.3684e-02, -5.2958e-03,\n                       2.2559e-02,  1.7132e-02,  8.6094e-02,  8.0219e-03,  1.6718e-02,\n                      -1.3282e-02,  2.6632e-02,  8.6989e-03,  9.3759e-04,  4.1254e-02,\n                       3.7190e-02, -2.2759e-03, -3.0796e-03, -1.3822e-02,  1.9237e-03,\n                       7.7910e-03,  1.8392e-02,  4.1946e-03, -2.2235e-02,  1.4167e-02,\n                       2.1969e-02, -9.4622e-03, -2.4217e-02,  2.0078e-02,  1.8211e-02,\n                       3.2670e-02, -7.0043e-03,  6.8892e-03, -2.3756e-02,  3.3952e-04,\n                      -1.1687e-02,  1.4528e-02,  3.0780e-02,  3.9705e-03,  3.5723e-02,\n                       1.6759e-02,  2.1029e-02,  1.3736e-02,  2.7744e-02,  4.5304e-02,\n                       2.3603e-02, -2.9128e-02, -2.5507e-02, -5.4489e-03, -2.5541e-02,\n                      -2.3866e-02,  8.1965e-03,  9.0280e-03, -8.2619e-03,  2.4149e-02,\n                      -7.1127e-03,  1.4616e-02, -1.2184e-02, -1.5055e-03,  1.4971e-02,\n                      -2.2341e-02,  2.5975e-02, -3.7818e-03, -1.3764e-02, -8.5748e-03,\n                      -1.7207e-02,  2.3453e-02,  2.2108e-02, -2.4027e-02, -3.0636e-03])),\n             ('conv2.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('conv2.lin.weight',\n              tensor([[-0.0440,  0.0430, -0.0198,  ..., -0.0621,  0.0248,  0.0242],\n                      [-0.0251,  0.0008, -0.0210,  ..., -0.0629, -0.0195,  0.0599],\n                      [-0.0549, -0.0624,  0.0326,  ..., -0.0178, -0.0376,  0.0737],\n                      ...,\n                      [ 0.0598, -0.0621,  0.0331,  ...,  0.0678, -0.0094,  0.0585],\n                      [ 0.0498, -0.0638,  0.0345,  ...,  0.0605, -0.0251, -0.0573],\n                      [ 0.0194,  0.0516, -0.0053,  ...,  0.0335, -0.0196, -0.0288]])),\n             ('lin2.weight',\n              tensor([[ 0.0322,  0.0280, -0.0018,  ...,  0.0225,  0.0363, -0.0388],\n                      [ 0.0017, -0.0016,  0.0390,  ..., -0.0267,  0.0131, -0.0178],\n                      [-0.0358,  0.0029,  0.0253,  ..., -0.0361,  0.0279,  0.0105],\n                      ...,\n                      [ 0.0321,  0.0185, -0.0093,  ..., -0.0398,  0.0077, -0.0210],\n                      [ 0.0156, -0.0336,  0.0403,  ...,  0.0182, -0.0043,  0.0143],\n                      [-0.0289, -0.0186,  0.0016,  ...,  0.0132,  0.0075, -0.0033]])),\n             ('lin2.bias',\n              tensor([ 0.0027, -0.0186, -0.0258, -0.0065, -0.0247, -0.0403, -0.0168,  0.0010,\n                      -0.0402,  0.0056, -0.0077,  0.0102,  0.0158,  0.0241, -0.0249, -0.0266,\n                       0.0054,  0.0397, -0.0204,  0.0179]))])"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T12:24:12.750801Z",
     "start_time": "2023-08-03T12:24:12.721382Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "['MY GABA', 'MY Glut', 'Astro-Epen', 'Oligo', 'P GABA', ..., 'CNU-HYa GABA', 'MB Glut', 'CNU-HYa Glut', 'TH Glut', 'MB Dopa']\nLength: 20\nCategories (20, object): ['Astro-Epen', 'CB GABA', 'CB Glut', 'CNU-HYa GABA', ..., 'P GABA', 'P Glut', 'TH Glut', 'Vascular']"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_constructor.adata.obs.class_label.unique())#.cat.codes.values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T12:19:38.508047Z",
     "start_time": "2023-08-03T12:19:38.477139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,  3600, 17561])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset:\n",
    "    print(batch.x)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T12:19:38.709555Z",
     "start_time": "2023-08-03T12:19:38.677522Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T15:24:02.928579Z",
     "start_time": "2023-08-03T15:24:02.919963Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1302,  6712,  6727,  ..., 39346, 40173, 50669])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.x)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T12:19:39.524170Z",
     "start_time": "2023-08-03T12:19:39.353033Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc47254144b546c8bcd197066f8ec03c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch import optim\n",
    "from statistics import mean\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = spatialSSL.Utils.split_dataset(dataset=dataset, split_percent=(0.8, 0.1, 0.1), batch_size = 64)\n",
    "\n",
    "epochs = 20\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = GCN(in_channels=550, hidden_channels=550, num_classes=len(dataset_constructor.adata.obs.class_label.unique()))\n",
    "\n",
    "#net.load_state_dict(torch.load(\"../models/full_model.pt\",  map_location=torch.device(device)), strict=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)#, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net.to(device)\n",
    "# Train the model\n",
    "# training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    net.train()\n",
    "\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    train_acc = MulticlassAccuracy()\n",
    "    val_acc = MulticlassAccuracy()\n",
    "\n",
    "    for data in train_loader:\n",
    "        #inputs, _ = data.\n",
    "        input = torch.tensor(dataset_constructor.adata.X[data.x].toarray(), dtype=torch.double).to(device).float()\n",
    "\n",
    "        labels = torch.tensor(dataset_constructor.adata[data.x.numpy()].obs.class_label.cat.codes.values).to(device).long()\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(input, data.edge_index)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        #accuracy = (outputs.argmax(dim=1) == labels).sum().item() / len(labels)\n",
    "        train_acc.update(outputs.argmax(dim=1), labels)\n",
    "        #train_accs.append(accuracy)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_loss = running_loss / len(train_loader)\n",
    "       # Validation\n",
    "    net.eval()\n",
    "    val_accs = []\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for data in val_loader:\n",
    "\n",
    "            input = torch.tensor(dataset_constructor.adata.X[data.x].toarray(), dtype=torch.double).to(device).float()\n",
    "            labels = torch.tensor(dataset_constructor.adata[data.x.numpy()].obs.class_label.cat.codes.values).to(device).long()\n",
    "            outputs = net(input, data.edge_index)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_acc.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "            #accuracy = (outputs.argmax(dim=1) == labels).sum().item() / len(labels)\n",
    "            #val_accs.append(accuracy)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        #val_accuracy = 100 * correct / total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{epochs}, Train Loss: {running_loss:.4f}, Val Loss: {avg_val_loss:.4f} Train Acc: {train_acc.compute():.4f}, Val Acc: {val_acc.compute():.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    #print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.3f}, accuracy: {sum(all_accuracy) / len(all_accuracy):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T10:04:54.184259Z",
     "start_time": "2023-08-04T10:02:03.851900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd10e365ddf44ea1b30654367bb95869"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20, Train Loss: 2.6104, Val Loss: 2.4669 Train Acc: 0.5257, Val Acc: 0.6286\n",
      "Epoch: 2/20, Train Loss: 2.4438, Val Loss: 2.4517 Train Acc: 0.6449, Val Acc: 0.6324\n",
      "Epoch: 3/20, Train Loss: 2.4395, Val Loss: 2.4484 Train Acc: 0.6452, Val Acc: 0.6336\n",
      "Epoch: 4/20, Train Loss: 2.4346, Val Loss: 2.4379 Train Acc: 0.6492, Val Acc: 0.6473\n",
      "Epoch: 5/20, Train Loss: 2.4257, Val Loss: 2.4348 Train Acc: 0.6580, Val Acc: 0.6492\n",
      "Epoch: 6/20, Train Loss: 2.4218, Val Loss: 2.4333 Train Acc: 0.6611, Val Acc: 0.6502\n",
      "Epoch: 7/20, Train Loss: 2.4218, Val Loss: 2.4320 Train Acc: 0.6603, Val Acc: 0.6511\n",
      "Epoch: 8/20, Train Loss: 2.4230, Val Loss: 2.4321 Train Acc: 0.6592, Val Acc: 0.6507\n",
      "Epoch: 9/20, Train Loss: 2.4216, Val Loss: 2.4312 Train Acc: 0.6603, Val Acc: 0.6512\n",
      "Epoch: 10/20, Train Loss: 2.4202, Val Loss: 2.4310 Train Acc: 0.6620, Val Acc: 0.6512\n",
      "Epoch: 11/20, Train Loss: 2.4149, Val Loss: 2.4311 Train Acc: 0.6674, Val Acc: 0.6518\n",
      "Epoch: 12/20, Train Loss: 2.4194, Val Loss: 2.4300 Train Acc: 0.6629, Val Acc: 0.6529\n",
      "Epoch: 13/20, Train Loss: 2.4200, Val Loss: 2.4300 Train Acc: 0.6614, Val Acc: 0.6529\n",
      "Epoch: 14/20, Train Loss: 2.4222, Val Loss: 2.4292 Train Acc: 0.6596, Val Acc: 0.6533\n",
      "Epoch: 15/20, Train Loss: 2.4176, Val Loss: 2.4291 Train Acc: 0.6645, Val Acc: 0.6534\n",
      "Epoch: 16/20, Train Loss: 2.4181, Val Loss: 2.4287 Train Acc: 0.6645, Val Acc: 0.6541\n",
      "Epoch: 17/20, Train Loss: 2.4167, Val Loss: 2.4294 Train Acc: 0.6650, Val Acc: 0.6530\n",
      "Epoch: 18/20, Train Loss: 2.4195, Val Loss: 2.4279 Train Acc: 0.6631, Val Acc: 0.6549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch import optim\n",
    "from statistics import mean\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "epochs = 20\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = GCN(in_channels=550, hidden_channels=550, num_classes=len(dataset_constructor.adata.obs.class_label.unique()))\n",
    "\n",
    "#net.load_state_dict(torch.load(\"../models/full_model.pt\",  map_location=torch.device(device)), strict=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)#, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net.to(device)\n",
    "# Train the model\n",
    "# training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    net.train()\n",
    "\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    train_acc = MulticlassAccuracy()\n",
    "    val_acc = MulticlassAccuracy()\n",
    "\n",
    "    for data in train_loader:\n",
    "        #inputs, _ = data.\n",
    "        input = torch.tensor(dataset_constructor.adata.X[data.x].toarray(), dtype=torch.double).to(device).float()\n",
    "\n",
    "        labels = torch.tensor(dataset_constructor.adata[data.x.numpy()].obs.class_label.cat.codes.values).to(device).long()\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(input, data.edge_index)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        #accuracy = (outputs.argmax(dim=1) == labels).sum().item() / len(labels)\n",
    "        train_acc.update(outputs.argmax(dim=1), labels)\n",
    "        #train_accs.append(accuracy)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_loss = running_loss / len(train_loader)\n",
    "       # Validation\n",
    "    net.eval()\n",
    "    val_accs = []\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for data in val_loader:\n",
    "\n",
    "            input = torch.tensor(dataset_constructor.adata.X[data.x].toarray(), dtype=torch.double).to(device).float()\n",
    "            labels = torch.tensor(dataset_constructor.adata[data.x.numpy()].obs.class_label.cat.codes.values).to(device).long()\n",
    "            outputs = net(input, data.edge_index)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_acc.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "            #accuracy = (outputs.argmax(dim=1) == labels).sum().item() / len(labels)\n",
    "            #val_accs.append(accuracy)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        #val_accuracy = 100 * correct / total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{epochs}, Train Loss: {running_loss:.4f}, Val Loss: {avg_val_loss:.4f} Train Acc: {train_acc.compute():.4f}, Val Acc: {val_acc.compute():.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    #print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.3f}, accuracy: {sum(all_accuracy) / len(all_accuracy):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T17:07:20.318866Z",
     "start_time": "2023-08-03T16:31:28.678488Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, training=True):\n",
    "    model.train(training)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = batch.x.to(device)\n",
    "        edge_index = batch.edge_index.to(device)\n",
    "        y = batch.y.to(device)\n",
    "        out = model(x, edge_index)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        losses.append(loss.item())\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        acc = (out.argmax(dim=1) == y).sum().item() / len(y)\n",
    "        accuracies.append(acc)\n",
    "    return mean(losses), mean(accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
