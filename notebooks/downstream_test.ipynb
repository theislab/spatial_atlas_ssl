{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-12T07:43:34.814115Z",
     "start_time": "2023-08-12T07:42:34.322797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Processing 1 images:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ca4f651e53b40f983f02ca2a2f433f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Processing 26230 nodes:   0%|          | 0/26230 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e28ec780e2c54e0c9d1e92795df3df1c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24778 of 26230\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import spatialSSL\n",
    "\n",
    "file_path = \"../example_files/img_1199670929.h5ad\"\n",
    "\n",
    "# Create the dataloader\n",
    "dataset_constructor = spatialSSL.Dataloader.EgoNetDatasetConstructor(file_path=file_path, image_col=\"section\",\n",
    "                                                                     label_col=\"class_label\", include_label=False,\n",
    "                                                                     radius=20, node_level=3)\n",
    "\n",
    "# Load the data\n",
    "dataset_constructor.load_data()\n",
    "\n",
    "# Construct the graph\n",
    "dataset = dataset_constructor.construct_graph(True)\n",
    "#dataset = torch.load(\"../dataset/dataset2.pt\")\n",
    "\n",
    "total_cells = len(dataset_constructor.adata)\n",
    "print(len(dataset), \"of\", total_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24778 of 26230\n"
     ]
    }
   ],
   "source": [
    "# Construct the graph\n",
    "#dataset = dataset_constructor.construct_graph()\n",
    "#dataset = torch.load(\"../dataset/dataset2.pt\")\n",
    "\n",
    "total_cells = len(dataset_constructor.adata)\n",
    "print(len(dataset), \"of\", total_cells)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T07:43:51.247730Z",
     "start_time": "2023-08-12T07:43:51.235284Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('preconv1.bias',\n              tensor([ 0.1466,  0.8431,  0.1395, -0.4153,  0.5844, -0.3269,  0.2011,  0.2596,\n                      -0.1122, -0.0133,  0.7679, -0.1675,  0.2850,  0.5696,  0.1298, -0.1109,\n                       0.1179,  1.0377, -0.3357, -0.0749, -0.3219, -0.2919, -0.3573, -0.0899,\n                      -0.7089,  0.4092, -0.3035,  0.2246,  0.5494, -0.1565, -0.4105, -0.5618,\n                      -0.3672,  0.2295,  0.3898, -0.0584, -0.0399, -0.1789, -0.3600,  0.3952,\n                      -0.5660, -0.6186, -0.3196, -0.0661,  0.1466, -0.0098, -0.2885,  0.3426,\n                       0.1503, -0.2307, -0.4500, -0.2208, -0.3602,  0.6797,  0.5027, -0.0995,\n                      -0.0468,  0.2296,  0.3180, -0.4975, -0.2046, -0.1110,  0.0871, -0.3174,\n                      -0.5075,  0.0569,  0.4397, -0.1375, -0.6812,  0.2742,  0.1161,  0.2259,\n                       0.1535, -0.2713, -0.0582,  0.0098,  0.5491,  0.0740,  0.9834, -0.7831,\n                       0.1721, -0.8999, -0.0612, -0.1986,  0.0966,  0.0253,  0.3854,  0.3364,\n                       0.5476, -0.1575, -0.1055, -0.0907,  0.6002, -0.4603, -0.1309,  0.4016,\n                       0.0335, -0.0047,  0.4711, -0.2076,  0.5791,  0.0692, -0.3812, -0.3707,\n                      -0.8807, -0.5013,  0.2753, -0.4139,  0.1863,  0.3691,  0.1589,  0.0496,\n                      -0.0447,  0.5985,  1.0728, -0.3913, -0.0508,  0.1272, -0.2655,  0.0207,\n                      -0.1753,  0.8517,  0.7213, -0.4028,  0.5543, -0.5223,  0.4895, -0.1802])),\n             ('preconv1.lin.weight',\n              tensor([[-0.1370, -0.0103, -0.3340,  ..., -0.0289,  0.4123, -0.1240],\n                      [ 0.0541,  0.0309,  0.0179,  ..., -0.0051, -0.1280,  0.0473],\n                      [-0.2776,  0.2851,  0.1078,  ..., -0.0223, -0.0758, -0.0754],\n                      ...,\n                      [-0.1147, -0.1052,  0.2259,  ..., -0.0302, -0.0394,  0.2127],\n                      [-0.3062,  0.2560, -0.5042,  ..., -0.0095, -0.1452,  0.2553],\n                      [-0.0730, -0.2864, -0.1323,  ...,  0.0127, -0.0916,  0.2303]])),\n             ('preconv2.bias',\n              tensor([0.2918, 0.2902, 0.5275, 0.7181, 0.1357, 0.3107, 0.0829, 0.0731, 0.7551,\n                      0.0852, 0.9001, 0.1486, 1.2705, 0.0434, 0.1090, 0.3135, 0.1452, 0.4796,\n                      0.0228, 0.2238, 0.0438, 0.0979, 1.3583, 1.2400, 0.5369, 0.5401, 0.2228,\n                      0.6794, 1.0706, 0.6430, 2.2760, 1.9472, 2.3036, 0.1465, 0.7818, 0.5247,\n                      0.5125, 0.0403, 0.5122, 0.1691, 0.0325, 0.7736, 0.0418, 0.5131, 2.3784,\n                      0.7131, 1.5832, 1.0352, 1.1682, 1.5323, 0.4454, 0.6863, 0.6522, 1.7736,\n                      0.2066, 0.8466, 0.1194, 0.1511, 0.9387, 0.3357, 0.0942, 0.2520, 0.2550,\n                      1.1894, 0.3322, 0.1688, 0.9417, 0.2744, 1.4855, 1.7909, 0.8748, 1.6707,\n                      0.3972, 0.1471, 0.1181, 0.4128, 0.9298, 1.3422, 0.1830, 0.2129, 1.4651,\n                      1.5892, 1.3789, 0.1052, 0.2714, 0.1516, 0.9467, 0.2251, 0.4687, 0.2591,\n                      0.7959, 0.6414, 1.8910, 0.2879, 0.1490, 0.4663, 0.1978, 1.5107, 0.9232,\n                      0.1836, 0.1858, 0.0503, 0.1734, 0.6572, 0.7962, 0.1756, 0.1118, 0.2354,\n                      0.1232, 0.1020, 0.1465, 0.2115, 0.2860, 0.4936, 0.0835, 0.4296, 0.2181,\n                      0.2072, 0.7039, 0.7054, 0.6534, 0.1637, 0.6613, 1.1164, 0.4708, 0.0308,\n                      0.1592, 0.1652, 0.0262, 0.7749, 0.2087, 0.0444, 0.0966, 0.1239, 0.2115,\n                      0.1508, 0.0359, 0.9669, 0.1001, 0.1305, 0.7093, 0.2527, 0.3377, 0.2211,\n                      0.2723, 0.0606, 0.3782, 0.0493, 1.3098, 0.0928, 0.1038, 0.0539, 0.0571,\n                      2.1566, 0.1820, 0.1350, 0.7607, 1.6727, 1.1623, 0.1353, 0.4773, 0.1801,\n                      0.8229, 0.3427, 0.0811, 0.4304, 0.1759, 0.8441, 0.1528, 0.0466, 0.0797,\n                      0.1042, 0.4394, 1.0865, 0.0934, 0.0679, 0.4821, 0.0956, 1.3640, 0.8852,\n                      0.4573, 0.1748, 0.3342, 0.6760, 0.1278, 0.3473, 0.2526, 0.0279, 0.1189,\n                      0.3260, 0.4259, 0.0520, 0.0675, 0.4752, 0.1842, 0.1155, 0.2544, 0.0492,\n                      0.0304, 0.1535, 0.5249, 0.5380, 0.5698, 0.0993, 0.1103, 0.1700, 0.0946,\n                      0.8788, 1.7791, 0.3565, 0.9758, 0.2439, 0.0930, 0.3887, 1.6815, 0.4080,\n                      0.1296, 0.5188, 0.2862, 0.1625, 0.2690, 0.2282, 0.3045, 0.1261, 0.4264,\n                      0.3316, 0.1272, 0.7203, 0.2967, 0.3600, 0.2417, 0.0518, 0.3355, 0.1781,\n                      0.0499, 0.0861, 0.0476, 0.0820, 0.4755, 0.6105, 0.0818, 0.2294, 0.1141,\n                      0.0317, 0.2922, 0.1216, 0.2617, 0.0212, 0.6576, 0.1034, 0.2345, 0.0377,\n                      0.1207, 0.1230, 0.2378, 0.1661, 0.0185, 0.5331, 0.0989, 0.1281, 0.1201,\n                      0.0509, 0.1197, 0.0517, 0.2945, 0.0712, 0.4686, 0.0503, 0.1385, 0.1144,\n                      0.1905, 0.6270, 0.2227, 0.0951, 0.1626, 0.0531, 0.4668, 0.1373, 0.2497,\n                      0.5355, 0.6513, 0.3045, 0.0858, 0.0899, 0.0886, 0.5036, 0.0305, 1.4439,\n                      0.0362, 0.0410, 0.0982, 0.3518, 0.0487, 0.4886, 0.0579, 0.0512, 1.0673,\n                      0.0518, 0.1444, 0.2254, 0.5524, 0.0657, 0.1755, 0.2089, 0.0933, 0.2394,\n                      0.8574, 0.1953, 0.0627, 0.0793, 0.0982, 0.0271, 0.0318, 0.5325, 0.0847,\n                      0.2182, 0.3804, 0.2640, 0.3464, 0.5920, 0.1083, 0.1064, 0.0301, 0.1655,\n                      0.1250, 0.2404, 0.2155, 0.2402, 0.1427, 0.2939, 0.0516, 0.2003, 0.1575,\n                      0.1707, 0.1404, 0.7868, 0.0628, 0.0406, 0.1077, 0.0916, 0.0336, 0.4713,\n                      0.2673, 0.1617, 0.1372, 0.3720, 0.0351, 0.1755, 0.0970, 0.3454, 0.1187,\n                      0.1923, 0.1191, 0.0277, 0.2788, 0.3770, 0.3394, 0.0406, 0.0238, 0.4298,\n                      0.9627, 0.0964, 0.5506, 2.9682, 0.2523, 1.0986, 0.3122, 0.7468, 0.0829,\n                      0.1439, 0.0630, 0.2899, 1.0297, 0.1529, 0.7628, 0.0625, 0.0934, 0.2505,\n                      0.1758, 0.0707, 0.0143, 0.1559, 0.2556, 0.1707, 0.0407, 0.0642, 0.0592,\n                      0.0804, 0.1622, 0.9418, 0.6221, 0.0755, 0.0594, 0.1655, 0.3829, 0.1699,\n                      2.2457, 0.1496, 0.3972, 0.0603, 0.0706, 0.0234, 0.0358, 0.0096, 0.0504,\n                      0.1867, 0.0267, 0.0656, 0.2217, 0.4340, 0.2268, 0.0607, 0.1449, 0.0228,\n                      0.1247, 0.0836, 0.3451, 0.0912, 0.2108, 0.0871, 0.0722, 0.6488, 0.2114,\n                      0.0699, 0.0182, 0.0542, 0.0782, 0.0317, 0.0245, 0.0882, 0.2801, 0.0736,\n                      0.0846, 0.0248, 0.1164, 0.2157, 0.2268, 0.0268, 0.0552, 0.0176, 0.1084,\n                      0.1460, 0.5766, 0.4618, 1.6125, 0.0157, 0.0768, 0.0509, 0.0169, 0.0410,\n                      1.5481, 2.0188, 0.0299, 0.0498, 0.0515, 0.0346, 0.1244, 0.1535, 0.0103,\n                      0.1054, 0.0682, 0.1042, 0.1747, 0.0315, 0.0713, 0.2710, 0.0162, 0.0453,\n                      0.1391, 0.0421, 0.0539, 0.3891, 0.6652, 0.0369, 0.3348, 0.0599, 0.2014,\n                      0.2187, 0.1827, 0.0493, 0.7358, 0.9392, 0.0176, 0.0265, 0.0610, 0.0290,\n                      0.0427, 0.6787, 0.0498, 0.0260, 0.0383, 0.0610, 0.0326, 0.1250, 0.1406,\n                      0.4469, 0.0182, 0.1973, 0.0224, 0.0369, 0.0299, 0.0311, 0.0320, 0.0157,\n                      0.0202, 0.0062, 0.0824, 0.0359, 0.0313, 0.0801, 0.0301, 0.0112, 0.0338,\n                      0.0187, 0.0390, 0.0219, 0.0365, 0.0755, 0.0271, 0.0437, 0.0280, 0.0387,\n                      0.0314, 0.0264, 0.1837, 0.0218, 0.0237, 0.0275, 0.1311, 0.0183, 0.0265,\n                      0.0580, 0.0145, 0.0136, 0.0391, 0.0180, 0.0200, 0.0337, 0.0218, 0.0216,\n                      0.0165, 0.0295, 0.0112, 0.0315, 0.0092, 0.0138, 0.0258, 0.0115, 0.0110,\n                      0.0105])),\n             ('preconv2.lin.weight',\n              tensor([[ 1.6441e-02,  1.5643e-02, -1.0346e-01,  ..., -6.6271e-02,\n                       -4.0811e-02, -5.7839e-02],\n                      [-3.7228e-02, -5.5485e-03,  1.7305e-01,  ..., -4.3752e-02,\n                        6.1687e-02, -5.3897e-03],\n                      [-6.4818e-01,  5.1866e-02,  1.5741e-01,  ...,  4.6438e-01,\n                       -1.8494e-01, -1.7170e-01],\n                      ...,\n                      [-7.5130e-03, -4.4296e-03,  8.0545e-04,  ...,  9.9114e-03,\n                       -2.9549e-03,  7.3155e-04],\n                      [ 6.8917e-03, -5.4499e-03,  2.2593e-04,  ..., -4.8752e-03,\n                        3.2720e-03,  2.8242e-03],\n                      [-9.5753e-03,  3.5971e-04, -4.9864e-03,  ...,  2.5706e-03,\n                        3.8112e-03,  1.1009e-02]]))])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test new Training.py\n",
    "train_loader, val_loader, test_loader = spatialSSL.Utils.split_dataset(dataset=dataset, split_percent=(0.8, 0.1, 0.1), batch_size = 64)\n",
    "\n",
    "\n",
    "pretrain_weights = torch.load(\"/Users/leopoldendres/Documents/Bioinformatik/MasterStudium/spatial_atlas_ssl/models/model_2_20_0.9_GCN.pt\", map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pretrain_weights\n",
    "#torch.save(dataset, \"../dataset/dataset2.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T07:43:54.158405Z",
     "start_time": "2023-08-12T07:43:54.109812Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('preconv1.bias',\n              tensor([ 0.1466,  0.8431,  0.1395, -0.4153,  0.5844, -0.3269,  0.2011,  0.2596,\n                      -0.1122, -0.0133,  0.7679, -0.1675,  0.2850,  0.5696,  0.1298, -0.1109,\n                       0.1179,  1.0377, -0.3357, -0.0749, -0.3219, -0.2919, -0.3573, -0.0899,\n                      -0.7089,  0.4092, -0.3035,  0.2246,  0.5494, -0.1565, -0.4105, -0.5618,\n                      -0.3672,  0.2295,  0.3898, -0.0584, -0.0399, -0.1789, -0.3600,  0.3952,\n                      -0.5660, -0.6186, -0.3196, -0.0661,  0.1466, -0.0098, -0.2885,  0.3426,\n                       0.1503, -0.2307, -0.4500, -0.2208, -0.3602,  0.6797,  0.5027, -0.0995,\n                      -0.0468,  0.2296,  0.3180, -0.4975, -0.2046, -0.1110,  0.0871, -0.3174,\n                      -0.5075,  0.0569,  0.4397, -0.1375, -0.6812,  0.2742,  0.1161,  0.2259,\n                       0.1535, -0.2713, -0.0582,  0.0098,  0.5491,  0.0740,  0.9834, -0.7831,\n                       0.1721, -0.8999, -0.0612, -0.1986,  0.0966,  0.0253,  0.3854,  0.3364,\n                       0.5476, -0.1575, -0.1055, -0.0907,  0.6002, -0.4603, -0.1309,  0.4016,\n                       0.0335, -0.0047,  0.4711, -0.2076,  0.5791,  0.0692, -0.3812, -0.3707,\n                      -0.8807, -0.5013,  0.2753, -0.4139,  0.1863,  0.3691,  0.1589,  0.0496,\n                      -0.0447,  0.5985,  1.0728, -0.3913, -0.0508,  0.1272, -0.2655,  0.0207,\n                      -0.1753,  0.8517,  0.7213, -0.4028,  0.5543, -0.5223,  0.4895, -0.1802])),\n             ('preconv1.lin.weight',\n              tensor([[-0.1370, -0.0103, -0.3340,  ..., -0.0289,  0.4123, -0.1240],\n                      [ 0.0541,  0.0309,  0.0179,  ..., -0.0051, -0.1280,  0.0473],\n                      [-0.2776,  0.2851,  0.1078,  ..., -0.0223, -0.0758, -0.0754],\n                      ...,\n                      [-0.1147, -0.1052,  0.2259,  ..., -0.0302, -0.0394,  0.2127],\n                      [-0.3062,  0.2560, -0.5042,  ..., -0.0095, -0.1452,  0.2553],\n                      [-0.0730, -0.2864, -0.1323,  ...,  0.0127, -0.0916,  0.2303]])),\n             ('preconv2.bias',\n              tensor([0.2918, 0.2902, 0.5275, 0.7181, 0.1357, 0.3107, 0.0829, 0.0731, 0.7551,\n                      0.0852, 0.9001, 0.1486, 1.2705, 0.0434, 0.1090, 0.3135, 0.1452, 0.4796,\n                      0.0228, 0.2238, 0.0438, 0.0979, 1.3583, 1.2400, 0.5369, 0.5401, 0.2228,\n                      0.6794, 1.0706, 0.6430, 2.2760, 1.9472, 2.3036, 0.1465, 0.7818, 0.5247,\n                      0.5125, 0.0403, 0.5122, 0.1691, 0.0325, 0.7736, 0.0418, 0.5131, 2.3784,\n                      0.7131, 1.5832, 1.0352, 1.1682, 1.5323, 0.4454, 0.6863, 0.6522, 1.7736,\n                      0.2066, 0.8466, 0.1194, 0.1511, 0.9387, 0.3357, 0.0942, 0.2520, 0.2550,\n                      1.1894, 0.3322, 0.1688, 0.9417, 0.2744, 1.4855, 1.7909, 0.8748, 1.6707,\n                      0.3972, 0.1471, 0.1181, 0.4128, 0.9298, 1.3422, 0.1830, 0.2129, 1.4651,\n                      1.5892, 1.3789, 0.1052, 0.2714, 0.1516, 0.9467, 0.2251, 0.4687, 0.2591,\n                      0.7959, 0.6414, 1.8910, 0.2879, 0.1490, 0.4663, 0.1978, 1.5107, 0.9232,\n                      0.1836, 0.1858, 0.0503, 0.1734, 0.6572, 0.7962, 0.1756, 0.1118, 0.2354,\n                      0.1232, 0.1020, 0.1465, 0.2115, 0.2860, 0.4936, 0.0835, 0.4296, 0.2181,\n                      0.2072, 0.7039, 0.7054, 0.6534, 0.1637, 0.6613, 1.1164, 0.4708, 0.0308,\n                      0.1592, 0.1652, 0.0262, 0.7749, 0.2087, 0.0444, 0.0966, 0.1239, 0.2115,\n                      0.1508, 0.0359, 0.9669, 0.1001, 0.1305, 0.7093, 0.2527, 0.3377, 0.2211,\n                      0.2723, 0.0606, 0.3782, 0.0493, 1.3098, 0.0928, 0.1038, 0.0539, 0.0571,\n                      2.1566, 0.1820, 0.1350, 0.7607, 1.6727, 1.1623, 0.1353, 0.4773, 0.1801,\n                      0.8229, 0.3427, 0.0811, 0.4304, 0.1759, 0.8441, 0.1528, 0.0466, 0.0797,\n                      0.1042, 0.4394, 1.0865, 0.0934, 0.0679, 0.4821, 0.0956, 1.3640, 0.8852,\n                      0.4573, 0.1748, 0.3342, 0.6760, 0.1278, 0.3473, 0.2526, 0.0279, 0.1189,\n                      0.3260, 0.4259, 0.0520, 0.0675, 0.4752, 0.1842, 0.1155, 0.2544, 0.0492,\n                      0.0304, 0.1535, 0.5249, 0.5380, 0.5698, 0.0993, 0.1103, 0.1700, 0.0946,\n                      0.8788, 1.7791, 0.3565, 0.9758, 0.2439, 0.0930, 0.3887, 1.6815, 0.4080,\n                      0.1296, 0.5188, 0.2862, 0.1625, 0.2690, 0.2282, 0.3045, 0.1261, 0.4264,\n                      0.3316, 0.1272, 0.7203, 0.2967, 0.3600, 0.2417, 0.0518, 0.3355, 0.1781,\n                      0.0499, 0.0861, 0.0476, 0.0820, 0.4755, 0.6105, 0.0818, 0.2294, 0.1141,\n                      0.0317, 0.2922, 0.1216, 0.2617, 0.0212, 0.6576, 0.1034, 0.2345, 0.0377,\n                      0.1207, 0.1230, 0.2378, 0.1661, 0.0185, 0.5331, 0.0989, 0.1281, 0.1201,\n                      0.0509, 0.1197, 0.0517, 0.2945, 0.0712, 0.4686, 0.0503, 0.1385, 0.1144,\n                      0.1905, 0.6270, 0.2227, 0.0951, 0.1626, 0.0531, 0.4668, 0.1373, 0.2497,\n                      0.5355, 0.6513, 0.3045, 0.0858, 0.0899, 0.0886, 0.5036, 0.0305, 1.4439,\n                      0.0362, 0.0410, 0.0982, 0.3518, 0.0487, 0.4886, 0.0579, 0.0512, 1.0673,\n                      0.0518, 0.1444, 0.2254, 0.5524, 0.0657, 0.1755, 0.2089, 0.0933, 0.2394,\n                      0.8574, 0.1953, 0.0627, 0.0793, 0.0982, 0.0271, 0.0318, 0.5325, 0.0847,\n                      0.2182, 0.3804, 0.2640, 0.3464, 0.5920, 0.1083, 0.1064, 0.0301, 0.1655,\n                      0.1250, 0.2404, 0.2155, 0.2402, 0.1427, 0.2939, 0.0516, 0.2003, 0.1575,\n                      0.1707, 0.1404, 0.7868, 0.0628, 0.0406, 0.1077, 0.0916, 0.0336, 0.4713,\n                      0.2673, 0.1617, 0.1372, 0.3720, 0.0351, 0.1755, 0.0970, 0.3454, 0.1187,\n                      0.1923, 0.1191, 0.0277, 0.2788, 0.3770, 0.3394, 0.0406, 0.0238, 0.4298,\n                      0.9627, 0.0964, 0.5506, 2.9682, 0.2523, 1.0986, 0.3122, 0.7468, 0.0829,\n                      0.1439, 0.0630, 0.2899, 1.0297, 0.1529, 0.7628, 0.0625, 0.0934, 0.2505,\n                      0.1758, 0.0707, 0.0143, 0.1559, 0.2556, 0.1707, 0.0407, 0.0642, 0.0592,\n                      0.0804, 0.1622, 0.9418, 0.6221, 0.0755, 0.0594, 0.1655, 0.3829, 0.1699,\n                      2.2457, 0.1496, 0.3972, 0.0603, 0.0706, 0.0234, 0.0358, 0.0096, 0.0504,\n                      0.1867, 0.0267, 0.0656, 0.2217, 0.4340, 0.2268, 0.0607, 0.1449, 0.0228,\n                      0.1247, 0.0836, 0.3451, 0.0912, 0.2108, 0.0871, 0.0722, 0.6488, 0.2114,\n                      0.0699, 0.0182, 0.0542, 0.0782, 0.0317, 0.0245, 0.0882, 0.2801, 0.0736,\n                      0.0846, 0.0248, 0.1164, 0.2157, 0.2268, 0.0268, 0.0552, 0.0176, 0.1084,\n                      0.1460, 0.5766, 0.4618, 1.6125, 0.0157, 0.0768, 0.0509, 0.0169, 0.0410,\n                      1.5481, 2.0188, 0.0299, 0.0498, 0.0515, 0.0346, 0.1244, 0.1535, 0.0103,\n                      0.1054, 0.0682, 0.1042, 0.1747, 0.0315, 0.0713, 0.2710, 0.0162, 0.0453,\n                      0.1391, 0.0421, 0.0539, 0.3891, 0.6652, 0.0369, 0.3348, 0.0599, 0.2014,\n                      0.2187, 0.1827, 0.0493, 0.7358, 0.9392, 0.0176, 0.0265, 0.0610, 0.0290,\n                      0.0427, 0.6787, 0.0498, 0.0260, 0.0383, 0.0610, 0.0326, 0.1250, 0.1406,\n                      0.4469, 0.0182, 0.1973, 0.0224, 0.0369, 0.0299, 0.0311, 0.0320, 0.0157,\n                      0.0202, 0.0062, 0.0824, 0.0359, 0.0313, 0.0801, 0.0301, 0.0112, 0.0338,\n                      0.0187, 0.0390, 0.0219, 0.0365, 0.0755, 0.0271, 0.0437, 0.0280, 0.0387,\n                      0.0314, 0.0264, 0.1837, 0.0218, 0.0237, 0.0275, 0.1311, 0.0183, 0.0265,\n                      0.0580, 0.0145, 0.0136, 0.0391, 0.0180, 0.0200, 0.0337, 0.0218, 0.0216,\n                      0.0165, 0.0295, 0.0112, 0.0315, 0.0092, 0.0138, 0.0258, 0.0115, 0.0110,\n                      0.0105])),\n             ('preconv2.lin.weight',\n              tensor([[ 1.6441e-02,  1.5643e-02, -1.0346e-01,  ..., -6.6271e-02,\n                       -4.0811e-02, -5.7839e-02],\n                      [-3.7228e-02, -5.5485e-03,  1.7305e-01,  ..., -4.3752e-02,\n                        6.1687e-02, -5.3897e-03],\n                      [-6.4818e-01,  5.1866e-02,  1.5741e-01,  ...,  4.6438e-01,\n                       -1.8494e-01, -1.7170e-01],\n                      ...,\n                      [-7.5130e-03, -4.4296e-03,  8.0545e-04,  ...,  9.9114e-03,\n                       -2.9549e-03,  7.3155e-04],\n                      [ 6.8917e-03, -5.4499e-03,  2.2593e-04,  ..., -4.8752e-03,\n                        3.2720e-03,  2.8242e-03],\n                      [-9.5753e-03,  3.5971e-04, -4.9864e-03,  ...,  2.5706e-03,\n                        3.8112e-03,  1.1009e-02]]))])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_weights#['preconv2.bias'].shape#[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T18:56:34.211133Z",
     "start_time": "2023-08-11T18:56:34.193432Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preconv1.bias torch.Size([128])\n",
      "preconv1.lin.weight torch.Size([128, 550])\n",
      "preconv2.bias torch.Size([550])\n",
      "preconv2.lin.weight torch.Size([550, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": "128"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for item in list(pretrain_weights.items()):\n",
    "    print(item[0], item[1].shape)\n",
    "\n",
    "list(pretrain_weights.items())[0][1].shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T18:58:26.172814Z",
     "start_time": "2023-08-11T18:58:26.164349Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model = spatialSSL.TrainModels.GCN_classifier(in_channels=550, hidden_channels=list(pretrain_weights.items())[0][1].shape[0], num_classes=len(dataset_constructor.adata.obs.class_label.unique()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T07:46:43.974896Z",
     "start_time": "2023-08-12T07:46:43.962824Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.1370, -0.0103, -0.3340,  ..., -0.0289,  0.4123, -0.1240],\n        [ 0.0541,  0.0309,  0.0179,  ..., -0.0051, -0.1280,  0.0473],\n        [-0.2776,  0.2851,  0.1078,  ..., -0.0223, -0.0758, -0.0754],\n        ...,\n        [-0.1147, -0.1052,  0.2259,  ..., -0.0302, -0.0394,  0.2127],\n        [-0.3062,  0.2560, -0.5042,  ..., -0.0095, -0.1452,  0.2553],\n        [-0.0730, -0.2864, -0.1323,  ...,  0.0127, -0.0916,  0.2303]],\n       requires_grad=True)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.preconv1.lin.weight = torch.nn.Parameter(pretrain_weights['preconv1.lin.weight'])\n",
    "model.preconv1.lin.weight# = torch.nn.Parameter(pretrain_weights['preconv1.bias'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T07:47:16.037630Z",
     "start_time": "2023-08-12T07:47:16.026264Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "Training model:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d04cc7af7164d0696818a048a6ba576"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, train loss: 2.2243, train r2: 0.6340,  val loss: 2.1900, val r2: 0.6629, Time: 20.24s\n",
      "Epoch 2/100, train loss: 2.1761, train r2: 0.6775,  val loss: 2.1779, val r2: 0.6729, Time: 20.51s\n",
      "Epoch 3/100, train loss: 2.1730, train r2: 0.6800,  val loss: 2.1760, val r2: 0.6748, Time: 21.79s\n",
      "Epoch 4/100, train loss: 2.1674, train r2: 0.6852,  val loss: 2.1781, val r2: 0.6732, Time: 23.25s\n",
      "Epoch 5/100, train loss: 2.1703, train r2: 0.6815,  val loss: 2.1869, val r2: 0.6644, Time: 27.26s\n",
      "Epoch 6/100, train loss: 2.1694, train r2: 0.6835,  val loss: 2.1723, val r2: 0.6793, Time: 36.51s\n",
      "Epoch 7/100, train loss: 2.1664, train r2: 0.6872,  val loss: 2.1800, val r2: 0.6710, Time: 28.60s\n",
      "Epoch 8/100, train loss: 2.1682, train r2: 0.6844,  val loss: 2.1704, val r2: 0.6804, Time: 24.24s\n",
      "Epoch 9/100, train loss: 2.1651, train r2: 0.6866,  val loss: 2.1679, val r2: 0.6831, Time: 25.22s\n",
      "Epoch 10/100, train loss: 2.1647, train r2: 0.6884,  val loss: 2.1634, val r2: 0.6881, Time: 26.98s\n",
      "Epoch 11/100, train loss: 2.1587, train r2: 0.6928,  val loss: 2.1666, val r2: 0.6841, Time: 28.77s\n",
      "Epoch 12/100, train loss: 2.1572, train r2: 0.6948,  val loss: 2.1662, val r2: 0.6845, Time: 32.04s\n",
      "Epoch 13/100, train loss: 2.1556, train r2: 0.6977,  val loss: 2.1628, val r2: 0.6883, Time: 45.71s\n",
      "Epoch 14/100, train loss: 2.1613, train r2: 0.6915,  val loss: 2.1640, val r2: 0.6871, Time: 40.92s\n",
      "Epoch 15/100, train loss: 2.1575, train r2: 0.6950,  val loss: 2.1597, val r2: 0.6911, Time: 23.28s\n",
      "Epoch 16/100, train loss: 2.1547, train r2: 0.6987,  val loss: 2.1610, val r2: 0.6898, Time: 24.14s\n",
      "Epoch 17/100, train loss: 2.1538, train r2: 0.6984,  val loss: 2.1589, val r2: 0.6920, Time: 27.28s\n",
      "Epoch 18/100, train loss: 2.1573, train r2: 0.6954,  val loss: 2.1570, val r2: 0.6934, Time: 26.41s\n",
      "Epoch 19/100, train loss: 2.1533, train r2: 0.6997,  val loss: 2.1575, val r2: 0.6929, Time: 31.17s\n",
      "Epoch 20/100, train loss: 2.1468, train r2: 0.7056,  val loss: 2.1527, val r2: 0.6985, Time: 37.91s\n",
      "Epoch 21/100, train loss: 2.1447, train r2: 0.7078,  val loss: 2.1508, val r2: 0.7002, Time: 27.02s\n",
      "Epoch 22/100, train loss: 2.1449, train r2: 0.7071,  val loss: 2.1523, val r2: 0.6990, Time: 29.46s\n",
      "Epoch 23/100, train loss: 2.1521, train r2: 0.7017,  val loss: 2.1517, val r2: 0.6992, Time: 34.14s\n",
      "Epoch 24/100, train loss: 2.1485, train r2: 0.7047,  val loss: 2.1643, val r2: 0.6875, Time: 32.21s\n",
      "Epoch 25/100, train loss: 2.1406, train r2: 0.7110,  val loss: 2.1689, val r2: 0.6828, Time: 43.70s\n",
      "Epoch 26/100, train loss: 2.1528, train r2: 0.6996,  val loss: 2.1483, val r2: 0.7028, Time: 53.05s\n",
      "Epoch 27/100, train loss: 2.1524, train r2: 0.7005,  val loss: 2.1469, val r2: 0.7041, Time: 41.82s\n",
      "Epoch 28/100, train loss: 2.1424, train r2: 0.7113,  val loss: 2.1449, val r2: 0.7061, Time: 35.34s\n",
      "Epoch 29/100, train loss: 2.1427, train r2: 0.7089,  val loss: 2.1636, val r2: 0.6882, Time: 28.04s\n",
      "Epoch 30/100, train loss: 2.1499, train r2: 0.7028,  val loss: 2.1600, val r2: 0.6921, Time: 28.60s\n",
      "Epoch 31/100, train loss: 2.1384, train r2: 0.7148,  val loss: 2.1570, val r2: 0.6946, Time: 31.41s\n",
      "Epoch 32/100, train loss: 2.1423, train r2: 0.7097,  val loss: 2.1571, val r2: 0.6942, Time: 29.00s\n",
      "Epoch 33/100, train loss: 2.1376, train r2: 0.7140,  val loss: 2.1574, val r2: 0.6945, Time: 29.53s\n",
      "Epoch 34/100, train loss: 2.1478, train r2: 0.7054,  val loss: 2.1444, val r2: 0.7063, Time: 25.75s\n",
      "Epoch 35/100, train loss: 2.1388, train r2: 0.7144,  val loss: 2.1445, val r2: 0.7061, Time: 27.87s\n",
      "Epoch 36/100, train loss: 2.1425, train r2: 0.7110,  val loss: 2.1417, val r2: 0.7088, Time: 40.93s\n",
      "Epoch 37/100, train loss: 2.1373, train r2: 0.7159,  val loss: 2.1402, val r2: 0.7106, Time: 87.89s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = spatialSSL.TrainModels.GCN_classifier(in_channels=550, hidden_channels=list(pretrain_weights.items())[0][1].shape[0], num_classes=len(dataset_constructor.adata.obs.class_label.unique()))\n",
    "\n",
    "# costum layer initialization\n",
    "model.preconv1.weight = torch.nn.Parameter(pretrain_weights['preconv1.lin.weight'])\n",
    "model.preconv1.bias = torch.nn.Parameter(pretrain_weights['preconv1.bias'])\n",
    "\n",
    "\n",
    "#model.load_state_dict(pretrain_weights, strict=False)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "training_summary = spatialSSL.Training.train(model, train_loader, val_loader, optimizer, criterion, 100, 10, model_path = \"../downstream_models/output_test.pt\", gene_expression=dataset_constructor.adata)#, masking_ratio=masking_mode)\n",
    "\n",
    "\n",
    "\n",
    "model\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T19:23:08.308674Z",
     "start_time": "2023-08-11T19:02:07.567540Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                      fov       volume     center_x     center_y        min_x  \\\n1015221640100570419  True   521.489213  1313.267618  2804.310553  1307.762779   \n1015221640101030179  True   434.644809  2337.988188  4469.224479  2332.236809   \n1015221640101040298  True  1301.728682  2753.101886  4534.981912  2744.089138   \n1015221640101040300  True   949.511632  2525.507108  4535.021918  2518.233714   \n1015221640100930227  True  1832.142257  2816.284756  4036.023695  2806.999792   \n...                   ...          ...          ...          ...          ...   \n1015221640100240734  True   739.178047  1348.739592  1433.912120  1342.893678   \n1015221640100710728  True   454.965108  2434.388524  3390.977631  2430.263258   \n1015221640100340405  True  1470.242392   963.977757  1906.023814   955.829571   \n1015221640100350582  True   976.467347  1345.458493  1860.768403  1338.098138   \n1015221640101020209  True   868.087985  1527.135496  4552.843780  1522.236198   \n\n                           min_y        max_x        max_y  barcodeCount  \\\n1015221640100570419  2799.531567  1318.574285  2808.685538          True   \n1015221640101030179  4464.972032  2343.273111  4472.625751          True   \n1015221640101040298  4527.718707  2762.242116  4541.635168          True   \n1015221640101040300  4528.029029  2531.782917  4542.876204          True   \n1015221640100930227  4024.808109  2825.768915  4046.358749          True   \n...                          ...          ...          ...           ...   \n1015221640100240734  1427.961523  1355.041469  1439.032137          True   \n1015221640100710728  3385.330893  2439.302937  3396.104476          True   \n1015221640100340405  1896.588464   972.537121  1914.268653          True   \n1015221640100350582  1854.138736  1353.193144  1866.964757          True   \n1015221640101020209  4544.583262  1531.889825  4560.846774          True   \n\n                     corrected_x  ...        supertype_label  \\\n1015221640100570419  2291.554254  ...          MY Calcb Chol   \n1015221640101030179  4164.954813  ...  SPVI-SPVC Mafa Glut_2   \n1015221640101040298  4335.910947  ...  SPVI-SPVC Mafa Glut_2   \n1015221640101040300  4277.043727  ...  SPVI-SPVC Mafa Glut_2   \n1015221640100930227  3870.307249  ...  SPVI-SPVC Mafa Glut_2   \n...                          ...  ...                    ...   \n1015221640100240734   977.031838  ...          Astro-NT NN_2   \n1015221640100710728  3148.398579  ...          Astro-NT NN_2   \n1015221640100340405  1333.473025  ...          Astro-NT NN_2   \n1015221640100350582  1388.494134  ...          Astro-NT NN_2   \n1015221640101020209  4035.860736  ...                Endo NN   \n\n                    nt_type_combo_label  CCF_landmark CCF_bin_x  CCF_bin_y  \\\n1015221640100570419                Chol           TBD       0.0        0.0   \n1015221640101030179                Glut          SPVC       0.0        0.0   \n1015221640101040298                Glut          SPVC       0.0        0.0   \n1015221640101040300                Glut          SPVC       0.0        0.0   \n1015221640100930227                Glut          SPVC       0.0        0.0   \n...                                 ...           ...       ...        ...   \n1015221640100240734                  NA           TBD       0.0        0.0   \n1015221640100710728                  NA           TBD       0.0        0.0   \n1015221640100340405                  NA           TBD       0.0        0.0   \n1015221640100350582                  NA           TBD       0.0        0.0   \n1015221640101020209                  NA           TBD       0.0        0.0   \n\n                    CCF_bin_z  structures_id CCF_acronym  CCF_analysis  \\\n1015221640100570419       0.0              0          NA            NA   \n1015221640101030179       0.0              0          NA            NA   \n1015221640101040298       0.0              0          NA            NA   \n1015221640101040300       0.0              0          NA            NA   \n1015221640100930227       0.0              0          NA            NA   \n...                       ...            ...         ...           ...   \n1015221640100240734       0.0              0          NA            NA   \n1015221640100710728       0.0              0          NA            NA   \n1015221640100340405       0.0              0          NA            NA   \n1015221640100350582       0.0              0          NA            NA   \n1015221640101020209       0.0              0          NA            NA   \n\n                     CCF_broad  \n1015221640100570419         NA  \n1015221640101030179         NA  \n1015221640101040298         NA  \n1015221640101040300         NA  \n1015221640100930227         NA  \n...                        ...  \n1015221640100240734         NA  \n1015221640100710728         NA  \n1015221640100340405         NA  \n1015221640100350582         NA  \n1015221640101020209         NA  \n\n[26230 rows x 61 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fov</th>\n      <th>volume</th>\n      <th>center_x</th>\n      <th>center_y</th>\n      <th>min_x</th>\n      <th>min_y</th>\n      <th>max_x</th>\n      <th>max_y</th>\n      <th>barcodeCount</th>\n      <th>corrected_x</th>\n      <th>...</th>\n      <th>supertype_label</th>\n      <th>nt_type_combo_label</th>\n      <th>CCF_landmark</th>\n      <th>CCF_bin_x</th>\n      <th>CCF_bin_y</th>\n      <th>CCF_bin_z</th>\n      <th>structures_id</th>\n      <th>CCF_acronym</th>\n      <th>CCF_analysis</th>\n      <th>CCF_broad</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1015221640100570419</th>\n      <td>True</td>\n      <td>521.489213</td>\n      <td>1313.267618</td>\n      <td>2804.310553</td>\n      <td>1307.762779</td>\n      <td>2799.531567</td>\n      <td>1318.574285</td>\n      <td>2808.685538</td>\n      <td>True</td>\n      <td>2291.554254</td>\n      <td>...</td>\n      <td>MY Calcb Chol</td>\n      <td>Chol</td>\n      <td>TBD</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>1015221640101030179</th>\n      <td>True</td>\n      <td>434.644809</td>\n      <td>2337.988188</td>\n      <td>4469.224479</td>\n      <td>2332.236809</td>\n      <td>4464.972032</td>\n      <td>2343.273111</td>\n      <td>4472.625751</td>\n      <td>True</td>\n      <td>4164.954813</td>\n      <td>...</td>\n      <td>SPVI-SPVC Mafa Glut_2</td>\n      <td>Glut</td>\n      <td>SPVC</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>1015221640101040298</th>\n      <td>True</td>\n      <td>1301.728682</td>\n      <td>2753.101886</td>\n      <td>4534.981912</td>\n      <td>2744.089138</td>\n      <td>4527.718707</td>\n      <td>2762.242116</td>\n      <td>4541.635168</td>\n      <td>True</td>\n      <td>4335.910947</td>\n      <td>...</td>\n      <td>SPVI-SPVC Mafa Glut_2</td>\n      <td>Glut</td>\n      <td>SPVC</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>1015221640101040300</th>\n      <td>True</td>\n      <td>949.511632</td>\n      <td>2525.507108</td>\n      <td>4535.021918</td>\n      <td>2518.233714</td>\n      <td>4528.029029</td>\n      <td>2531.782917</td>\n      <td>4542.876204</td>\n      <td>True</td>\n      <td>4277.043727</td>\n      <td>...</td>\n      <td>SPVI-SPVC Mafa Glut_2</td>\n      <td>Glut</td>\n      <td>SPVC</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>1015221640100930227</th>\n      <td>True</td>\n      <td>1832.142257</td>\n      <td>2816.284756</td>\n      <td>4036.023695</td>\n      <td>2806.999792</td>\n      <td>4024.808109</td>\n      <td>2825.768915</td>\n      <td>4046.358749</td>\n      <td>True</td>\n      <td>3870.307249</td>\n      <td>...</td>\n      <td>SPVI-SPVC Mafa Glut_2</td>\n      <td>Glut</td>\n      <td>SPVC</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1015221640100240734</th>\n      <td>True</td>\n      <td>739.178047</td>\n      <td>1348.739592</td>\n      <td>1433.912120</td>\n      <td>1342.893678</td>\n      <td>1427.961523</td>\n      <td>1355.041469</td>\n      <td>1439.032137</td>\n      <td>True</td>\n      <td>977.031838</td>\n      <td>...</td>\n      <td>Astro-NT NN_2</td>\n      <td>NA</td>\n      <td>TBD</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>1015221640100710728</th>\n      <td>True</td>\n      <td>454.965108</td>\n      <td>2434.388524</td>\n      <td>3390.977631</td>\n      <td>2430.263258</td>\n      <td>3385.330893</td>\n      <td>2439.302937</td>\n      <td>3396.104476</td>\n      <td>True</td>\n      <td>3148.398579</td>\n      <td>...</td>\n      <td>Astro-NT NN_2</td>\n      <td>NA</td>\n      <td>TBD</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>1015221640100340405</th>\n      <td>True</td>\n      <td>1470.242392</td>\n      <td>963.977757</td>\n      <td>1906.023814</td>\n      <td>955.829571</td>\n      <td>1896.588464</td>\n      <td>972.537121</td>\n      <td>1914.268653</td>\n      <td>True</td>\n      <td>1333.473025</td>\n      <td>...</td>\n      <td>Astro-NT NN_2</td>\n      <td>NA</td>\n      <td>TBD</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>1015221640100350582</th>\n      <td>True</td>\n      <td>976.467347</td>\n      <td>1345.458493</td>\n      <td>1860.768403</td>\n      <td>1338.098138</td>\n      <td>1854.138736</td>\n      <td>1353.193144</td>\n      <td>1866.964757</td>\n      <td>True</td>\n      <td>1388.494134</td>\n      <td>...</td>\n      <td>Astro-NT NN_2</td>\n      <td>NA</td>\n      <td>TBD</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>1015221640101020209</th>\n      <td>True</td>\n      <td>868.087985</td>\n      <td>1527.135496</td>\n      <td>4552.843780</td>\n      <td>1522.236198</td>\n      <td>4544.583262</td>\n      <td>1531.889825</td>\n      <td>4560.846774</td>\n      <td>True</td>\n      <td>4035.860736</td>\n      <td>...</td>\n      <td>Endo NN</td>\n      <td>NA</td>\n      <td>TBD</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n  </tbody>\n</table>\n<p>26230 rows Ã— 61 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_constructor.adata.obs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T18:34:27.080506Z",
     "start_time": "2023-08-11T18:34:27.023834Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, training=True):\n",
    "    model.train(training)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = batch.x.to(device)\n",
    "        edge_index = batch.edge_index.to(device)\n",
    "        y = batch.y.to(device)\n",
    "        out = model(x, edge_index)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        losses.append(loss.item())\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        acc = (out.argmax(dim=1) == y).sum().item() / len(y)\n",
    "        accuracies.append(acc)\n",
    "    return mean(losses), mean(accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T10:01:48.763383Z",
     "start_time": "2023-08-04T10:01:48.641704Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'out_channels'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m net \u001B[38;5;241m=\u001B[39m \u001B[43mGCN\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m550\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m550\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m550\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#weigths = torch.load(\"../models/best_model.pt\")\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# load model state\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#net.load_state_dict(torch.load(\"../models/best_model.pt\"),  strict=False)\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#weigths\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'out_channels'"
     ]
    }
   ],
   "source": [
    "net = GCN(in_channels=550, hidden_channels=550, nu=550)\n",
    "\n",
    "#weigths = torch.load(\"../models/best_model.pt\")\n",
    "\n",
    "# load model state\n",
    "#net.load_state_dict(torch.load(\"../models/best_model.pt\"),  strict=False)\n",
    "#weigths"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T10:01:50.218555Z",
     "start_time": "2023-08-04T10:01:49.912532Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.bias',\n              tensor([ 1.8485e-01,  5.2149e-02,  2.5553e-01,  1.6856e-01,  2.3748e-01,\n                      -3.8776e-02,  1.3727e-01,  2.3419e-01,  1.7705e-01,  2.0290e-01,\n                       1.7215e-01,  9.0370e-02,  2.4223e-01,  8.7808e-02,  2.3943e-02,\n                       2.1018e-01,  1.6180e-01,  2.3478e-01,  2.4502e-01,  2.8955e-01,\n                       1.5057e-01,  1.2783e-01,  2.5978e-01,  1.2592e-01,  2.7632e-01,\n                       2.6478e-01, -1.7233e-02,  1.4562e-01,  1.9541e-01,  8.0785e-03,\n                       1.6502e-01,  1.5623e-01,  1.5175e-01,  1.0052e-01,  1.9142e-01,\n                       5.7890e-02,  1.3251e-01,  1.6412e-01,  6.0202e-02,  1.8642e-01,\n                       1.5235e-01,  1.6511e-01,  2.1999e-01,  3.1011e-01,  2.2046e-02,\n                       1.9942e-01,  2.5617e-01,  1.4763e-01,  2.0891e-01,  9.1363e-02,\n                       1.9775e-01,  1.7344e-01,  1.4490e-01,  1.8761e-01,  1.9027e-01,\n                       1.5716e-01,  1.4542e-01,  1.3092e-01,  2.1551e-01,  2.2406e-01,\n                       2.2685e-01,  9.9780e-02,  1.2310e-01,  2.2045e-01,  1.6048e-01,\n                       2.3204e-01,  1.6478e-01,  5.6377e-04,  2.4416e-01,  1.5092e-01,\n                       2.4642e-01,  1.4442e-01,  2.2841e-01,  1.6431e-01,  1.1585e-01,\n                       1.6063e-01,  3.3918e-02,  5.0120e-02,  8.1384e-02,  2.8329e-01,\n                      -2.7745e-02,  2.1187e-01,  1.8256e-01,  2.2876e-01,  1.2362e-01,\n                       9.6573e-02,  1.6133e-02,  1.3151e-01,  2.4455e-01,  1.8640e-01,\n                       7.5113e-02,  2.4936e-01,  1.7684e-01,  2.4821e-01,  2.0124e-01,\n                       2.0831e-01,  1.4094e-01,  6.4234e-02,  9.3472e-02,  9.2401e-02,\n                       1.5743e-01,  1.5009e-01, -6.5547e-03,  1.2855e-01,  1.5631e-01,\n                       7.2713e-02,  1.9626e-01,  2.1521e-01,  1.9377e-01,  2.6814e-01,\n                       8.0402e-02, -1.3851e-02,  1.4931e-01,  2.1327e-01,  1.0298e-01,\n                       1.0106e-01,  1.5346e-01,  1.5860e-01,  1.1011e-01,  1.4001e-01,\n                       1.7397e-01,  2.2768e-01,  2.4467e-01,  8.9902e-02,  2.0451e-01,\n                       1.1335e-02,  2.8690e-01,  1.3532e-01, -1.1143e-02,  1.2155e-01,\n                       2.2186e-01,  3.0469e-01,  1.2380e-01,  1.7407e-01,  2.5974e-01,\n                       1.8303e-01,  1.0868e-01,  1.9205e-01,  1.8926e-01,  7.0396e-03,\n                       2.5589e-01,  1.2740e-01,  2.0655e-01,  2.0560e-01,  1.7900e-02,\n                       1.4923e-01,  2.0537e-01,  7.2561e-02,  1.4306e-01,  1.5655e-01,\n                       1.4732e-01,  6.7199e-02,  2.5941e-01,  2.2985e-01,  1.4965e-01,\n                       1.0782e-01,  1.4436e-01,  1.3956e-01,  2.8022e-01,  6.6853e-04,\n                       1.3744e-01,  1.5716e-01,  1.2834e-01,  1.9055e-01,  1.3085e-02,\n                       2.2516e-01,  2.1618e-01,  1.4735e-01,  8.8396e-02,  1.3374e-01,\n                       7.7784e-02,  4.7229e-02,  2.0882e-01,  1.4718e-01,  1.3380e-01,\n                       2.5035e-01,  2.1888e-01,  2.2671e-02,  2.4817e-01,  2.0390e-01,\n                      -9.8238e-03,  1.3420e-01,  2.5132e-01,  2.9965e-01, -4.8892e-02,\n                       1.5182e-01,  1.7745e-01,  1.6768e-01, -5.4926e-04,  1.1216e-01,\n                       1.2404e-01,  2.2863e-01,  1.7228e-01,  2.5350e-01,  1.4478e-01,\n                       1.7821e-01,  1.8011e-01,  2.4736e-01,  2.1987e-01,  1.5184e-01,\n                       2.5122e-01,  2.4771e-01,  6.5260e-02,  8.8111e-02,  1.1653e-01,\n                       1.0705e-01,  1.4875e-01, -2.7144e-02,  2.4277e-01, -9.5776e-03,\n                       2.4832e-01,  1.7463e-01,  2.0603e-01,  1.5268e-01,  1.7037e-01,\n                       4.5385e-02,  3.1866e-02,  2.6076e-01,  2.2558e-01,  1.7380e-01,\n                       1.9398e-01,  2.1543e-01,  1.7802e-01,  2.2567e-01,  1.9093e-01,\n                       1.8593e-01,  7.7149e-02,  5.0428e-02,  2.2112e-01,  2.2040e-01,\n                       7.1806e-02,  2.9001e-02,  6.1762e-02,  1.6894e-01,  6.3069e-02,\n                       2.3122e-01,  2.6386e-01,  1.6374e-01,  9.7900e-02,  2.3801e-01,\n                      -1.3627e-03,  1.0599e-01,  1.4318e-01,  1.9527e-01,  7.9790e-02,\n                       2.4492e-01,  1.9606e-01, -9.0381e-04,  1.1868e-01,  2.8069e-01,\n                       4.2157e-02,  1.6073e-01,  1.6679e-01,  1.3923e-01,  1.3182e-01,\n                       1.7044e-01,  2.3186e-01,  1.9577e-01,  9.2536e-02,  1.5442e-01,\n                       2.0474e-01,  1.6660e-01,  3.0000e-01,  3.3990e-02,  2.4981e-01,\n                       9.8492e-02,  2.0493e-01,  2.2236e-01,  2.3595e-01,  1.8293e-01,\n                       1.1078e-01,  1.5513e-01,  1.1207e-01,  1.0309e-01,  1.9716e-01,\n                       1.3000e-01,  1.1119e-01,  2.7321e-03,  1.5198e-01,  2.1449e-01,\n                       2.0739e-01,  1.4816e-01,  6.2773e-02,  7.9414e-02,  1.6294e-01,\n                       1.6543e-01,  1.7404e-01,  2.4746e-01,  2.0090e-01,  8.0322e-02,\n                       1.6326e-01,  1.7688e-01,  1.0548e-01,  1.5770e-01,  2.1374e-01,\n                       4.7513e-02,  1.2729e-01,  1.6806e-01,  2.7018e-01,  3.0111e-01,\n                       1.6916e-02,  1.8126e-01,  8.7910e-02,  1.7701e-01,  1.5449e-01,\n                       1.6206e-01,  1.7603e-01,  1.4853e-01,  1.6489e-01,  2.4449e-01,\n                       1.6837e-01,  1.4007e-01,  1.8982e-01,  1.9890e-01,  7.1064e-02,\n                       5.7352e-02,  2.5305e-01,  7.9121e-02,  1.4156e-01,  1.0354e-01,\n                       1.6085e-01,  1.4212e-01,  1.4512e-01,  2.5833e-01,  1.4458e-01,\n                       9.6352e-02,  2.1406e-01,  1.5851e-01,  8.1640e-02,  4.4468e-02,\n                       1.0816e-01,  7.0428e-02,  2.0603e-01,  1.3540e-01,  9.0560e-02,\n                       2.7097e-01,  2.3813e-01,  2.0559e-01,  1.6946e-01,  1.2411e-01,\n                       2.3732e-01,  1.9676e-01,  7.6195e-02, -2.9446e-02,  1.6073e-01,\n                       1.2995e-01,  1.2700e-01,  1.3671e-01,  1.9575e-01,  2.2203e-01,\n                       2.0268e-01,  1.7363e-01,  1.0855e-01,  1.5474e-01,  2.2735e-01,\n                       2.2089e-01,  2.5747e-01,  2.2795e-02,  1.4592e-01,  2.1373e-01,\n                       1.1396e-02,  2.5342e-01,  1.2497e-01,  2.9179e-01,  1.9787e-01,\n                       5.5658e-02, -6.8447e-03,  1.3298e-01,  9.0851e-02,  1.4201e-01,\n                       2.6922e-01,  3.1274e-01,  9.6789e-02,  1.8654e-01,  2.3855e-01,\n                       1.8531e-01,  2.1629e-01,  1.1657e-01,  1.4333e-01,  2.0023e-01,\n                       1.6063e-02,  1.5936e-01,  1.8350e-01,  7.1355e-02,  1.0392e-01,\n                       6.3036e-02,  1.8861e-01,  2.0985e-01,  1.6132e-01,  1.9888e-01,\n                       1.8924e-01,  2.3211e-01,  1.8988e-01,  8.4955e-02,  9.5749e-02,\n                       1.2342e-01,  1.3421e-01,  3.0905e-02,  1.6444e-01,  2.3715e-01,\n                       1.2549e-01,  1.5014e-01,  7.9445e-02,  1.8625e-01,  7.9371e-02,\n                       2.3940e-01,  1.5158e-01,  1.7114e-01,  2.9115e-01,  1.8765e-01,\n                       2.1121e-01, -6.6247e-03,  2.4198e-01,  1.6645e-01,  2.6192e-01,\n                       4.1397e-02,  2.9695e-01,  6.8004e-02,  1.6226e-02,  1.3417e-01,\n                       1.5660e-01,  1.8576e-01,  1.7047e-01,  5.8743e-02,  2.5969e-01,\n                       3.0342e-02,  2.0776e-01,  1.2044e-01,  2.5205e-01,  1.6124e-01,\n                       5.0298e-02,  1.6785e-01,  1.0927e-01,  1.0464e-01,  6.6573e-02,\n                       2.0558e-01,  6.7997e-02,  1.3755e-01,  1.2344e-01,  2.7897e-01,\n                       1.5694e-01,  2.7355e-01,  1.7051e-01,  1.9111e-01,  1.4660e-01,\n                       2.6273e-01,  2.0791e-01,  2.8506e-01,  2.3512e-01,  2.1263e-01,\n                       1.4377e-01,  1.5697e-01,  5.5881e-02,  2.5761e-01, -3.0553e-04,\n                       1.9544e-01,  1.8897e-01,  2.2189e-01,  3.0542e-01,  1.7684e-01,\n                       2.2809e-01,  2.0340e-01, -1.9454e-03,  4.2986e-02,  1.2899e-01,\n                       1.1196e-01,  1.3832e-01,  5.3408e-02,  1.8476e-01,  1.6015e-01,\n                       3.4159e-02,  2.2295e-01,  1.0042e-01,  1.7574e-01,  1.2616e-01,\n                       1.9766e-01,  2.3120e-01,  1.9411e-01,  1.4333e-01,  1.7723e-01,\n                       2.4718e-01,  2.9439e-01,  2.2433e-01,  1.0258e-02,  1.2999e-01,\n                       1.5353e-01,  1.0847e-01,  2.4015e-01,  9.8725e-02,  3.1328e-02,\n                       1.2340e-01,  1.8933e-01,  8.3448e-02,  1.9422e-01,  2.0673e-01,\n                       1.1272e-01,  1.9087e-01,  1.0817e-01,  2.1169e-01,  1.0863e-01,\n                       1.6317e-01,  9.1915e-02,  2.1050e-01,  8.7667e-02,  7.4715e-02,\n                       2.3227e-01,  1.4166e-01,  3.0253e-01,  2.0086e-01,  1.4689e-01,\n                       2.1998e-01,  9.7012e-02,  1.4937e-01,  2.9861e-01,  1.1612e-01,\n                       1.4221e-01,  1.4524e-01,  2.2179e-01,  1.3563e-01,  2.3456e-01,\n                       2.1010e-01,  2.8749e-01,  1.9761e-01,  1.2335e-01,  2.4275e-01,\n                       3.6971e-02,  2.4170e-01, -5.8185e-02,  1.1069e-01,  2.0230e-01,\n                       1.5991e-01,  1.4572e-01,  2.8271e-01,  1.4154e-01,  1.6500e-01,\n                       1.5858e-01,  7.3406e-02,  1.9506e-01,  9.3060e-02,  2.1629e-01,\n                       1.9252e-01,  1.7534e-01,  1.4763e-01,  3.0253e-01,  2.1001e-01,\n                       1.8275e-01,  1.6962e-01,  2.4632e-02,  1.5854e-01,  9.9111e-02])),\n             ('conv1.lin.weight',\n              tensor([[ 0.0399,  0.0268, -0.0519,  ..., -0.0570, -0.0697, -0.0198],\n                      [ 0.0569,  0.0345, -0.0008,  ...,  0.0278, -0.0396,  0.1098],\n                      [ 0.0025, -0.0425, -0.0441,  ...,  0.0300,  0.0126, -0.0118],\n                      ...,\n                      [ 0.0250,  0.0095,  0.0885,  ...,  0.0235,  0.0911, -0.0281],\n                      [-0.0422,  0.0630,  0.0354,  ...,  0.0157, -0.0562,  0.0997],\n                      [-0.0115, -0.0652,  0.0391,  ...,  0.0961,  0.0255,  0.0470]])),\n             ('lin1.weight',\n              tensor([[ 0.0054, -0.0245,  0.0248,  ..., -0.0305, -0.0450,  0.0167],\n                      [ 0.0203,  0.0130, -0.0155,  ...,  0.0309, -0.0120,  0.0014],\n                      [ 0.0270,  0.0071,  0.0218,  ..., -0.0016,  0.0641, -0.0310],\n                      ...,\n                      [ 0.0001, -0.0086, -0.0013,  ...,  0.0043,  0.0006,  0.0116],\n                      [ 0.0020,  0.0023,  0.0035,  ...,  0.0080, -0.0013,  0.0034],\n                      [ 0.0003,  0.0091,  0.0035,  ...,  0.0105, -0.0037, -0.0054]])),\n             ('lin1.bias',\n              tensor([ 7.1005e-03, -1.5774e-04,  6.4521e-02,  1.0825e-01,  3.2342e-02,\n                       2.3915e-02,  3.1321e-02,  1.9586e-02,  7.0526e-02,  5.1634e-02,\n                       7.4801e-02,  1.5856e-03,  8.8172e-02,  2.7410e-02,  4.4370e-03,\n                       3.2543e-02,  5.6643e-02,  2.8571e-02, -7.2299e-05,  6.5116e-02,\n                       1.3178e-02,  2.7955e-03,  7.0829e-02,  9.5379e-02,  4.3011e-02,\n                       5.9723e-02, -1.5675e-02,  8.8198e-02,  6.5428e-02,  8.4637e-02,\n                       1.0815e-01,  1.7626e-01,  1.4158e-01, -1.0145e-03,  9.1921e-02,\n                       6.2826e-02,  3.0033e-02,  1.0844e-03,  4.0177e-02,  1.9745e-02,\n                       3.3739e-02,  6.0843e-02,  1.3822e-02,  5.2545e-02,  1.7879e-01,\n                       6.7130e-02,  8.4623e-02,  9.8934e-02,  1.2450e-01,  8.7494e-02,\n                       6.8426e-02,  1.0213e-01,  8.2926e-02,  1.2638e-01, -4.8533e-03,\n                       6.7830e-02,  3.9714e-02,  6.1248e-02,  4.2278e-02,  7.0054e-02,\n                       1.3417e-02,  9.8456e-03,  6.9485e-03,  1.3387e-01,  3.7583e-02,\n                       1.9560e-02,  7.2855e-02,  5.2389e-02,  9.5648e-02,  1.3006e-01,\n                       7.2179e-02,  1.0699e-01,  4.2468e-02,  2.2907e-02,  1.6964e-02,\n                       6.5212e-02,  1.0297e-01,  1.0932e-01,  1.5811e-02,  1.9961e-02,\n                       1.0728e-01,  1.4520e-01,  1.0083e-01, -1.1064e-02,  4.5948e-03,\n                       6.6635e-02,  1.2599e-01,  6.2064e-03,  7.9031e-02,  5.3121e-02,\n                       5.6224e-02,  1.0247e-01,  1.2860e-01,  3.0876e-02,  3.6186e-02,\n                       9.0453e-02,  4.0460e-02,  7.9643e-02,  6.8432e-02,  7.2676e-03,\n                       3.2689e-02,  3.6158e-03,  8.7829e-03,  2.7767e-02,  8.5016e-02,\n                       1.4124e-02, -2.0218e-02,  4.8019e-02,  7.3316e-02, -7.0496e-03,\n                       8.8273e-03,  3.0502e-02,  5.8905e-02,  9.3163e-02, -2.0409e-03,\n                       4.8339e-02, -2.6669e-02,  2.7763e-02,  6.8285e-02,  4.2328e-02,\n                       6.9946e-02,  4.4069e-02,  8.4792e-02,  8.6959e-02,  4.7647e-03,\n                       7.2001e-03,  4.2012e-02,  2.6560e-02,  2.4356e-02,  8.1536e-02,\n                       1.7101e-02,  1.3422e-02,  2.1882e-02,  9.2540e-03, -2.1189e-03,\n                       3.0341e-02,  3.4444e-02,  1.0545e-01, -1.3849e-02,  2.2386e-02,\n                       5.7463e-02,  3.2156e-02, -1.0943e-02, -6.3761e-03,  2.8963e-02,\n                       3.0762e-02,  5.7277e-02,  3.4084e-02,  9.0434e-02,  1.3796e-02,\n                      -1.0150e-02, -3.1245e-02,  2.5650e-03,  8.4970e-02,  1.0114e-02,\n                       2.6730e-02,  6.3526e-02,  7.2126e-02,  8.9261e-02,  1.7417e-02,\n                       3.2578e-02,  8.4578e-03,  4.8359e-02,  3.2796e-02,  1.1776e-02,\n                       4.2887e-02, -3.3832e-03,  8.5710e-02,  5.5065e-02, -4.7329e-03,\n                       8.1066e-03,  3.0400e-02,  4.6880e-02,  9.8175e-02, -1.2383e-02,\n                      -8.8133e-03,  3.6636e-02,  2.7116e-02,  5.2327e-02,  1.0797e-01,\n                       3.1136e-02,  2.4380e-02,  2.9864e-02,  5.1657e-02,  4.1148e-02,\n                       4.4720e-02, -8.0952e-03, -7.1492e-03,  2.3945e-03,  6.0869e-02,\n                       4.8771e-02,  2.2853e-02,  4.1554e-02,  3.5141e-02, -6.6212e-03,\n                       6.4646e-02,  2.3349e-02,  2.0531e-02,  9.5587e-03,  3.9920e-02,\n                       9.2446e-02,  2.0753e-02,  3.9118e-02,  1.0496e-02,  4.1371e-03,\n                      -6.1558e-03,  7.0777e-03,  4.5824e-02,  5.9083e-02,  7.5463e-02,\n                       1.8036e-02,  3.2459e-02,  6.4037e-03,  7.1601e-02,  6.1576e-02,\n                       8.7845e-02,  9.5700e-03,  9.7675e-02,  6.0397e-02,  4.5801e-02,\n                       5.1017e-02,  6.9791e-02,  6.3269e-02,  1.3115e-02,  6.9480e-03,\n                       1.2666e-02,  2.8236e-02,  4.8137e-02,  8.5563e-02,  2.7087e-02,\n                      -8.2860e-03, -5.1361e-03,  2.7147e-02,  3.7014e-02,  7.9769e-03,\n                       2.8408e-02,  8.7227e-03,  3.9483e-02,  8.5783e-02,  5.7463e-02,\n                      -5.6920e-03,  3.6979e-02,  2.3466e-02, -1.7207e-02,  1.7803e-02,\n                       3.1593e-02,  1.7918e-02,  2.2972e-02,  5.9402e-02,  3.4341e-02,\n                       3.1674e-02,  1.0679e-02,  9.2944e-04,  9.4734e-04, -3.3846e-03,\n                       1.4659e-02, -1.1881e-02,  2.8535e-02, -1.2096e-02,  4.4068e-03,\n                      -2.3091e-03, -2.1395e-02,  1.7066e-02, -1.4985e-02,  1.1627e-02,\n                       2.7396e-02,  3.4192e-02,  1.7690e-02,  1.9808e-03,  3.5007e-03,\n                       6.0094e-03,  9.6252e-02,  6.9052e-02,  2.7810e-02,  4.6193e-02,\n                       5.5051e-02,  4.1896e-02,  1.3981e-02, -9.4268e-03,  6.1485e-02,\n                       1.1376e-01,  3.3655e-02,  4.7662e-03,  8.0320e-03,  2.6336e-02,\n                       1.5847e-02, -2.5538e-02,  8.1685e-02, -1.3575e-02, -1.4373e-02,\n                       3.6671e-02,  1.9281e-02, -2.4967e-02,  3.2555e-02, -1.8897e-02,\n                      -3.3027e-02,  7.9526e-02,  3.2261e-02,  2.3468e-03,  2.8618e-03,\n                       4.7266e-02, -2.2239e-02,  8.1038e-03,  3.4181e-02, -2.1629e-02,\n                       3.6511e-02,  7.9015e-02, -2.5515e-03, -1.0515e-02, -2.1589e-02,\n                      -9.0025e-03,  3.4782e-02, -1.2788e-02,  6.2110e-02,  7.6538e-03,\n                       5.7754e-02,  2.8051e-02,  3.2841e-02,  1.8490e-02,  3.8258e-02,\n                       4.1668e-02,  1.9781e-02, -8.2080e-03,  1.0854e-02,  2.6238e-02,\n                       1.8414e-02,  5.0454e-02,  1.5261e-02,  8.6916e-03,  7.9400e-03,\n                       2.5291e-02,  4.0253e-03,  2.1923e-04, -2.5752e-02,  4.2672e-02,\n                       6.2142e-02,  3.5900e-03, -1.5476e-02, -2.8570e-02,  3.3400e-03,\n                       2.1830e-02,  3.3882e-02, -3.3093e-03,  2.1143e-02,  1.0876e-02,\n                       4.7325e-02, -8.0325e-03,  5.4634e-02, -2.7667e-03,  5.6239e-02,\n                       5.2291e-02,  8.0879e-03,  4.9097e-02, -1.3555e-02,  5.4650e-02,\n                       3.7184e-02,  4.6312e-02,  3.7548e-02, -1.6382e-02,  2.4203e-02,\n                       1.1110e-01,  3.1891e-02,  2.3385e-02,  1.4679e-01,  6.0288e-02,\n                       6.0746e-02,  7.5881e-02,  9.4696e-02, -7.5338e-03,  4.7555e-02,\n                       1.7970e-02,  2.5708e-02,  4.9468e-02,  4.1334e-03,  8.1744e-03,\n                       1.4069e-02,  3.4180e-02,  8.8634e-03,  5.0994e-02, -1.6305e-02,\n                      -3.8909e-03,  3.5498e-02,  3.7327e-02,  5.7945e-04, -2.5237e-03,\n                       1.6049e-02,  3.8678e-02,  2.9816e-02, -1.7294e-02,  6.9034e-02,\n                       3.1534e-02, -7.1494e-03,  2.0877e-02,  1.3006e-02,  5.2721e-02,\n                       3.3864e-03,  1.3647e-01,  3.2233e-02,  3.2173e-02, -1.3954e-02,\n                      -1.3950e-02, -1.1712e-02, -2.2847e-02,  1.9769e-02, -2.5221e-02,\n                       2.4192e-02, -3.3107e-03,  6.4434e-03,  4.7105e-02,  4.9574e-02,\n                      -3.7450e-04,  4.4164e-02,  3.8647e-02,  2.4604e-02,  4.3708e-02,\n                       3.3224e-02,  4.3697e-02,  2.6379e-02,  5.2367e-02,  3.4475e-02,\n                      -1.7982e-02,  3.2766e-02,  1.1707e-02,  4.0473e-02, -2.5842e-02,\n                       3.4724e-02,  9.9870e-03, -3.8057e-03, -1.4084e-02,  3.5722e-03,\n                       3.5478e-02,  5.0594e-02,  3.1872e-02, -2.0432e-02,  4.7862e-02,\n                       4.8990e-02, -8.6978e-03,  2.7698e-02,  6.7555e-03,  1.4186e-02,\n                      -1.6816e-02,  5.6678e-02,  2.0556e-02,  3.9375e-02,  1.1426e-01,\n                      -1.4905e-02, -6.8814e-03, -4.1307e-03, -2.2322e-02,  8.6816e-03,\n                       7.8755e-02,  1.3975e-01,  3.3124e-02,  3.1422e-02,  8.6533e-03,\n                      -4.4773e-03, -2.0892e-02,  6.9524e-02, -5.5738e-03,  1.1185e-02,\n                       1.2697e-02,  3.4615e-02,  1.5952e-02,  2.1525e-02,  2.5201e-02,\n                      -1.0566e-02, -9.1406e-03, -5.7862e-03,  1.0105e-02,  1.9061e-02,\n                      -1.4667e-03,  5.2289e-02,  6.2695e-02,  2.0604e-02,  3.4132e-02,\n                       1.8818e-02,  6.1269e-02,  1.3101e-03,  2.6824e-02, -1.8801e-02,\n                       1.0113e-01,  6.2023e-02, -1.1430e-02,  2.3684e-02, -5.2958e-03,\n                       2.2559e-02,  1.7132e-02,  8.6094e-02,  8.0219e-03,  1.6718e-02,\n                      -1.3282e-02,  2.6632e-02,  8.6989e-03,  9.3759e-04,  4.1254e-02,\n                       3.7190e-02, -2.2759e-03, -3.0796e-03, -1.3822e-02,  1.9237e-03,\n                       7.7910e-03,  1.8392e-02,  4.1946e-03, -2.2235e-02,  1.4167e-02,\n                       2.1969e-02, -9.4622e-03, -2.4217e-02,  2.0078e-02,  1.8211e-02,\n                       3.2670e-02, -7.0043e-03,  6.8892e-03, -2.3756e-02,  3.3952e-04,\n                      -1.1687e-02,  1.4528e-02,  3.0780e-02,  3.9705e-03,  3.5723e-02,\n                       1.6759e-02,  2.1029e-02,  1.3736e-02,  2.7744e-02,  4.5304e-02,\n                       2.3603e-02, -2.9128e-02, -2.5507e-02, -5.4489e-03, -2.5541e-02,\n                      -2.3866e-02,  8.1965e-03,  9.0280e-03, -8.2619e-03,  2.4149e-02,\n                      -7.1127e-03,  1.4616e-02, -1.2184e-02, -1.5055e-03,  1.4971e-02,\n                      -2.2341e-02,  2.5975e-02, -3.7818e-03, -1.3764e-02, -8.5748e-03,\n                      -1.7207e-02,  2.3453e-02,  2.2108e-02, -2.4027e-02, -3.0636e-03]))])"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigths"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T12:24:31.189063Z",
     "start_time": "2023-08-03T12:24:31.153315Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.bias',\n              tensor([ 1.8485e-01,  5.2149e-02,  2.5553e-01,  1.6856e-01,  2.3748e-01,\n                      -3.8776e-02,  1.3727e-01,  2.3419e-01,  1.7705e-01,  2.0290e-01,\n                       1.7215e-01,  9.0370e-02,  2.4223e-01,  8.7808e-02,  2.3943e-02,\n                       2.1018e-01,  1.6180e-01,  2.3478e-01,  2.4502e-01,  2.8955e-01,\n                       1.5057e-01,  1.2783e-01,  2.5978e-01,  1.2592e-01,  2.7632e-01,\n                       2.6478e-01, -1.7233e-02,  1.4562e-01,  1.9541e-01,  8.0785e-03,\n                       1.6502e-01,  1.5623e-01,  1.5175e-01,  1.0052e-01,  1.9142e-01,\n                       5.7890e-02,  1.3251e-01,  1.6412e-01,  6.0202e-02,  1.8642e-01,\n                       1.5235e-01,  1.6511e-01,  2.1999e-01,  3.1011e-01,  2.2046e-02,\n                       1.9942e-01,  2.5617e-01,  1.4763e-01,  2.0891e-01,  9.1363e-02,\n                       1.9775e-01,  1.7344e-01,  1.4490e-01,  1.8761e-01,  1.9027e-01,\n                       1.5716e-01,  1.4542e-01,  1.3092e-01,  2.1551e-01,  2.2406e-01,\n                       2.2685e-01,  9.9780e-02,  1.2310e-01,  2.2045e-01,  1.6048e-01,\n                       2.3204e-01,  1.6478e-01,  5.6377e-04,  2.4416e-01,  1.5092e-01,\n                       2.4642e-01,  1.4442e-01,  2.2841e-01,  1.6431e-01,  1.1585e-01,\n                       1.6063e-01,  3.3918e-02,  5.0120e-02,  8.1384e-02,  2.8329e-01,\n                      -2.7745e-02,  2.1187e-01,  1.8256e-01,  2.2876e-01,  1.2362e-01,\n                       9.6573e-02,  1.6133e-02,  1.3151e-01,  2.4455e-01,  1.8640e-01,\n                       7.5113e-02,  2.4936e-01,  1.7684e-01,  2.4821e-01,  2.0124e-01,\n                       2.0831e-01,  1.4094e-01,  6.4234e-02,  9.3472e-02,  9.2401e-02,\n                       1.5743e-01,  1.5009e-01, -6.5547e-03,  1.2855e-01,  1.5631e-01,\n                       7.2713e-02,  1.9626e-01,  2.1521e-01,  1.9377e-01,  2.6814e-01,\n                       8.0402e-02, -1.3851e-02,  1.4931e-01,  2.1327e-01,  1.0298e-01,\n                       1.0106e-01,  1.5346e-01,  1.5860e-01,  1.1011e-01,  1.4001e-01,\n                       1.7397e-01,  2.2768e-01,  2.4467e-01,  8.9902e-02,  2.0451e-01,\n                       1.1335e-02,  2.8690e-01,  1.3532e-01, -1.1143e-02,  1.2155e-01,\n                       2.2186e-01,  3.0469e-01,  1.2380e-01,  1.7407e-01,  2.5974e-01,\n                       1.8303e-01,  1.0868e-01,  1.9205e-01,  1.8926e-01,  7.0396e-03,\n                       2.5589e-01,  1.2740e-01,  2.0655e-01,  2.0560e-01,  1.7900e-02,\n                       1.4923e-01,  2.0537e-01,  7.2561e-02,  1.4306e-01,  1.5655e-01,\n                       1.4732e-01,  6.7199e-02,  2.5941e-01,  2.2985e-01,  1.4965e-01,\n                       1.0782e-01,  1.4436e-01,  1.3956e-01,  2.8022e-01,  6.6853e-04,\n                       1.3744e-01,  1.5716e-01,  1.2834e-01,  1.9055e-01,  1.3085e-02,\n                       2.2516e-01,  2.1618e-01,  1.4735e-01,  8.8396e-02,  1.3374e-01,\n                       7.7784e-02,  4.7229e-02,  2.0882e-01,  1.4718e-01,  1.3380e-01,\n                       2.5035e-01,  2.1888e-01,  2.2671e-02,  2.4817e-01,  2.0390e-01,\n                      -9.8238e-03,  1.3420e-01,  2.5132e-01,  2.9965e-01, -4.8892e-02,\n                       1.5182e-01,  1.7745e-01,  1.6768e-01, -5.4926e-04,  1.1216e-01,\n                       1.2404e-01,  2.2863e-01,  1.7228e-01,  2.5350e-01,  1.4478e-01,\n                       1.7821e-01,  1.8011e-01,  2.4736e-01,  2.1987e-01,  1.5184e-01,\n                       2.5122e-01,  2.4771e-01,  6.5260e-02,  8.8111e-02,  1.1653e-01,\n                       1.0705e-01,  1.4875e-01, -2.7144e-02,  2.4277e-01, -9.5776e-03,\n                       2.4832e-01,  1.7463e-01,  2.0603e-01,  1.5268e-01,  1.7037e-01,\n                       4.5385e-02,  3.1866e-02,  2.6076e-01,  2.2558e-01,  1.7380e-01,\n                       1.9398e-01,  2.1543e-01,  1.7802e-01,  2.2567e-01,  1.9093e-01,\n                       1.8593e-01,  7.7149e-02,  5.0428e-02,  2.2112e-01,  2.2040e-01,\n                       7.1806e-02,  2.9001e-02,  6.1762e-02,  1.6894e-01,  6.3069e-02,\n                       2.3122e-01,  2.6386e-01,  1.6374e-01,  9.7900e-02,  2.3801e-01,\n                      -1.3627e-03,  1.0599e-01,  1.4318e-01,  1.9527e-01,  7.9790e-02,\n                       2.4492e-01,  1.9606e-01, -9.0381e-04,  1.1868e-01,  2.8069e-01,\n                       4.2157e-02,  1.6073e-01,  1.6679e-01,  1.3923e-01,  1.3182e-01,\n                       1.7044e-01,  2.3186e-01,  1.9577e-01,  9.2536e-02,  1.5442e-01,\n                       2.0474e-01,  1.6660e-01,  3.0000e-01,  3.3990e-02,  2.4981e-01,\n                       9.8492e-02,  2.0493e-01,  2.2236e-01,  2.3595e-01,  1.8293e-01,\n                       1.1078e-01,  1.5513e-01,  1.1207e-01,  1.0309e-01,  1.9716e-01,\n                       1.3000e-01,  1.1119e-01,  2.7321e-03,  1.5198e-01,  2.1449e-01,\n                       2.0739e-01,  1.4816e-01,  6.2773e-02,  7.9414e-02,  1.6294e-01,\n                       1.6543e-01,  1.7404e-01,  2.4746e-01,  2.0090e-01,  8.0322e-02,\n                       1.6326e-01,  1.7688e-01,  1.0548e-01,  1.5770e-01,  2.1374e-01,\n                       4.7513e-02,  1.2729e-01,  1.6806e-01,  2.7018e-01,  3.0111e-01,\n                       1.6916e-02,  1.8126e-01,  8.7910e-02,  1.7701e-01,  1.5449e-01,\n                       1.6206e-01,  1.7603e-01,  1.4853e-01,  1.6489e-01,  2.4449e-01,\n                       1.6837e-01,  1.4007e-01,  1.8982e-01,  1.9890e-01,  7.1064e-02,\n                       5.7352e-02,  2.5305e-01,  7.9121e-02,  1.4156e-01,  1.0354e-01,\n                       1.6085e-01,  1.4212e-01,  1.4512e-01,  2.5833e-01,  1.4458e-01,\n                       9.6352e-02,  2.1406e-01,  1.5851e-01,  8.1640e-02,  4.4468e-02,\n                       1.0816e-01,  7.0428e-02,  2.0603e-01,  1.3540e-01,  9.0560e-02,\n                       2.7097e-01,  2.3813e-01,  2.0559e-01,  1.6946e-01,  1.2411e-01,\n                       2.3732e-01,  1.9676e-01,  7.6195e-02, -2.9446e-02,  1.6073e-01,\n                       1.2995e-01,  1.2700e-01,  1.3671e-01,  1.9575e-01,  2.2203e-01,\n                       2.0268e-01,  1.7363e-01,  1.0855e-01,  1.5474e-01,  2.2735e-01,\n                       2.2089e-01,  2.5747e-01,  2.2795e-02,  1.4592e-01,  2.1373e-01,\n                       1.1396e-02,  2.5342e-01,  1.2497e-01,  2.9179e-01,  1.9787e-01,\n                       5.5658e-02, -6.8447e-03,  1.3298e-01,  9.0851e-02,  1.4201e-01,\n                       2.6922e-01,  3.1274e-01,  9.6789e-02,  1.8654e-01,  2.3855e-01,\n                       1.8531e-01,  2.1629e-01,  1.1657e-01,  1.4333e-01,  2.0023e-01,\n                       1.6063e-02,  1.5936e-01,  1.8350e-01,  7.1355e-02,  1.0392e-01,\n                       6.3036e-02,  1.8861e-01,  2.0985e-01,  1.6132e-01,  1.9888e-01,\n                       1.8924e-01,  2.3211e-01,  1.8988e-01,  8.4955e-02,  9.5749e-02,\n                       1.2342e-01,  1.3421e-01,  3.0905e-02,  1.6444e-01,  2.3715e-01,\n                       1.2549e-01,  1.5014e-01,  7.9445e-02,  1.8625e-01,  7.9371e-02,\n                       2.3940e-01,  1.5158e-01,  1.7114e-01,  2.9115e-01,  1.8765e-01,\n                       2.1121e-01, -6.6247e-03,  2.4198e-01,  1.6645e-01,  2.6192e-01,\n                       4.1397e-02,  2.9695e-01,  6.8004e-02,  1.6226e-02,  1.3417e-01,\n                       1.5660e-01,  1.8576e-01,  1.7047e-01,  5.8743e-02,  2.5969e-01,\n                       3.0342e-02,  2.0776e-01,  1.2044e-01,  2.5205e-01,  1.6124e-01,\n                       5.0298e-02,  1.6785e-01,  1.0927e-01,  1.0464e-01,  6.6573e-02,\n                       2.0558e-01,  6.7997e-02,  1.3755e-01,  1.2344e-01,  2.7897e-01,\n                       1.5694e-01,  2.7355e-01,  1.7051e-01,  1.9111e-01,  1.4660e-01,\n                       2.6273e-01,  2.0791e-01,  2.8506e-01,  2.3512e-01,  2.1263e-01,\n                       1.4377e-01,  1.5697e-01,  5.5881e-02,  2.5761e-01, -3.0553e-04,\n                       1.9544e-01,  1.8897e-01,  2.2189e-01,  3.0542e-01,  1.7684e-01,\n                       2.2809e-01,  2.0340e-01, -1.9454e-03,  4.2986e-02,  1.2899e-01,\n                       1.1196e-01,  1.3832e-01,  5.3408e-02,  1.8476e-01,  1.6015e-01,\n                       3.4159e-02,  2.2295e-01,  1.0042e-01,  1.7574e-01,  1.2616e-01,\n                       1.9766e-01,  2.3120e-01,  1.9411e-01,  1.4333e-01,  1.7723e-01,\n                       2.4718e-01,  2.9439e-01,  2.2433e-01,  1.0258e-02,  1.2999e-01,\n                       1.5353e-01,  1.0847e-01,  2.4015e-01,  9.8725e-02,  3.1328e-02,\n                       1.2340e-01,  1.8933e-01,  8.3448e-02,  1.9422e-01,  2.0673e-01,\n                       1.1272e-01,  1.9087e-01,  1.0817e-01,  2.1169e-01,  1.0863e-01,\n                       1.6317e-01,  9.1915e-02,  2.1050e-01,  8.7667e-02,  7.4715e-02,\n                       2.3227e-01,  1.4166e-01,  3.0253e-01,  2.0086e-01,  1.4689e-01,\n                       2.1998e-01,  9.7012e-02,  1.4937e-01,  2.9861e-01,  1.1612e-01,\n                       1.4221e-01,  1.4524e-01,  2.2179e-01,  1.3563e-01,  2.3456e-01,\n                       2.1010e-01,  2.8749e-01,  1.9761e-01,  1.2335e-01,  2.4275e-01,\n                       3.6971e-02,  2.4170e-01, -5.8185e-02,  1.1069e-01,  2.0230e-01,\n                       1.5991e-01,  1.4572e-01,  2.8271e-01,  1.4154e-01,  1.6500e-01,\n                       1.5858e-01,  7.3406e-02,  1.9506e-01,  9.3060e-02,  2.1629e-01,\n                       1.9252e-01,  1.7534e-01,  1.4763e-01,  3.0253e-01,  2.1001e-01,\n                       1.8275e-01,  1.6962e-01,  2.4632e-02,  1.5854e-01,  9.9111e-02])),\n             ('conv1.lin.weight',\n              tensor([[ 0.0399,  0.0268, -0.0519,  ..., -0.0570, -0.0697, -0.0198],\n                      [ 0.0569,  0.0345, -0.0008,  ...,  0.0278, -0.0396,  0.1098],\n                      [ 0.0025, -0.0425, -0.0441,  ...,  0.0300,  0.0126, -0.0118],\n                      ...,\n                      [ 0.0250,  0.0095,  0.0885,  ...,  0.0235,  0.0911, -0.0281],\n                      [-0.0422,  0.0630,  0.0354,  ...,  0.0157, -0.0562,  0.0997],\n                      [-0.0115, -0.0652,  0.0391,  ...,  0.0961,  0.0255,  0.0470]])),\n             ('lin1.weight',\n              tensor([[ 0.0054, -0.0245,  0.0248,  ..., -0.0305, -0.0450,  0.0167],\n                      [ 0.0203,  0.0130, -0.0155,  ...,  0.0309, -0.0120,  0.0014],\n                      [ 0.0270,  0.0071,  0.0218,  ..., -0.0016,  0.0641, -0.0310],\n                      ...,\n                      [ 0.0001, -0.0086, -0.0013,  ...,  0.0043,  0.0006,  0.0116],\n                      [ 0.0020,  0.0023,  0.0035,  ...,  0.0080, -0.0013,  0.0034],\n                      [ 0.0003,  0.0091,  0.0035,  ...,  0.0105, -0.0037, -0.0054]])),\n             ('lin1.bias',\n              tensor([ 7.1005e-03, -1.5774e-04,  6.4521e-02,  1.0825e-01,  3.2342e-02,\n                       2.3915e-02,  3.1321e-02,  1.9586e-02,  7.0526e-02,  5.1634e-02,\n                       7.4801e-02,  1.5856e-03,  8.8172e-02,  2.7410e-02,  4.4370e-03,\n                       3.2543e-02,  5.6643e-02,  2.8571e-02, -7.2299e-05,  6.5116e-02,\n                       1.3178e-02,  2.7955e-03,  7.0829e-02,  9.5379e-02,  4.3011e-02,\n                       5.9723e-02, -1.5675e-02,  8.8198e-02,  6.5428e-02,  8.4637e-02,\n                       1.0815e-01,  1.7626e-01,  1.4158e-01, -1.0145e-03,  9.1921e-02,\n                       6.2826e-02,  3.0033e-02,  1.0844e-03,  4.0177e-02,  1.9745e-02,\n                       3.3739e-02,  6.0843e-02,  1.3822e-02,  5.2545e-02,  1.7879e-01,\n                       6.7130e-02,  8.4623e-02,  9.8934e-02,  1.2450e-01,  8.7494e-02,\n                       6.8426e-02,  1.0213e-01,  8.2926e-02,  1.2638e-01, -4.8533e-03,\n                       6.7830e-02,  3.9714e-02,  6.1248e-02,  4.2278e-02,  7.0054e-02,\n                       1.3417e-02,  9.8456e-03,  6.9485e-03,  1.3387e-01,  3.7583e-02,\n                       1.9560e-02,  7.2855e-02,  5.2389e-02,  9.5648e-02,  1.3006e-01,\n                       7.2179e-02,  1.0699e-01,  4.2468e-02,  2.2907e-02,  1.6964e-02,\n                       6.5212e-02,  1.0297e-01,  1.0932e-01,  1.5811e-02,  1.9961e-02,\n                       1.0728e-01,  1.4520e-01,  1.0083e-01, -1.1064e-02,  4.5948e-03,\n                       6.6635e-02,  1.2599e-01,  6.2064e-03,  7.9031e-02,  5.3121e-02,\n                       5.6224e-02,  1.0247e-01,  1.2860e-01,  3.0876e-02,  3.6186e-02,\n                       9.0453e-02,  4.0460e-02,  7.9643e-02,  6.8432e-02,  7.2676e-03,\n                       3.2689e-02,  3.6158e-03,  8.7829e-03,  2.7767e-02,  8.5016e-02,\n                       1.4124e-02, -2.0218e-02,  4.8019e-02,  7.3316e-02, -7.0496e-03,\n                       8.8273e-03,  3.0502e-02,  5.8905e-02,  9.3163e-02, -2.0409e-03,\n                       4.8339e-02, -2.6669e-02,  2.7763e-02,  6.8285e-02,  4.2328e-02,\n                       6.9946e-02,  4.4069e-02,  8.4792e-02,  8.6959e-02,  4.7647e-03,\n                       7.2001e-03,  4.2012e-02,  2.6560e-02,  2.4356e-02,  8.1536e-02,\n                       1.7101e-02,  1.3422e-02,  2.1882e-02,  9.2540e-03, -2.1189e-03,\n                       3.0341e-02,  3.4444e-02,  1.0545e-01, -1.3849e-02,  2.2386e-02,\n                       5.7463e-02,  3.2156e-02, -1.0943e-02, -6.3761e-03,  2.8963e-02,\n                       3.0762e-02,  5.7277e-02,  3.4084e-02,  9.0434e-02,  1.3796e-02,\n                      -1.0150e-02, -3.1245e-02,  2.5650e-03,  8.4970e-02,  1.0114e-02,\n                       2.6730e-02,  6.3526e-02,  7.2126e-02,  8.9261e-02,  1.7417e-02,\n                       3.2578e-02,  8.4578e-03,  4.8359e-02,  3.2796e-02,  1.1776e-02,\n                       4.2887e-02, -3.3832e-03,  8.5710e-02,  5.5065e-02, -4.7329e-03,\n                       8.1066e-03,  3.0400e-02,  4.6880e-02,  9.8175e-02, -1.2383e-02,\n                      -8.8133e-03,  3.6636e-02,  2.7116e-02,  5.2327e-02,  1.0797e-01,\n                       3.1136e-02,  2.4380e-02,  2.9864e-02,  5.1657e-02,  4.1148e-02,\n                       4.4720e-02, -8.0952e-03, -7.1492e-03,  2.3945e-03,  6.0869e-02,\n                       4.8771e-02,  2.2853e-02,  4.1554e-02,  3.5141e-02, -6.6212e-03,\n                       6.4646e-02,  2.3349e-02,  2.0531e-02,  9.5587e-03,  3.9920e-02,\n                       9.2446e-02,  2.0753e-02,  3.9118e-02,  1.0496e-02,  4.1371e-03,\n                      -6.1558e-03,  7.0777e-03,  4.5824e-02,  5.9083e-02,  7.5463e-02,\n                       1.8036e-02,  3.2459e-02,  6.4037e-03,  7.1601e-02,  6.1576e-02,\n                       8.7845e-02,  9.5700e-03,  9.7675e-02,  6.0397e-02,  4.5801e-02,\n                       5.1017e-02,  6.9791e-02,  6.3269e-02,  1.3115e-02,  6.9480e-03,\n                       1.2666e-02,  2.8236e-02,  4.8137e-02,  8.5563e-02,  2.7087e-02,\n                      -8.2860e-03, -5.1361e-03,  2.7147e-02,  3.7014e-02,  7.9769e-03,\n                       2.8408e-02,  8.7227e-03,  3.9483e-02,  8.5783e-02,  5.7463e-02,\n                      -5.6920e-03,  3.6979e-02,  2.3466e-02, -1.7207e-02,  1.7803e-02,\n                       3.1593e-02,  1.7918e-02,  2.2972e-02,  5.9402e-02,  3.4341e-02,\n                       3.1674e-02,  1.0679e-02,  9.2944e-04,  9.4734e-04, -3.3846e-03,\n                       1.4659e-02, -1.1881e-02,  2.8535e-02, -1.2096e-02,  4.4068e-03,\n                      -2.3091e-03, -2.1395e-02,  1.7066e-02, -1.4985e-02,  1.1627e-02,\n                       2.7396e-02,  3.4192e-02,  1.7690e-02,  1.9808e-03,  3.5007e-03,\n                       6.0094e-03,  9.6252e-02,  6.9052e-02,  2.7810e-02,  4.6193e-02,\n                       5.5051e-02,  4.1896e-02,  1.3981e-02, -9.4268e-03,  6.1485e-02,\n                       1.1376e-01,  3.3655e-02,  4.7662e-03,  8.0320e-03,  2.6336e-02,\n                       1.5847e-02, -2.5538e-02,  8.1685e-02, -1.3575e-02, -1.4373e-02,\n                       3.6671e-02,  1.9281e-02, -2.4967e-02,  3.2555e-02, -1.8897e-02,\n                      -3.3027e-02,  7.9526e-02,  3.2261e-02,  2.3468e-03,  2.8618e-03,\n                       4.7266e-02, -2.2239e-02,  8.1038e-03,  3.4181e-02, -2.1629e-02,\n                       3.6511e-02,  7.9015e-02, -2.5515e-03, -1.0515e-02, -2.1589e-02,\n                      -9.0025e-03,  3.4782e-02, -1.2788e-02,  6.2110e-02,  7.6538e-03,\n                       5.7754e-02,  2.8051e-02,  3.2841e-02,  1.8490e-02,  3.8258e-02,\n                       4.1668e-02,  1.9781e-02, -8.2080e-03,  1.0854e-02,  2.6238e-02,\n                       1.8414e-02,  5.0454e-02,  1.5261e-02,  8.6916e-03,  7.9400e-03,\n                       2.5291e-02,  4.0253e-03,  2.1923e-04, -2.5752e-02,  4.2672e-02,\n                       6.2142e-02,  3.5900e-03, -1.5476e-02, -2.8570e-02,  3.3400e-03,\n                       2.1830e-02,  3.3882e-02, -3.3093e-03,  2.1143e-02,  1.0876e-02,\n                       4.7325e-02, -8.0325e-03,  5.4634e-02, -2.7667e-03,  5.6239e-02,\n                       5.2291e-02,  8.0879e-03,  4.9097e-02, -1.3555e-02,  5.4650e-02,\n                       3.7184e-02,  4.6312e-02,  3.7548e-02, -1.6382e-02,  2.4203e-02,\n                       1.1110e-01,  3.1891e-02,  2.3385e-02,  1.4679e-01,  6.0288e-02,\n                       6.0746e-02,  7.5881e-02,  9.4696e-02, -7.5338e-03,  4.7555e-02,\n                       1.7970e-02,  2.5708e-02,  4.9468e-02,  4.1334e-03,  8.1744e-03,\n                       1.4069e-02,  3.4180e-02,  8.8634e-03,  5.0994e-02, -1.6305e-02,\n                      -3.8909e-03,  3.5498e-02,  3.7327e-02,  5.7945e-04, -2.5237e-03,\n                       1.6049e-02,  3.8678e-02,  2.9816e-02, -1.7294e-02,  6.9034e-02,\n                       3.1534e-02, -7.1494e-03,  2.0877e-02,  1.3006e-02,  5.2721e-02,\n                       3.3864e-03,  1.3647e-01,  3.2233e-02,  3.2173e-02, -1.3954e-02,\n                      -1.3950e-02, -1.1712e-02, -2.2847e-02,  1.9769e-02, -2.5221e-02,\n                       2.4192e-02, -3.3107e-03,  6.4434e-03,  4.7105e-02,  4.9574e-02,\n                      -3.7450e-04,  4.4164e-02,  3.8647e-02,  2.4604e-02,  4.3708e-02,\n                       3.3224e-02,  4.3697e-02,  2.6379e-02,  5.2367e-02,  3.4475e-02,\n                      -1.7982e-02,  3.2766e-02,  1.1707e-02,  4.0473e-02, -2.5842e-02,\n                       3.4724e-02,  9.9870e-03, -3.8057e-03, -1.4084e-02,  3.5722e-03,\n                       3.5478e-02,  5.0594e-02,  3.1872e-02, -2.0432e-02,  4.7862e-02,\n                       4.8990e-02, -8.6978e-03,  2.7698e-02,  6.7555e-03,  1.4186e-02,\n                      -1.6816e-02,  5.6678e-02,  2.0556e-02,  3.9375e-02,  1.1426e-01,\n                      -1.4905e-02, -6.8814e-03, -4.1307e-03, -2.2322e-02,  8.6816e-03,\n                       7.8755e-02,  1.3975e-01,  3.3124e-02,  3.1422e-02,  8.6533e-03,\n                      -4.4773e-03, -2.0892e-02,  6.9524e-02, -5.5738e-03,  1.1185e-02,\n                       1.2697e-02,  3.4615e-02,  1.5952e-02,  2.1525e-02,  2.5201e-02,\n                      -1.0566e-02, -9.1406e-03, -5.7862e-03,  1.0105e-02,  1.9061e-02,\n                      -1.4667e-03,  5.2289e-02,  6.2695e-02,  2.0604e-02,  3.4132e-02,\n                       1.8818e-02,  6.1269e-02,  1.3101e-03,  2.6824e-02, -1.8801e-02,\n                       1.0113e-01,  6.2023e-02, -1.1430e-02,  2.3684e-02, -5.2958e-03,\n                       2.2559e-02,  1.7132e-02,  8.6094e-02,  8.0219e-03,  1.6718e-02,\n                      -1.3282e-02,  2.6632e-02,  8.6989e-03,  9.3759e-04,  4.1254e-02,\n                       3.7190e-02, -2.2759e-03, -3.0796e-03, -1.3822e-02,  1.9237e-03,\n                       7.7910e-03,  1.8392e-02,  4.1946e-03, -2.2235e-02,  1.4167e-02,\n                       2.1969e-02, -9.4622e-03, -2.4217e-02,  2.0078e-02,  1.8211e-02,\n                       3.2670e-02, -7.0043e-03,  6.8892e-03, -2.3756e-02,  3.3952e-04,\n                      -1.1687e-02,  1.4528e-02,  3.0780e-02,  3.9705e-03,  3.5723e-02,\n                       1.6759e-02,  2.1029e-02,  1.3736e-02,  2.7744e-02,  4.5304e-02,\n                       2.3603e-02, -2.9128e-02, -2.5507e-02, -5.4489e-03, -2.5541e-02,\n                      -2.3866e-02,  8.1965e-03,  9.0280e-03, -8.2619e-03,  2.4149e-02,\n                      -7.1127e-03,  1.4616e-02, -1.2184e-02, -1.5055e-03,  1.4971e-02,\n                      -2.2341e-02,  2.5975e-02, -3.7818e-03, -1.3764e-02, -8.5748e-03,\n                      -1.7207e-02,  2.3453e-02,  2.2108e-02, -2.4027e-02, -3.0636e-03])),\n             ('conv2.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('conv2.lin.weight',\n              tensor([[-0.0440,  0.0430, -0.0198,  ..., -0.0621,  0.0248,  0.0242],\n                      [-0.0251,  0.0008, -0.0210,  ..., -0.0629, -0.0195,  0.0599],\n                      [-0.0549, -0.0624,  0.0326,  ..., -0.0178, -0.0376,  0.0737],\n                      ...,\n                      [ 0.0598, -0.0621,  0.0331,  ...,  0.0678, -0.0094,  0.0585],\n                      [ 0.0498, -0.0638,  0.0345,  ...,  0.0605, -0.0251, -0.0573],\n                      [ 0.0194,  0.0516, -0.0053,  ...,  0.0335, -0.0196, -0.0288]])),\n             ('lin2.weight',\n              tensor([[ 0.0322,  0.0280, -0.0018,  ...,  0.0225,  0.0363, -0.0388],\n                      [ 0.0017, -0.0016,  0.0390,  ..., -0.0267,  0.0131, -0.0178],\n                      [-0.0358,  0.0029,  0.0253,  ..., -0.0361,  0.0279,  0.0105],\n                      ...,\n                      [ 0.0321,  0.0185, -0.0093,  ..., -0.0398,  0.0077, -0.0210],\n                      [ 0.0156, -0.0336,  0.0403,  ...,  0.0182, -0.0043,  0.0143],\n                      [-0.0289, -0.0186,  0.0016,  ...,  0.0132,  0.0075, -0.0033]])),\n             ('lin2.bias',\n              tensor([ 0.0027, -0.0186, -0.0258, -0.0065, -0.0247, -0.0403, -0.0168,  0.0010,\n                      -0.0402,  0.0056, -0.0077,  0.0102,  0.0158,  0.0241, -0.0249, -0.0266,\n                       0.0054,  0.0397, -0.0204,  0.0179]))])"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T12:24:12.750801Z",
     "start_time": "2023-08-03T12:24:12.721382Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "['MY GABA', 'MY Glut', 'Astro-Epen', 'Oligo', 'P GABA', ..., 'CNU-HYa GABA', 'MB Glut', 'CNU-HYa Glut', 'TH Glut', 'MB Dopa']\nLength: 20\nCategories (20, object): ['Astro-Epen', 'CB GABA', 'CB Glut', 'CNU-HYa GABA', ..., 'P GABA', 'P Glut', 'TH Glut', 'Vascular']"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_constructor.adata.obs.class_label.unique())#.cat.codes.values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T12:19:38.508047Z",
     "start_time": "2023-08-03T12:19:38.477139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,  3600, 17561])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset:\n",
    "    print(batch.x)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T12:19:38.709555Z",
     "start_time": "2023-08-03T12:19:38.677522Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T15:24:02.928579Z",
     "start_time": "2023-08-03T15:24:02.919963Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1302,  6712,  6727,  ..., 39346, 40173, 50669])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.x)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T12:19:39.524170Z",
     "start_time": "2023-08-03T12:19:39.353033Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc47254144b546c8bcd197066f8ec03c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch import optim\n",
    "from statistics import mean\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = spatialSSL.Utils.split_dataset(dataset=dataset, split_percent=(0.8, 0.1, 0.1), batch_size = 64)\n",
    "\n",
    "epochs = 20\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = GCN(in_channels=550, hidden_channels=550, num_classes=len(dataset_constructor.adata.obs.class_label.unique()))\n",
    "\n",
    "#net.load_state_dict(torch.load(\"../models/full_model.pt\",  map_location=torch.device(device)), strict=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)#, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net.to(device)\n",
    "# Train the model\n",
    "# training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    net.train()\n",
    "\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    train_acc = MulticlassAccuracy()\n",
    "    val_acc = MulticlassAccuracy()\n",
    "\n",
    "    for data in train_loader:\n",
    "        #inputs, _ = data.\n",
    "        input = torch.tensor(dataset_constructor.adata.X[data.x].toarray(), dtype=torch.double).to(device).float()\n",
    "\n",
    "        labels = torch.tensor(dataset_constructor.adata[data.x.numpy()].obs.class_label.cat.codes.values).to(device).long()\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(input, data.edge_index)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        #accuracy = (outputs.argmax(dim=1) == labels).sum().item() / len(labels)\n",
    "        train_acc.update(outputs.argmax(dim=1), labels)\n",
    "        #train_accs.append(accuracy)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_loss = running_loss / len(train_loader)\n",
    "       # Validation\n",
    "    net.eval()\n",
    "    val_accs = []\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for data in val_loader:\n",
    "\n",
    "            input = torch.tensor(dataset_constructor.adata.X[data.x].toarray(), dtype=torch.double).to(device).float()\n",
    "            labels = torch.tensor(dataset_constructor.adata[data.x.numpy()].obs.class_label.cat.codes.values).to(device).long()\n",
    "            outputs = net(input, data.edge_index)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_acc.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "            #accuracy = (outputs.argmax(dim=1) == labels).sum().item() / len(labels)\n",
    "            #val_accs.append(accuracy)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        #val_accuracy = 100 * correct / total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{epochs}, Train Loss: {running_loss:.4f}, Val Loss: {avg_val_loss:.4f} Train Acc: {train_acc.compute():.4f}, Val Acc: {val_acc.compute():.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    #print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.3f}, accuracy: {sum(all_accuracy) / len(all_accuracy):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T10:04:54.184259Z",
     "start_time": "2023-08-04T10:02:03.851900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd10e365ddf44ea1b30654367bb95869"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20, Train Loss: 2.6104, Val Loss: 2.4669 Train Acc: 0.5257, Val Acc: 0.6286\n",
      "Epoch: 2/20, Train Loss: 2.4438, Val Loss: 2.4517 Train Acc: 0.6449, Val Acc: 0.6324\n",
      "Epoch: 3/20, Train Loss: 2.4395, Val Loss: 2.4484 Train Acc: 0.6452, Val Acc: 0.6336\n",
      "Epoch: 4/20, Train Loss: 2.4346, Val Loss: 2.4379 Train Acc: 0.6492, Val Acc: 0.6473\n",
      "Epoch: 5/20, Train Loss: 2.4257, Val Loss: 2.4348 Train Acc: 0.6580, Val Acc: 0.6492\n",
      "Epoch: 6/20, Train Loss: 2.4218, Val Loss: 2.4333 Train Acc: 0.6611, Val Acc: 0.6502\n",
      "Epoch: 7/20, Train Loss: 2.4218, Val Loss: 2.4320 Train Acc: 0.6603, Val Acc: 0.6511\n",
      "Epoch: 8/20, Train Loss: 2.4230, Val Loss: 2.4321 Train Acc: 0.6592, Val Acc: 0.6507\n",
      "Epoch: 9/20, Train Loss: 2.4216, Val Loss: 2.4312 Train Acc: 0.6603, Val Acc: 0.6512\n",
      "Epoch: 10/20, Train Loss: 2.4202, Val Loss: 2.4310 Train Acc: 0.6620, Val Acc: 0.6512\n",
      "Epoch: 11/20, Train Loss: 2.4149, Val Loss: 2.4311 Train Acc: 0.6674, Val Acc: 0.6518\n",
      "Epoch: 12/20, Train Loss: 2.4194, Val Loss: 2.4300 Train Acc: 0.6629, Val Acc: 0.6529\n",
      "Epoch: 13/20, Train Loss: 2.4200, Val Loss: 2.4300 Train Acc: 0.6614, Val Acc: 0.6529\n",
      "Epoch: 14/20, Train Loss: 2.4222, Val Loss: 2.4292 Train Acc: 0.6596, Val Acc: 0.6533\n",
      "Epoch: 15/20, Train Loss: 2.4176, Val Loss: 2.4291 Train Acc: 0.6645, Val Acc: 0.6534\n",
      "Epoch: 16/20, Train Loss: 2.4181, Val Loss: 2.4287 Train Acc: 0.6645, Val Acc: 0.6541\n",
      "Epoch: 17/20, Train Loss: 2.4167, Val Loss: 2.4294 Train Acc: 0.6650, Val Acc: 0.6530\n",
      "Epoch: 18/20, Train Loss: 2.4195, Val Loss: 2.4279 Train Acc: 0.6631, Val Acc: 0.6549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch import optim\n",
    "from statistics import mean\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "epochs = 20\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = GCN(in_channels=550, hidden_channels=550, num_classes=len(dataset_constructor.adata.obs.class_label.unique()))\n",
    "\n",
    "#net.load_state_dict(torch.load(\"../models/full_model.pt\",  map_location=torch.device(device)), strict=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)#, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net.to(device)\n",
    "# Train the model\n",
    "# training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    net.train()\n",
    "\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    train_acc = MulticlassAccuracy()\n",
    "    val_acc = MulticlassAccuracy()\n",
    "\n",
    "    for data in train_loader:\n",
    "        #inputs, _ = data.\n",
    "        input = torch.tensor(dataset_constructor.adata.X[data.x].toarray(), dtype=torch.double).to(device).float()\n",
    "\n",
    "        labels = torch.tensor(dataset_constructor.adata[data.x.numpy()].obs.class_label.cat.codes.values).to(device).long()\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(input, data.edge_index)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        #accuracy = (outputs.argmax(dim=1) == labels).sum().item() / len(labels)\n",
    "        train_acc.update(outputs.argmax(dim=1), labels)\n",
    "        #train_accs.append(accuracy)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_loss = running_loss / len(train_loader)\n",
    "       # Validation\n",
    "    net.eval()\n",
    "    val_accs = []\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for data in val_loader:\n",
    "\n",
    "            input = torch.tensor(dataset_constructor.adata.X[data.x].toarray(), dtype=torch.double).to(device).float()\n",
    "            labels = torch.tensor(dataset_constructor.adata[data.x.numpy()].obs.class_label.cat.codes.values).to(device).long()\n",
    "            outputs = net(input, data.edge_index)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_acc.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "            #accuracy = (outputs.argmax(dim=1) == labels).sum().item() / len(labels)\n",
    "            #val_accs.append(accuracy)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        #val_accuracy = 100 * correct / total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{epochs}, Train Loss: {running_loss:.4f}, Val Loss: {avg_val_loss:.4f} Train Acc: {train_acc.compute():.4f}, Val Acc: {val_acc.compute():.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    #print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.3f}, accuracy: {sum(all_accuracy) / len(all_accuracy):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T17:07:20.318866Z",
     "start_time": "2023-08-03T16:31:28.678488Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, training=True):\n",
    "    model.train(training)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = batch.x.to(device)\n",
    "        edge_index = batch.edge_index.to(device)\n",
    "        y = batch.y.to(device)\n",
    "        out = model(x, edge_index)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        losses.append(loss.item())\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        acc = (out.argmax(dim=1) == y).sum().item() / len(y)\n",
    "        accuracies.append(acc)\n",
    "    return mean(losses), mean(accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
