{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-24T11:32:16.254306Z",
     "start_time": "2023-07-24T11:32:16.249041Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lg\\anaconda3\\envs\\spatial_atlas_ssl\\lib\\site-packages\\geopandas\\_compat.py:124: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lg\\anaconda3\\envs\\spatial_atlas_ssl\\lib\\site-packages\\spatialdata\\__init__.py:9: UserWarning: Geopandas was set to use PyGEOS, changing to shapely 2.0 with:\n",
      "\n",
      "\tgeopandas.options.use_pygeos = True\n",
      "\n",
      "If you intended to use PyGEOS, set the option to False.\n",
      "  _check_geopandas_using_shapely()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\lg\\\\PycharmProjects\\\\spatial_atlas_ssl\\\\notebooks'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "adata = sc.read(\"C:\\\\Users\\\\lg\\\\PycharmProjects\\\\spatial_atlas_ssl\\\\data\\\\img_119670929.h5ad\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T11:26:42.046540Z",
     "start_time": "2023-07-24T11:26:41.575309Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# now we have the adata object of just a single image\n",
    "sq.gr.spatial_neighbors(adata=adata, radius=1000, key_added=\"adjacency_matrix\", coord_type=\"generic\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T11:28:02.177308Z",
     "start_time": "2023-07-24T11:26:42.047489Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# function to get k lowest values from each row of a sparse matrix\n",
    "def get_k_lowest_values(matrix, k):\n",
    "    n_rows = matrix.shape[0]\n",
    "    k_lowest_indices = np.empty((n_rows, k), dtype=int)\n",
    "    for i in range(n_rows):\n",
    "        start = matrix.indptr[i]\n",
    "        end = matrix.indptr[i + 1]\n",
    "        row_data = matrix.data[start:end]\n",
    "        row_indices = matrix.indices[start:end]\n",
    "        k_smallest_indices = np.argpartition(row_data, k)[:k]\n",
    "        k_lowest_indices[i] = row_indices[k_smallest_indices]\n",
    "    return k_lowest_indices\n",
    "\n",
    "num_nearest = 10\n",
    "closest_matrix = get_k_lowest_values(adata.obsp['adjacency_matrix_distances'], num_nearest)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T11:28:04.430846Z",
     "start_time": "2023-07-24T11:28:02.180126Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/26230 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14dd07302cab497dafdee218011274f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we construct dataset using closest 5 cells\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i, cell in tqdm(enumerate(adata.X), total=len(adata)):\n",
    "    y.append(cell.toarray())\n",
    "    five_closest_cells = np.array([adata.X[index].toarray() for index in closest_matrix[i]])\n",
    "    X.append(five_closest_cells.flatten())\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.concatenate(y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T11:33:46.565717Z",
     "start_time": "2023-07-24T11:33:28.349158Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26230, 550) (26230, 5500)\n"
     ]
    }
   ],
   "source": [
    "#X = np.concatenate(X)\n",
    "print(y.shape, X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T11:33:46.571853Z",
     "start_time": "2023-07-24T11:33:46.568833Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# we use 80% of the data for training and 10% for validation and 10% for testing\n",
    "# Create a custom dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create the dataset and split it into training, validation, and testing sets\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "dataset = MyDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T11:33:46.775690Z",
     "start_time": "2023-07-24T11:33:46.577530Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T11:33:46.814770Z",
     "start_time": "2023-07-24T11:33:46.781120Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# improve model structure with 2 hidden layer\n",
    "\n",
    "\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_nearest):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim*num_nearest,input_dim*(num_nearest-2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*(num_nearest-2) , input_dim*(num_nearest-4)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*(num_nearest-4), output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T11:33:46.823148Z",
     "start_time": "2023-07-24T11:33:46.819188Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5094, Train R2: 0.1001, Val Loss: 0.4899, Val R2: 0.1284\n",
      "Epoch 2/10, Train Loss: 0.4723, Train R2: 0.1513, Val Loss: 0.4844, Val R2: 0.1363\n",
      "Epoch 3/10, Train Loss: 0.4492, Train R2: 0.1792, Val Loss: 0.4908, Val R2: 0.1356\n",
      "Epoch 4/10, Train Loss: 0.4198, Train R2: 0.2121, Val Loss: 0.5057, Val R2: 0.1178\n",
      "Epoch 5/10, Train Loss: 0.3806, Train R2: 0.2535, Val Loss: 0.5161, Val R2: 0.1142\n",
      "Epoch 6/10, Train Loss: 0.3416, Train R2: 0.2945, Val Loss: 0.5128, Val R2: 0.1158\n",
      "Epoch 7/10, Train Loss: 0.3115, Train R2: 0.3263, Val Loss: 0.5165, Val R2: 0.1121\n",
      "Epoch 8/10, Train Loss: 0.2890, Train R2: 0.3517, Val Loss: 0.5228, Val R2: 0.1035\n",
      "Epoch 9/10, Train Loss: 0.2715, Train R2: 0.3738, Val Loss: 0.5232, Val R2: 0.1022\n",
      "Epoch 10/10, Train Loss: 0.2561, Train R2: 0.3950, Val Loss: 0.5272, Val R2: 0.0945\n"
     ]
    }
   ],
   "source": [
    "# Set device for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create an instance of the model and move it to the device\n",
    "input_dim = output_dim = y.shape[1]\n",
    "#output_dim = y.shape[1]\n",
    "\n",
    "model = LinearModel(input_dim, output_dim,num_nearest).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_r2_scores = []\n",
    "val_r2_scores = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 20  # Number of epochs to wait for improvement in validation loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        targets_list.append(targets.cpu().numpy())\n",
    "        outputs_list.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "    # Compute and store the average training loss for this epoch\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Calculate R2 score for the training set\n",
    "    train_targets_all = np.concatenate(targets_list)\n",
    "    train_outputs_all = np.concatenate(outputs_list)\n",
    "    train_r2 = r2_score(train_targets_all, train_outputs_all)\n",
    "    train_r2_scores.append(train_r2)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    val_targets_list = []\n",
    "    val_outputs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the validation loss\n",
    "            val_loss = criterion(outputs, targets)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            val_targets_list.append(targets.cpu().numpy())\n",
    "            val_outputs_list.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "        # Compute and store the average validation loss for this epoch\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Calculate R2 score for the validation set\n",
    "        val_targets_all = np.concatenate(val_targets_list)\n",
    "        val_outputs_all = np.concatenate(val_outputs_list)\n",
    "        val_r2 = r2_score(val_targets_all, val_outputs_all)\n",
    "        val_r2_scores.append(val_r2)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train R2: {train_r2:.4f}, Val Loss: {avg_val_loss:.4f}, Val R2: {val_r2:.4f}\")\n",
    "\n",
    "\n",
    "    # early stopping with patience\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"../models/best_model_1.pt\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping after epoch {epoch + 1}\")\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-07-24T11:37:13.521914Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4862, Test R2: 0.1388\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance on test loader\n",
    "model.load_state_dict(torch.load(\"../models/best_model_1.pt\"))\n",
    "model.eval()\n",
    "test_targets_list = []\n",
    "test_outputs_list = []\n",
    "\n",
    "# loop over test loader and save r2 and loss\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        test_targets_list.append(targets.cpu().numpy())\n",
    "        test_outputs_list.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "test_targets_all = np.concatenate(test_targets_list)\n",
    "test_outputs_all = np.concatenate(test_outputs_list)\n",
    "test_r2 = r2_score(test_targets_all, test_outputs_all)\n",
    "test_loss = criterion(torch.from_numpy(test_outputs_all), torch.from_numpy(test_targets_all)).item()\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test R2: {test_r2:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# improved model structure with 2 hidden layers\n",
    "\n",
    "class LinearModel2(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(5*550, 3*550),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3*550, 2*550),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*550, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/26230 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03221df7c748421daedfe7e7492df240"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create dataset using connectivity matrix of 5 closest cells and expression of 5 closest cells\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i, cell in tqdm(enumerate(adata.X), total=len(adata)):\n",
    "    y.append(cell.toarray())\n",
    "    five_closest_cells = np.array([adata.X[index].toarray() for index in closest_matrix[i]])\n",
    "    X.append(five_closest_cells.flatten())\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.concatenate(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26230, 550) (26230, 2750)\n"
     ]
    }
   ],
   "source": [
    "# print shape\n",
    "print(y.shape, X.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# using connectivity matrix of 5 closest cells and expression of 5 closest cells with torch geometric\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create dataset using connectivity matrix of 5 closest cells and expression of 5 closest cells\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i, cell in tqdm(enumerate(adata.X), total=len(adata)):\n",
    "    y.append(cell.toarray())\n",
    "    five_closest_cells = np.array([adata.X[index].toarray() for index in closest_matrix[i]])\n",
    "    X.append(five_closest_cells.flatten())\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.concatenate(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print shape\n",
    "print(y.shape, X.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#get the connectivity matrix\n",
    "connectivity_matrix = adata.obsp['adjacency_matrix']"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
